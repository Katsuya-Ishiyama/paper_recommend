{"title": "Generating Sentences Using a Dynamic Canvas. (arXiv:1806.05178v1 [cs.CL])", "description": "We introduce the Attentive Unsupervised Text (W)riter (AUTR), which is a word level generative model for natural language. It uses a recurrent neural network with a dynamic attention and canvas memory mechanism to iteratively construct sentences. By viewing the state of the memory at intermediate stages and where the model is placing its attention, we gain insight into how it constructs sentences. We demonstrate that AUTR learns a meaningful latent representation for each sentence, and achieves competitive log-likelihood lower bounds whilst being computationally efficient. It is effective at generating and reconstructing sentences, as well as imputing missing words. ", "link": "http://arxiv.org/abs/1806.05178", "authors": [{"name": "Harshil Shah", "link": "http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1"}, {"name": "Bowen Zheng", "link": "http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"}, {"name": "David Barber", "link": "http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Enabling End-To-End Machine Learning Replicability: A Case Study in Educational Data Mining. (arXiv:1806.05208v1 [cs.CY])", "description": "The use of machine learning techniques has expanded in education research, driven by the rich data from digital learning environments and institutional data warehouses. However, replication of machine learned models in the domain of the learning sciences is particularly challenging due to a confluence of experimental, methodological, and data barriers. We discuss the challenges of end-to-end machine learning replication in this context, and present an open-source software toolkit, the MOOC Replication Framework (MORF), to address them. We demonstrate the use of MORF by conducting a replication at scale, and provide a complete executable container, with unique DOIs documenting the configurations of each individual trial, for replication or future extension at https://github.com/educational-technology-collective/fy2015-replication. This work demonstrates an approach to end-to-end machine learning replication which is relevant to any domain with large, complex or multi-format, privacy-protected data with a consistent schema. ", "link": "http://arxiv.org/abs/1806.05208", "authors": [{"name": "Josh Gardner", "link": "http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1"}, {"name": "Yuming Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"}, {"name": "Ryan Baker", "link": "http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1"}, {"name": "Christopher Brooks", "link": "http://arxiv.org/find/cs/1/au:+Brooks_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "A latent spatial factor approach for synthesizing opioid associated deaths and treatment admissions in Ohio counties. (arXiv:1806.05232v1 [stat.AP])", "description": "Background: Opioid misuse is a major public health issue in the United States and in particular Ohio. However, the burden of the epidemic is challenging to quantify as public health surveillance measures capture different aspects of the problem. Here we synthesize county-level death and treatment counts to compare the relative burden across counties and assess associations with social environmental covariates. Methods: We construct a generalized spatial factor model to jointly model death and treatment rates for each county. For each outcome, we specify a spatial rates parameterization for a Poisson regression model with spatially varying factor loadings. We use a conditional autoregressive model to account for spatial dependence within a Bayesian framework. Results: The estimated spatial factor was highest in the southern and southwestern counties of the state, representing a higher burden of the opioid epidemic. We found that relatively high rates of treatment contributed to the factor in the southern part of the state; whereas, relatively higher rates of death contributed in the southwest. The estimated factor was also positively associated with the proportion of residents aged 18-64 on disability and negatively associated with the proportion of residents reporting white race. Conclusions: We synthesized the information in the opioid associated death and treatment counts through a spatial factor model to estimate a latent factor representing the consensus between the two surveillance measures. We believe this framework provides a coherent approach to describe the epidemic while leveraging information from multiple surveillance measures. ", "link": "http://arxiv.org/abs/1806.05232", "authors": [{"name": "Staci Hepler", "link": "http://arxiv.org/find/stat/1/au:+Hepler_S/0/1/0/all/0/1"}, {"name": "Erin McKnight", "link": "http://arxiv.org/find/stat/1/au:+McKnight_E/0/1/0/all/0/1"}, {"name": "Andrea Bonny", "link": "http://arxiv.org/find/stat/1/au:+Bonny_A/0/1/0/all/0/1"}, {"name": "David Kline", "link": "http://arxiv.org/find/stat/1/au:+Kline_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Manifold Mixup: Encouraging Meaningful On-Manifold Interpolation as a Regularizer. (arXiv:1806.05236v1 [stat.ML])", "description": "Deep networks often perform well on the data manifold on which they are trained, yet give incorrect (and often very confident) answers when evaluated on points from off of the training distribution. This is exemplified by the adversarial examples phenomenon but can also be seen in terms of model generalization and domain shift. We propose Manifold Mixup which encourages the network to produce more reasonable and less confident predictions at points with combinations of attributes not seen in the training set. This is accomplished by training on convex combinations of the hidden state representations of data samples. Using this method, we demonstrate improved semi-supervised learning, learning with limited labeled data, and robustness to adversarial examples. Manifold Mixup requires no (significant) additional computation. Analytical experiments on both real data and synthetic data directly support our hypothesis for why the Manifold Mixup method improves results. ", "link": "http://arxiv.org/abs/1806.05236", "authors": [{"name": "Vikas Verma", "link": "http://arxiv.org/find/stat/1/au:+Verma_V/0/1/0/all/0/1"}, {"name": "Alex Lamb", "link": "http://arxiv.org/find/stat/1/au:+Lamb_A/0/1/0/all/0/1"}, {"name": "Christopher Beckham", "link": "http://arxiv.org/find/stat/1/au:+Beckham_C/0/1/0/all/0/1"}, {"name": "Aaron Courville", "link": "http://arxiv.org/find/stat/1/au:+Courville_A/0/1/0/all/0/1"}, {"name": "Ioannis Mitliagkis", "link": "http://arxiv.org/find/stat/1/au:+Mitliagkis_I/0/1/0/all/0/1"}, {"name": "Yoshua Bengio", "link": "http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "What About Applied Fairness?. (arXiv:1806.05250v1 [cs.AI])", "description": "Machine learning practitioners are often ambivalent about the ethical aspects of their products. We believe anything that gets us from that current state to one in which our systems are achieving some degree of fairness is an improvement that should be welcomed. This is true even when that progress does not get us 100% of the way to the goal of \"complete\" fairness or perfectly align with our personal belief on which measure of fairness is used. Some measure of fairness being built would still put us in a better position than the status quo. Impediments to getting fairness and ethical concerns applied in real applications, whether they are abstruse philosophical debates or technical overhead such as the introduction of ever more hyper-parameters, should be avoided. In this paper we further elaborate on our argument for this viewpoint and its importance. ", "link": "http://arxiv.org/abs/1806.05250", "authors": [{"name": "Jared Sylvester", "link": "http://arxiv.org/find/cs/1/au:+Sylvester_J/0/1/0/all/0/1"}, {"name": "Edward Raff", "link": "http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Benchmarks for Image Classification and Other High-dimensional Pattern Recognition Problems. (arXiv:1806.05272v1 [stat.ML])", "description": "A good classification method should yield more accurate results than simple heuristics. But there are classification problems, especially high-dimensional ones like the ones based on image/video data, for which simple heuristics can work quite accurately; the structure of the data in such problems is easy to uncover without any sophisticated or computationally expensive method. On the other hand, some problems have a structure that can only be found with sophisticated pattern recognition methods. We are interested in quantifying the difficulty of a given high-dimensional pattern recognition problem. We consider the case where the patterns come from two pre-determined classes and where the objects are represented by points in a high-dimensional vector space. However, the framework we propose is extendable to an arbitrarily large number of classes. We propose classification benchmarks based on simple random projection heuristics. Our benchmarks are 2D curves parameterized by the classification error and computational cost of these simple heuristics. Each curve divides the plane into a \"positive- gain\" and a \"negative-gain\" region. The latter contains methods that are ill-suited for the given classification problem. The former is divided into two by the curve asymptote; methods that lie in the small region under the curve but right of the asymptote merely provide a computational gain but no structural advantage over the random heuristics. We prove that the curve asymptotes are optimal (i.e. at Bayes error) in some cases, and thus no sophisticated method can provide a structural advantage over the random heuristics. Such classification problems, an example of which we present in our numerical experiments, provide poor ground for testing new pattern classification methods. ", "link": "http://arxiv.org/abs/1806.05272", "authors": [{"name": "Tarun Yellamraju", "link": "http://arxiv.org/find/stat/1/au:+Yellamraju_T/0/1/0/all/0/1"}, {"name": "Jonas Hepp", "link": "http://arxiv.org/find/stat/1/au:+Hepp_J/0/1/0/all/0/1"}, {"name": "Mireille Boutin", "link": "http://arxiv.org/find/stat/1/au:+Boutin_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "A theory of maximum likelihood for weighted infection graphs. (arXiv:1806.05273v1 [math.ST])", "description": "We study the problem of parameter estimation based on infection data from an epidemic outbreak on a graph. We assume that successive infections occur via contagion; i.e., transmissions can only spread across existing directed edges in the graph. Our stochastic spreading model allows individual nodes to be infected more than once, and the probability of the transmission spreading across a particular edge is proportional to both the cumulative number of times the source nodes has been infected in previous stages of the epidemic and the weight parameter of the edge. We propose a maximum likelihood estimator for inferring the unknown edge weights when full information is available concerning the order and identity of successive edge transmissions. When the weights take a particular form as exponential functions of a linear combination of known edge covariates, we show that maximum likelihood estimation amounts to optimizing a convex function, and produces a solution that is both consistent and asymptotically normal. Our proofs are based on martingale convergence theorems and the theory of weighted P\\'{o}lya urns. We also show how our theory may be generalized to settings where the weights are not exponential. Finally, we analyze the case where the available infection data comes in the form of an unordered set of edge transmissions. We propose two algorithms for weight parameter estimation in this setting and derive corresponding theoretical guarantees. Our methods are validated using both synthetic data and real-world data from the Ebola spread in West Africa. ", "link": "http://arxiv.org/abs/1806.05273", "authors": [{"name": "Justin Khim", "link": "http://arxiv.org/find/math/1/au:+Khim_J/0/1/0/all/0/1"}, {"name": "Po-Ling Loh", "link": "http://arxiv.org/find/math/1/au:+Loh_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Full Bayesian Modeling for fMRI Group Analysis. (arXiv:1806.05281v1 [stat.AP])", "description": "Functional magnetic resonance imaging or functional MRI (fMRI) is a non-invasive way to assess brain activity by detecting changes associated with blood flow. In this work, we propose a full Bayesian procedure to analyze fMRI data for individual and group stages. For the individual stage we use a multivariate dynamic linear model (MDLM), where the temporal dependence is modeled through the state parameters and the spatial dependence is modeled only locally, taking the nearest neighbors of each voxel location. For the group stage we take advantage of the posterior distribution of the state parameters obtained in the individual stage and create a new posterior distribution that represents the updated beliefs for the group analysis. Since the posterior distribution for the state parameters is indexed by the time $t$, we propose an algorithm that allows on-line estimated curves of the state parameters to be drawn and posterior probabilities computed in order to assess brain activation for both individual and group analysis. We propose an alternative analysis for the group stage using a Gaussian process ANOVA model, where the on-line estimated curves obtained in the individual stage are modeled as a functional response. Finally, we assess our proposed modeling procedure using real resting-state data and computing empirical false-positive brain activation rates. ", "link": "http://arxiv.org/abs/1806.05281", "authors": [{"name": "Johnatan Cardona Jim&#xe9;nez", "link": "http://arxiv.org/find/stat/1/au:+Jimenez_J/0/1/0/all/0/1"}, {"name": "Carlos Alberto de Bragan&#xe7;a Pereira", "link": "http://arxiv.org/find/stat/1/au:+Pereira_C/0/1/0/all/0/1"}, {"name": "Victor Fossaluza", "link": "http://arxiv.org/find/stat/1/au:+Fossaluza_V/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Asymptotic distribution of least square estimators for linear models with dependent errors. (arXiv:1806.05287v1 [math.ST])", "description": "In this paper, we consider the usual linear regression model in the case where the error process is assumed strictly stationary. We use a result from Hannan (1973), who proved a Central Limit Theorem for the usual least square estimator under general conditions on the design and on the error process. Whatever the design satisfying Hannan's conditions, we define an estimator of the covariance matrix and we prove its consistency under very mild conditions. As an application, we show how to modify the usual tests on the linear model in this dependent context, in such a way that the type-I error rate remains asymptotically correct, and we illustrate the performance of this procedure through different sets of simulations. ", "link": "http://arxiv.org/abs/1806.05287", "authors": [{"name": "Emmanuel Caron", "link": "http://arxiv.org/find/math/1/au:+Caron_E/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Pattern Dependence Detection using n-TARP Clustering. (arXiv:1806.05297v1 [stat.ML])", "description": "Consider an experiment involving a potentially small number of subjects. Some random variables are observed on each subject: a high-dimensional one called the \"observed\" random variable, and a one-dimensional one called the \"outcome\" random variable. We are interested in the dependencies between the observed random variable and the outcome random variable. We propose a method to quantify and validate the dependencies of the outcome random variable on the various patterns contained in the observed random variable. Different degrees of relationship are explored (linear, quadratic, cubic, ...). This work is motivated by the need to analyze educational data, which often involves high-dimensional data representing a small number of students. Thus our implementation is designed for a small number of subjects; however, it can be easily modified to handle a very large dataset. As an illustration, the proposed method is used to study the influence of certain skills on the course grade of students in a signal processing class. A valid dependency of the grade on the different skill patterns is observed in the data. ", "link": "http://arxiv.org/abs/1806.05297", "authors": [{"name": "Tarun Yellamraju", "link": "http://arxiv.org/find/stat/1/au:+Yellamraju_T/0/1/0/all/0/1"}, {"name": "Mireille Boutin", "link": "http://arxiv.org/find/stat/1/au:+Boutin_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Deep Reinforcement Learning for Dynamic Urban Transportation Problems. (arXiv:1806.05310v1 [stat.ML])", "description": "We explore the use of deep learning and deep reinforcement learning for optimization problems in transportation. Many transportation system analysis tasks are formulated as an optimization problem - such as optimal control problems in intelligent transportation systems and long term urban planning. Often transportation models used to represent dynamics of a transportation system involve large data sets with complex input-output interactions and are difficult to use in the context of optimization. Use of deep learning metamodels can produce a lower dimensional representation of those relations and allow to implement optimization and reinforcement learning algorithms in an efficient manner. In particular, we develop deep learning models for calibrating transportation simulators and for reinforcement learning to solve the problem of optimal scheduling of travelers on the network. ", "link": "http://arxiv.org/abs/1806.05310", "authors": [{"name": "Laura Schultz", "link": "http://arxiv.org/find/stat/1/au:+Schultz_L/0/1/0/all/0/1"}, {"name": "Vadim Sokolov", "link": "http://arxiv.org/find/stat/1/au:+Sokolov_V/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Hierarchical interpretations for neural network predictions. (arXiv:1806.05337v1 [cs.LG])", "description": "Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise. ", "link": "http://arxiv.org/abs/1806.05337", "authors": [{"name": "Chandan Singh", "link": "http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1"}, {"name": "W. James Murdoch", "link": "http://arxiv.org/find/cs/1/au:+Murdoch_W/0/1/0/all/0/1"}, {"name": "Bin Yu", "link": "http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Scalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization. (arXiv:1806.05355v1 [stat.ML])", "description": "We propose a simple and easy to implement neural network compression algorithm that achieves results competitive with more complicated state-of-the-art methods. The key idea is to modify the original optimization problem by adding K independent Gaussian priors (corresponding to the k-means objective) over the network parameters to achieve parameter quantization, as well as an L1 penalty to achieve pruning. Unlike many existing quantization-based methods, our method uses hard clustering assignments of network parameters, which adds minimal change or overhead to standard network training. We also demonstrate experimentally that tying neural network parameters provides less gain in generalization performance than changing network architecture and connectivity patterns entirely. ", "link": "http://arxiv.org/abs/1806.05355", "authors": [{"name": "Yibo Yang", "link": "http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1"}, {"name": "Nicholas Ruozzi", "link": "http://arxiv.org/find/stat/1/au:+Ruozzi_N/0/1/0/all/0/1"}, {"name": "Vibhav Gogate", "link": "http://arxiv.org/find/stat/1/au:+Gogate_V/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Finding GEMS: Multi-Scale Dictionaries for High-Dimensional Graph Signals. (arXiv:1806.05356v1 [cs.LG])", "description": "Modern data introduces new challenges to classic signal processing approaches, leading to a growing interest in the field of graph signal processing. A powerful and well established model for real world signals in various domains is sparse representation over a dictionary, combined with the ability to train the dictionary from signal examples. This model has been successfully applied to graph signals as well by integrating the underlying graph topology into the learned dictionary. Nonetheless, dictionary learning methods for graph signals are typically restricted to small dimensions due to the computational constraints that the dictionary learning problem entails, and due to the direct use of the graph Laplacian matrix. In this paper, we propose a dictionary learning algorithm that applies to a broader class of graph signals, and is capable of handling much higher dimensional data. We incorporate the underlying graph topology both implicitly, by forcing the learned dictionary atoms to be sparse combinations of graph-wavelet functions, and explicitly, by adding direct graph constraints to promote smoothness in both the feature and manifold domains. The resulting atoms are thus adapted to the data of interest while adhering to the underlying graph structure and possessing a desired multi-scale property. Experimental results on several datasets, representing both synthetic and real network data of different nature, demonstrate the effectiveness of the proposed algorithm for graph signal processing even in high dimensions. ", "link": "http://arxiv.org/abs/1806.05356", "authors": [{"name": "Yael Yankelevsky", "link": "http://arxiv.org/find/cs/1/au:+Yankelevsky_Y/0/1/0/all/0/1"}, {"name": "Michael Elad", "link": "http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Deep Multi-Output Forecasting: Learning to Accurately Predict Blood Glucose Trajectories. (arXiv:1806.05357v1 [cs.LG])", "description": "In many forecasting applications, it is valuable to predict not only the value of a signal at a certain time point in the future, but also the values leading up to that point. This is especially true in clinical applications, where the future state of the patient can be less important than the patient's overall trajectory. This requires multi-step forecasting, a forecasting variant where one aims to predict multiple values in the future simultaneously. Standard methods to accomplish this can propagate error from prediction to prediction, reducing quality over the long term. In light of these challenges, we propose multi-output deep architectures for multi-step forecasting in which we explicitly model the distribution of future values of the signal over a prediction horizon. We apply these techniques to the challenging and clinically relevant task of blood glucose forecasting. Through a series of experiments on a real-world dataset consisting of 550K blood glucose measurements, we demonstrate the effectiveness of our proposed approaches in capturing the underlying signal dynamics. Compared to existing shallow and deep methods, we find that our proposed approaches improve performance individually and capture complementary information, leading to a large improvement over the baseline when combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, the results suggest the efficacy of our proposed approach in predicting blood glucose level and multi-step forecasting more generally. ", "link": "http://arxiv.org/abs/1806.05357", "authors": [{"name": "Ian Fox", "link": "http://arxiv.org/find/cs/1/au:+Fox_I/0/1/0/all/0/1"}, {"name": "Lynn Ang", "link": "http://arxiv.org/find/cs/1/au:+Ang_L/0/1/0/all/0/1"}, {"name": "Mamta Jaiswal", "link": "http://arxiv.org/find/cs/1/au:+Jaiswal_M/0/1/0/all/0/1"}, {"name": "Rodica Pop-Busui", "link": "http://arxiv.org/find/cs/1/au:+Pop_Busui_R/0/1/0/all/0/1"}, {"name": "Jenna Wiens", "link": "http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning. (arXiv:1806.05358v1 [cs.LG])", "description": "In this paper, we study robust large-scale distributed learning in the presence of saddle points in non-convex loss functions. We consider the Byzantine setting where some worker machines may have abnormal or even arbitrary and adversarial behavior. We argue that in the Byzantine setting, optimizing a non-convex function and escaping saddle points become much more challenging, even when robust gradient estimators are used. We develop ByzantinePGD, a robust and communication-efficient algorithm that can provably escape saddle points and converge to approximate local minimizers. The iteration complexity of our algorithm in the Byzantine setting matches that of standard gradient descent in the usual setting. We further provide three robust aggregation subroutines that can be used in ByzantinePGD, including median, trimmed mean, and iterative filtering. We characterize their performance in statistical settings, and argue for their near-optimality in different regimes including the high dimensional setting. ", "link": "http://arxiv.org/abs/1806.05358", "authors": [{"name": "Dong Yin", "link": "http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"}, {"name": "Yudong Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"}, {"name": "Kannan Ramchandran", "link": "http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1"}, {"name": "Peter Bartlett", "link": "http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Financial Forecasting and Analysis for Low-Wage Workers. (arXiv:1806.05362v1 [stat.AP])", "description": "Despite the plethora of financial services and products on the market nowadays, there is a lack of such services and products designed especially for the low-wage population. Approximately 30% of the U.S. working population engage in low-wage work, and many of them lead a paycheck-to-paycheck lifestyle. Financial planning advice needs to explicitly address their financial instability. ", "link": "http://arxiv.org/abs/1806.05362", "authors": [{"name": "Wenyu Zhang", "link": "http://arxiv.org/find/stat/1/au:+Zhang_W/0/1/0/all/0/1"}, {"name": "Raya Horesh", "link": "http://arxiv.org/find/stat/1/au:+Horesh_R/0/1/0/all/0/1"}, {"name": "Karthikeyan N. Ramamurthy", "link": "http://arxiv.org/find/stat/1/au:+Ramamurthy_K/0/1/0/all/0/1"}, {"name": "Lingfei Wu", "link": "http://arxiv.org/find/stat/1/au:+Wu_L/0/1/0/all/0/1"}, {"name": "Jinfeng Yi", "link": "http://arxiv.org/find/stat/1/au:+Yi_J/0/1/0/all/0/1"}, {"name": "Kryn Anderson", "link": "http://arxiv.org/find/stat/1/au:+Anderson_K/0/1/0/all/0/1"}, {"name": "Kush R. Varshney", "link": "http://arxiv.org/find/stat/1/au:+Varshney_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "On the heavy-tail behavior of the distributionally robust newsvendor. (arXiv:1806.05379v1 [math.ST])", "description": "Since the seminal work of Scarf (1958) [A min-max solution of an inventory problem, Studies in the Mathematical Theory of Inventory and Production, pages 201-209] on the newsvendor problem with ambiguity in the demand distribution, there has been a growing interest in the study of the distributionally robust newsvendor problem. The optimal order quantity is computed by accounting for the worst possible distribution from a set of demand distributions that is characterized by partial information, such as moments. The model is criticized at times for being overly conservative since the worst-case distribution is discrete with a few support points. However, it is the order quantity from the model that is typically of practical relevance. A simple observation shows that the optimal order quantity in Scarf's model with known first and second moment is also optimal for a heavy-tailed censored student-t distribution with degrees of freedom 2. In this paper, we generalize this \"heavy-tail optimality\" property of the distributionally robust newsvendor to a more general ambiguity set where information on the first and the $n$th moment is known, for any real number $n > 1$. We provide a characterization of the optimal order quantity under this ambiguity set by showing that for high critical ratios, the order quantity is optimal for a regularly varying distribution with an approximate power law tail with tail index $n$. We illustrate the applicability of the model by calibrating the ambiguity set from data and comparing the performance of the order quantities computed via various methods in a dataset. ", "link": "http://arxiv.org/abs/1806.05379", "authors": [{"name": "Bikramjit Das", "link": "http://arxiv.org/find/math/1/au:+Das_B/0/1/0/all/0/1"}, {"name": "Anulekha Dhara", "link": "http://arxiv.org/find/math/1/au:+Dhara_A/0/1/0/all/0/1"}, {"name": "Karthik Natarjan", "link": "http://arxiv.org/find/math/1/au:+Natarjan_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "PCAS: Pruning Channels with Attention Statistics. (arXiv:1806.05382v1 [stat.ML])", "description": "To implement deep neural networks on small embedded devices, conventional techniques use channel pruning looking considering manual compression rate per layer to reduce parameters. Besides it is difficult to consider the relationships between layers and it takes a lot of time for deeper models. For addressing these issues, we propose a new channel pruning technique based on attention that can evaluate the importance of channels. We improved the method with the criterion to allow the automatic channel selection using a single compression rate for the entire model. Experimental results showed that a parameter reduction of 90.8% and FLOPs reduction of 79.4% was achieved with an accuracy degradation of around 1% for the compressed ResNet-50 model on the CIFAR-10 benchmark. ", "link": "http://arxiv.org/abs/1806.05382", "authors": [{"name": "Kohei Yamamoto", "link": "http://arxiv.org/find/stat/1/au:+Yamamoto_K/0/1/0/all/0/1"}, {"name": "Kurato Maeno", "link": "http://arxiv.org/find/stat/1/au:+Maeno_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Parameter Learning and Change Detection Using a Particle Filter With Accelerated Adaptation. (arXiv:1806.05387v1 [stat.ML])", "description": "This paper presents the construction of a particle filter, which incorporates elements inspired by genetic algorithms, in order to achieve accelerated adaptation of the estimated posterior distribution to changes in model parameters. Specifically, the filter is designed for the situation where the subsequent data in online sequential filtering does not match the model posterior filtered based on data up to a current point in time. The examples considered encompass parameter regime shifts and stochastic volatility. The filter adapts to regime shifts extremely rapidly and delivers a clear heuristic for distinguishing between regime shifts and stochastic volatility, even though the model dynamics assumed by the filter exhibit neither of those features. ", "link": "http://arxiv.org/abs/1806.05387", "authors": [{"name": "Karol Gellert", "link": "http://arxiv.org/find/stat/1/au:+Gellert_K/0/1/0/all/0/1"}, {"name": "Erik Schl&#xf6;gl", "link": "http://arxiv.org/find/stat/1/au:+Schlogl_E/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks. (arXiv:1806.05393v1 [stat.ML])", "description": "In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures. ", "link": "http://arxiv.org/abs/1806.05393", "authors": [{"name": "Lechao Xiao", "link": "http://arxiv.org/find/stat/1/au:+Xiao_L/0/1/0/all/0/1"}, {"name": "Yasaman Bahri", "link": "http://arxiv.org/find/stat/1/au:+Bahri_Y/0/1/0/all/0/1"}, {"name": "Jascha Sohl-Dickstein", "link": "http://arxiv.org/find/stat/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1"}, {"name": "Samuel S. Schoenholz", "link": "http://arxiv.org/find/stat/1/au:+Schoenholz_S/0/1/0/all/0/1"}, {"name": "Jeffrey Pennington", "link": "http://arxiv.org/find/stat/1/au:+Pennington_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks. (arXiv:1806.05394v1 [stat.ML])", "description": "Recurrent neural networks have gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an input. We show that this theory predicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings. Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly improvement in training dynamics. Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task. ", "link": "http://arxiv.org/abs/1806.05394", "authors": [{"name": "Minmin Chen", "link": "http://arxiv.org/find/stat/1/au:+Chen_M/0/1/0/all/0/1"}, {"name": "Jeffrey Pennington", "link": "http://arxiv.org/find/stat/1/au:+Pennington_J/0/1/0/all/0/1"}, {"name": "Samuel S. Schoenholz", "link": "http://arxiv.org/find/stat/1/au:+Schoenholz_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "On the Perceptron's Compression. (arXiv:1806.05403v1 [cs.LG])", "description": "We study and provide exposition to several phenomena that are related to the perceptron's compression. One theme concerns modifications of the perceptron algorithm that yield better guarantees on the margin of the hyperplane it outputs. These modifications can be useful in training neural networks as well, and we demonstrate them with some experimental data. In a second theme, we deduce conclusions from the perceptron's compression in various contexts. ", "link": "http://arxiv.org/abs/1806.05403", "authors": [{"name": "Shay Moran", "link": "http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1"}, {"name": "Ido Nachum", "link": "http://arxiv.org/find/cs/1/au:+Nachum_I/0/1/0/all/0/1"}, {"name": "Itai Panasoff", "link": "http://arxiv.org/find/cs/1/au:+Panasoff_I/0/1/0/all/0/1"}, {"name": "Amir Yehudayoff", "link": "http://arxiv.org/find/cs/1/au:+Yehudayoff_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Learning Dynamics of Linear Denoising Autoencoders. (arXiv:1806.05413v1 [stat.ML])", "description": "Denoising autoencoders (DAEs) have proven useful for unsupervised representation learning, but a thorough theoretical understanding is still lacking of how the input noise influences learning. Here we develop theory for how noise influences learning in DAEs. By focusing on linear DAEs, we are able to derive analytic expressions that exactly describe their learning dynamics. We verify our theoretical predictions with simulations as well as experiments on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise allows DAEs to ignore low variance directions in the inputs while learning to reconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs to standard regularised autoencoders, we show that noise has a similar regularisation effect to weight decay, but with faster training dynamics. We also show that our theoretical predictions approximate learning dynamics on real-world data and qualitatively match observed dynamics in nonlinear DAEs. ", "link": "http://arxiv.org/abs/1806.05413", "authors": [{"name": "Arnu Pretorius", "link": "http://arxiv.org/find/stat/1/au:+Pretorius_A/0/1/0/all/0/1"}, {"name": "Steve Kroon", "link": "http://arxiv.org/find/stat/1/au:+Kroon_S/0/1/0/all/0/1"}, {"name": "Herman Kamper", "link": "http://arxiv.org/find/stat/1/au:+Kamper_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Ranking Recovery from Limited Comparisons using Low-Rank Matrix Completion. (arXiv:1806.05419v1 [stat.ML])", "description": "This paper proposes a new method for solving the well-known rank aggregation problem from pairwise comparisons using the method of low-rank matrix completion. The partial and noisy data of pairwise comparisons is transformed into a matrix form. We then use tools from matrix completion, which has served as a major component in the low-rank completion solution of the Netflix challenge, to construct the preference of the different objects. In our approach, the data of multiple comparisons is used to create an estimate of the probability of object i to win (or be chosen) over object j, where only a partial set of comparisons between N objects is known. The data is then transformed into a matrix form for which the noiseless solution has a known rank of one. An alternating minimization algorithm, in which the target matrix takes a bilinear form, is then used in combination with maximum likelihood estimation for both factors. The reconstructed matrix is used to obtain the true underlying preference intensity. This work demonstrates the improvement of our proposed algorithm over the current state-of-the-art in both simulated scenarios and real data. ", "link": "http://arxiv.org/abs/1806.05419", "authors": [{"name": "Tal Levy", "link": "http://arxiv.org/find/stat/1/au:+Levy_T/0/1/0/all/0/1"}, {"name": "Alireza Vahid", "link": "http://arxiv.org/find/stat/1/au:+Vahid_A/0/1/0/all/0/1"}, {"name": "Raja Giryes", "link": "http://arxiv.org/find/stat/1/au:+Giryes_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Selfless Sequential Learning. (arXiv:1806.05421v1 [stat.ML])", "description": "Sequential learning studies the problem of learning tasks in a sequence with restricted access to only the data of the current task. In the setting with a fixed model capacity, the learning process should not be selfish and account for later tasks to be added and therefore aim at utilizing a minimum number of neurons, leaving enough capacity for future needs. We explore different regularization strategies and activation functions that could lead to less interference between the different tasks. We show that learning a sparse representation is more beneficial for sequential learning than encouraging parameter sparsity regardless of their corresponding neurons. We particularly propose a novel regularizer that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. We combine our regularizer with state-of-the-art lifelong learning methods that penalize changes on important previously learned parts of the network. We show that increased sparsity translates in a performance improvement on the different tasks that are learned in a sequence. ", "link": "http://arxiv.org/abs/1806.05421", "authors": [{"name": "Rahaf Aljundi", "link": "http://arxiv.org/find/stat/1/au:+Aljundi_R/0/1/0/all/0/1"}, {"name": "Marcus Rohrbach", "link": "http://arxiv.org/find/stat/1/au:+Rohrbach_M/0/1/0/all/0/1"}, {"name": "Tinne Tuytelaars", "link": "http://arxiv.org/find/stat/1/au:+Tuytelaars_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Sequential Bayesian inference for spatio-temporal models of temperature and humidity data. (arXiv:1806.05424v1 [stat.AP])", "description": "We develop a spatio-temporal model to forecast sensor output at five locations in North East England. The signal is described using coupled dynamic linear models, with spatial effects specified by a Gaussian process. Data streams are analysed using a stochastic algorithm which sequentially approximates the parameter posterior through a series of reweighting and resampling steps. An iterated batch importance sampling scheme is used to circumvent particle degeneracy through a resample-move step. The algorithm is modified to make it more efficient and parallisable. The model is shown to give a good description of the underlying process and provide reasonable forecast accuracy. ", "link": "http://arxiv.org/abs/1806.05424", "authors": [{"name": "Yingying Lai", "link": "http://arxiv.org/find/stat/1/au:+Lai_Y/0/1/0/all/0/1"}, {"name": "Andrew Golightly", "link": "http://arxiv.org/find/stat/1/au:+Golightly_A/0/1/0/all/0/1"}, {"name": "Richard Boys", "link": "http://arxiv.org/find/stat/1/au:+Boys_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Improving precipitation forecast using extreme quantile regression. (arXiv:1806.05429v1 [stat.ME])", "description": "Aiming to predict extreme precipitation forecast quantiles, we propose a nonparametric regression model that features a constant extreme value index. Using local linear quantile regression and an extrapolation technique from extreme value theory, we develop an estimator for conditional quantiles corresponding to extreme high probability levels. We establish uniform consistency and asymptotic normality of the estimators. In a simulation study, we examine the performance of our estimator on finite samples in comparison with existing methods. On a precipitation data set in the Netherlands, our estimators have more predictive power compared to the upper member of ensemble forecasts provided by a numerical weather predication model. ", "link": "http://arxiv.org/abs/1806.05429", "authors": [{"name": "Jasper Velthoen", "link": "http://arxiv.org/find/stat/1/au:+Velthoen_J/0/1/0/all/0/1"}, {"name": "Juan-Juan Cai", "link": "http://arxiv.org/find/stat/1/au:+Cai_J/0/1/0/all/0/1"}, {"name": "Geurt Jongbloed", "link": "http://arxiv.org/find/stat/1/au:+Jongbloed_G/0/1/0/all/0/1"}, {"name": "Maurice Schmeits", "link": "http://arxiv.org/find/stat/1/au:+Schmeits_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "ServeNet: A Deep Neural Network for Web Service Classification. (arXiv:1806.05437v1 [cs.LG])", "description": "Automated service classification plays a crucial role in service management such as service discovery, selection, and composition. In recent years, machine learning techniques have been used for service classification. However, they can only predict around 10 to 20 service categories due to the quality of feature engineering and the imbalance problem of service dataset. In this paper, we present a deep neural network ServeNet with a novel dataset splitting algorithm to deal with these issues. ServeNet can automatically abstract low-level representation to high-level features, and then predict service classification based on the service datasets produced by the proposed splitting algorithm. To demonstrate the effectiveness of our approach, we conducted a comprehensive experimental study on 10,000 real-world services in 50 categories. The result shows that ServeNet can achieve higher accuracy than other machine learning methods. ", "link": "http://arxiv.org/abs/1806.05437", "authors": [{"name": "Yilong Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"}, {"name": "Peng Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"}, {"name": "Lianchao Ding", "link": "http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"}, {"name": "Bingqing Shen", "link": "http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"}, {"name": "Weiru Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors. (arXiv:1806.05438v1 [stat.ML])", "description": "We consider stochastic gradient descent for binary classification problems in a reproducing kernel Hilbert space. In traditional analysis, it is known that the expected classification error converges more slowly than the expected risk even when assuming a low-noise condition on the conditional label probabilities. Consequently, the resulting rate is sublinear. Therefore, it is important to consider whether much faster convergence of the expected classification error can be achieved. In recent research, an exponential convergence rate for stochastic gradient descent was shown under a strong low-noise condition, but theoretical analysis of this was limited to the square loss function, which is somewhat inadequate for binary classification tasks. In this paper, we show an exponential convergence rate of the expected classification error in the final phase of learning for a wide class of differentiable convex loss functions under similar assumptions. ", "link": "http://arxiv.org/abs/1806.05438", "authors": [{"name": "Atsushi Nitanda", "link": "http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1"}, {"name": "Taiji Suzuki", "link": "http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Asymptotic maximal order statistic for SIR in $\\kappa-\\mu$ shadowed fading. (arXiv:1806.05450v1 [cs.IT])", "description": "Using tools from extreme value theory (EVT), it is proved that the limiting distribution of the maximum of L independent and identically distributed (i.i.d.) signal-to-interference ratio (SIR) random variables (RVs) is a Frechet distribution, when the user and the interferer signals undergo independent and non-identically distributed (i.n.i.d.) $\\kappa-\\mu$ shadowed fading. This limiting distribution is used to analyze the outage probability for selection combining (SC). Further, the moments of the maximum is shown to converge to the moments of the Frechet RV. This is used in deriving results for the asymptotic rate for SC. Finally, the rate of convergence of the actual maximum distribution to the Frechet distribution is derived and is analyzed for different $\\kappa$ and $\\mu$ parameters. Further, results from stochastic ordering are used to analyze the variations in the limiting distribution with respect to variations in the source fading parameters. A close match is observed between Monte-Carlo simulations and the limiting distributions for outage probability and rate. ", "link": "http://arxiv.org/abs/1806.05450", "authors": [{"name": "Athira Subhash", "link": "http://arxiv.org/find/cs/1/au:+Subhash_A/0/1/0/all/0/1"}, {"name": "Muralikrishnan Srinivasan", "link": "http://arxiv.org/find/cs/1/au:+Srinivasan_M/0/1/0/all/0/1"}, {"name": "Sheetal Kalyani", "link": "http://arxiv.org/find/cs/1/au:+Kalyani_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "The committee machine: Computational to statistical gaps in learning a two-layers neural network. (arXiv:1806.05451v1 [cs.LG])", "description": "Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it, strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap. ", "link": "http://arxiv.org/abs/1806.05451", "authors": [{"name": "Benjamin Aubin", "link": "http://arxiv.org/find/cs/1/au:+Aubin_B/0/1/0/all/0/1"}, {"name": "Antoine Maillard", "link": "http://arxiv.org/find/cs/1/au:+Maillard_A/0/1/0/all/0/1"}, {"name": "Jean Barbier", "link": "http://arxiv.org/find/cs/1/au:+Barbier_J/0/1/0/all/0/1"}, {"name": "Florent Krzakala", "link": "http://arxiv.org/find/cs/1/au:+Krzakala_F/0/1/0/all/0/1"}, {"name": "Nicolas Macris", "link": "http://arxiv.org/find/cs/1/au:+Macris_N/0/1/0/all/0/1"}, {"name": "Lenka Zdeborov&#xe1;", "link": "http://arxiv.org/find/cs/1/au:+Zdeborova_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Low-rank geometric mean metric learning. (arXiv:1806.05454v1 [cs.LG])", "description": "We propose a low-rank approach to learning a Mahalanobis metric from data. Inspired by the recent geometric mean metric learning (GMML) algorithm, we propose a low-rank variant of the algorithm. This allows to jointly learn a low-dimensional subspace where the data reside and the Mahalanobis metric that appropriately fits the data. Our results show that we compete effectively with GMML at lower ranks. ", "link": "http://arxiv.org/abs/1806.05454", "authors": [{"name": "Mukul Bhutani", "link": "http://arxiv.org/find/cs/1/au:+Bhutani_M/0/1/0/all/0/1"}, {"name": "Pratik Jawanpuria", "link": "http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1"}, {"name": "Hiroyuki Kasai", "link": "http://arxiv.org/find/cs/1/au:+Kasai_H/0/1/0/all/0/1"}, {"name": "Bamdev Mishra", "link": "http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Regression with Functional Errors-in-Predictors: A Generalized Method-of-Moments Approach. (arXiv:1806.05471v1 [stat.ME])", "description": "Functional regression is an important topic in functional data analysis. Traditionally, one often assumes that samples of the functional predictor are independent realizations of an underlying stochastic process, and are observed over a grid of points contaminated by independent and identically distributed measurement errors. In practice, however, the dynamical dependence across different curves may exist and the parametric assumption on the measurement error covariance structure could be unrealistic. In this paper, we consider functional linear regression with serially dependent functional predictors, when the contamination of predictors by the measurement error is \"genuinely functional\" with fully nonparametric covariance structure. Inspired by the fact that the autocovariance operator of observed functional predictors automatically filters out the impact from the unobservable measurement error, we propose a novel autocovariance-based generalized method-of-moments estimate of the slope parameter. The asymptotic properties of the resulting estimators under different functional scenarios are established. We also demonstrate that our proposed method significantly outperforms possible competitors through intensive simulation studies. Finally, the proposed method is applied to a public financial dataset, revealing some interesting findings. ", "link": "http://arxiv.org/abs/1806.05471", "authors": [{"name": "Xinghao Qiao", "link": "http://arxiv.org/find/stat/1/au:+Qiao_X/0/1/0/all/0/1"}, {"name": "Cheng Chen", "link": "http://arxiv.org/find/stat/1/au:+Chen_C/0/1/0/all/0/1"}, {"name": "Shaojun Guo", "link": "http://arxiv.org/find/stat/1/au:+Guo_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data. (arXiv:1806.05476v1 [cs.CV])", "description": "In the past few years, Convolutional Neural Networks (CNNs) have been achieving state-of-the-art performance on a variety of problems. Many companies employ resources and money to generate these models and provide them as an API, therefore it is in their best interest to protect them, i.e., to avoid that someone else copies them. Recent studies revealed that state-of-the-art CNNs are vulnerable to adversarial examples attacks, and this weakness indicates that CNNs do not need to operate in the problem domain (PD). Therefore, we hypothesize that they also do not need to be trained with examples of the PD in order to operate in it. ", "link": "http://arxiv.org/abs/1806.05476", "authors": [{"name": "Jacson Rodrigues Correia-Silva", "link": "http://arxiv.org/find/cs/1/au:+Correia_Silva_J/0/1/0/all/0/1"}, {"name": "Rodrigo F. Berriel", "link": "http://arxiv.org/find/cs/1/au:+Berriel_R/0/1/0/all/0/1"}, {"name": "Claudine Badue", "link": "http://arxiv.org/find/cs/1/au:+Badue_C/0/1/0/all/0/1"}, {"name": "Alberto F. de Souza", "link": "http://arxiv.org/find/cs/1/au:+Souza_A/0/1/0/all/0/1"}, {"name": "Thiago Oliveira-Santos", "link": "http://arxiv.org/find/cs/1/au:+Oliveira_Santos_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:1806.05490v1 [stat.ML])", "description": "Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Pro- cesses that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to directly sample from it. To efficiently optimize the hyperparameters, we intro- duce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs. ", "link": "http://arxiv.org/abs/1806.05490", "authors": [{"name": "Marton Havasi", "link": "http://arxiv.org/find/stat/1/au:+Havasi_M/0/1/0/all/0/1"}, {"name": "Jos&#xe9; Miguel Hern&#xe1;ndez Lobato", "link": "http://arxiv.org/find/stat/1/au:+Lobato_J/0/1/0/all/0/1"}, {"name": "Juan Jos&#xe9; Murillo Fuentes", "link": "http://arxiv.org/find/stat/1/au:+Fuentes_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "On the ranking of Test match batsmen. (arXiv:1806.05496v1 [stat.AP])", "description": "Ranking sportsmen whose careers took place in different eras is often a contentious issue and the topic of much debate. In this paper we focus on cricket and examine what conclusions may be drawn about the ranking of Test batsmen using data on batting scores from the first Test in 1877 onwards. The overlapping nature of playing careers is exploited to form a bridge from past to present so that all players can be compared simultaneously, rather than just relative to their contemporaries. The natural variation in runs scored by a batsman is modelled by an additive log-linear model with year, age and cricket-specific components used to extract the innate ability of an individual cricketer. Incomplete innings are handled via censoring and a zero-inflated component is incorporated into the model to allow for an excess of frailty at the start of an innings. The innings-by-innings variation of runs scored by each batsman leads to uncertainty in their ranking position. A Bayesian approach is used to fit the model and realisations from the posterior distribution are obtained by deploying a Markov Chain Monte Carlo algorithm. Posterior summaries of innate player ability are then used to assess uncertainty in ranking position and this is contrasted with rankings determined via the posterior mean runs scored. Posterior predictive checks show that the model provides a reasonably accurate description of runs scored. ", "link": "http://arxiv.org/abs/1806.05496", "authors": [{"name": "Richard J. Boys", "link": "http://arxiv.org/find/stat/1/au:+Boys_R/0/1/0/all/0/1"}, {"name": "Peter M. Philipson", "link": "http://arxiv.org/find/stat/1/au:+Philipson_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Statistical Aspects of Wasserstein Distances. (arXiv:1806.05500v1 [stat.ME])", "description": "Wasserstein distances are metrics on probability distributions inspired by the problem of optimal mass transportation. Roughly speaking, they measure the minimal effort required to reconfigure the probability mass of one distribution in order to recover the other distribution. They are ubiquitous in mathematics, with a long history that has seen them catalyse core developments in analysis, optimization, and probability. Beyond their intrinsic mathematical richness, they possess attractive features that make them a versatile tool for the statistician: they can be used to derive weak convergence and convergence of moments, and can be easily bounded; they are well-adapted to quantify a natural notion of perturbation of a probability distribution; and they seamlessly incorporate the geometry of the domain of the distributions in question, thus being useful for contrasting complex objects. Consequently, they frequently appear in the development of statistical theory and inferential methodology, and have recently become an object of inference in themselves. In this review, we provide a snapshot of the main concepts involved in Wasserstein distances and optimal transportation, and a succinct overview of some of their many statistical aspects. ", "link": "http://arxiv.org/abs/1806.05500", "authors": [{"name": "Victor M. Panaretos", "link": "http://arxiv.org/find/stat/1/au:+Panaretos_V/0/1/0/all/0/1"}, {"name": "Yoav Zemel", "link": "http://arxiv.org/find/stat/1/au:+Zemel_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Neural Stethoscopes: Unifying Analytic, Auxiliary and Adversarial Network Probing. (arXiv:1806.05502v1 [stat.ML])", "description": "Model interpretability and systematic, targeted model adaptation present central tenets in machine learning for addressing limited or biased datasets. In this paper, we introduce neural stethoscopes as a framework for quantifying the degree of importance of specific factors of influence in deep networks as well as for actively promoting and suppressing information as appropriate. In doing so we unify concepts from multitask learning as well as training with auxiliary and adversarial losses. We showcase the efficacy of neural stethoscopes in an intuitive physics domain. Specifically, we investigate the challenge of visually predicting stability of block towers and demonstrate that the network uses visual cues which makes it susceptible to biases in the dataset. Through the use of stethoscopes we interrogate the accessibility of specific information throughout the network stack and show that we are able to actively de-bias network predictions as well as enhance performance via suitable auxiliary and adversarial stethoscope losses. ", "link": "http://arxiv.org/abs/1806.05502", "authors": [{"name": "Fabian B. Fuchs", "link": "http://arxiv.org/find/stat/1/au:+Fuchs_F/0/1/0/all/0/1"}, {"name": "Oliver Groth", "link": "http://arxiv.org/find/stat/1/au:+Groth_O/0/1/0/all/0/1"}, {"name": "Adam R. Kosoriek", "link": "http://arxiv.org/find/stat/1/au:+Kosoriek_A/0/1/0/all/0/1"}, {"name": "Alex Bewley", "link": "http://arxiv.org/find/stat/1/au:+Bewley_A/0/1/0/all/0/1"}, {"name": "Markus Wulfmeier", "link": "http://arxiv.org/find/stat/1/au:+Wulfmeier_M/0/1/0/all/0/1"}, {"name": "Andrea Vedaldi", "link": "http://arxiv.org/find/stat/1/au:+Vedaldi_A/0/1/0/all/0/1"}, {"name": "Ingmar Posner", "link": "http://arxiv.org/find/stat/1/au:+Posner_I/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "NetScore: Towards Universal Metrics for Large-scale Performance Analysis of Deep Neural Networks for Practical Usage. (arXiv:1806.05512v1 [cs.CV])", "description": "Much of the focus in the design of deep neural networks has been on improving accuracy, leading to more powerful yet highly complex network architectures that are difficult to deploy in practical scenarios, particularly on edge devices such as mobile and other consumer devices, given their high computational and memory requirements. As a result, there has been a recent interest in the design of quantitative metrics for evaluating deep neural networks that accounts for more than just model accuracy as the sole indicator of network performance. In this study, we continue the conversation towards universal metrics for evaluating the performance of deep neural networks for practical usage. In particular, we propose a new balanced metric called NetScore, which is designed specifically to provide a quantitative assessment of the balance between accuracy, computational complexity, and network architecture complexity of a deep neural network. In what is one of the largest comparative analysis between deep neural networks in literature, the NetScore metric, the top-1 accuracy metric, and the popular information density metric were compared across a diverse set of 50 different deep convolutional neural networks for image classification on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2012) dataset. The evaluation results across these three metrics for this diverse set of networks are presented in this study to act as a reference guide for practitioners in the field. The proposed NetScore metric, along with the other tested metrics, are by no means perfect, but the hope is to push the conversation towards better universal metrics for evaluating deep neural networks for use in practical scenarios to help guide practitioners in model design. ", "link": "http://arxiv.org/abs/1806.05512", "authors": [{"name": "Alexander Wong", "link": "http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "The Exact Equivalence of Distance and Kernel Methods for Hypothesis Testing. (arXiv:1806.05514v1 [stat.ML])", "description": "Distance-based methods, also called \"energy statistics\", are leading methods for two-sample and independence tests from the statistics community. Kernel methods, developed from \"kernel mean embeddings\", are leading methods for two-sample and independence tests from the machine learning community. Previous works demonstrated the equivalence of distance and kernel methods only at the population level, for each kind of test, requiring an embedding theory of kernels. We propose a simple, bijective transformation between semimetrics and nondegenerate kernels. We prove that for finite samples, two-sample tests are special cases of independence tests, and the distance-based statistic is equivalent to the kernel-based statistic, including the biased, unbiased, and normalized versions. In other words, upon setting the kernel or metric to be bijective of each other, running any of the four algorithms will yield the exact same answer up to numerical precision. This deepens and unifies our understanding of interpoint comparison based methods. ", "link": "http://arxiv.org/abs/1806.05514", "authors": [{"name": "Cencheng Shen", "link": "http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1"}, {"name": "Joshua T. Vogelstein", "link": "http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation. (arXiv:1806.05559v1 [cs.CL])", "description": "Parallel sentence extraction is a task addressing the data sparsity problem found in multilingual natural language processing applications. We propose a bidirectional recurrent neural network based approach to extract parallel sentences from collections of multilingual texts. Our experiments with noisy parallel corpora show that we can achieve promising results against a competitive baseline by removing the need of specific feature engineering or additional external resources. To justify the utility of our approach, we extract sentence pairs from Wikipedia articles to train machine translation systems and show significant improvements in translation performance. ", "link": "http://arxiv.org/abs/1806.05559", "authors": [{"name": "Francis Gr&#xe9;goire", "link": "http://arxiv.org/find/cs/1/au:+Gregoire_F/0/1/0/all/0/1"}, {"name": "Philippe Langlais", "link": "http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Data-Driven Analytics for Benchmarking and Optimizing Retail Store Performance. (arXiv:1806.05563v1 [stat.AP])", "description": "Growing competitiveness and increasing availability of data is generating tremendous interest in data-driven analytics across industries. In the retail sector, stores need targeted guidance to improve both the efficiency and effectiveness of individual stores based on their specific locations, demographics, and environment. We propose an effective data-driven framework for internal benchmarking that can lead to targeted guidance for individual stores. In particular, we propose an objective method for segmenting stores using a model-based clustering technique that accounts for similarity in store performance dynamics. The proposed method relies on an effective Finite Mixture of Regressions technique based on competitive learning for carrying out the model-based clustering with `must-link' constraints and modeling store performance. We also propose an optimization framework to derive tailored recommendations for individual stores within store clusters that jointly improves profitability for the store while also improving sales to satisfy franchiser requirements. We validate the methods using synthetic experiments as well as a real-world automotive dealership network study for a leading global automotive manufacturer. ", "link": "http://arxiv.org/abs/1806.05563", "authors": [{"name": "Haidar Almohri", "link": "http://arxiv.org/find/stat/1/au:+Almohri_H/0/1/0/all/0/1"}, {"name": "Ratna Babu Chinnam", "link": "http://arxiv.org/find/stat/1/au:+Chinnam_R/0/1/0/all/0/1"}, {"name": "Mark Colosimo", "link": "http://arxiv.org/find/stat/1/au:+Colosimo_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Autoregressive Quantile Networks for Generative Modeling. (arXiv:1806.05575v1 [cs.LG])", "description": "We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution. ", "link": "http://arxiv.org/abs/1806.05575", "authors": [{"name": "Georg Ostrovski", "link": "http://arxiv.org/find/cs/1/au:+Ostrovski_G/0/1/0/all/0/1"}, {"name": "Will Dabney", "link": "http://arxiv.org/find/cs/1/au:+Dabney_W/0/1/0/all/0/1"}, {"name": "R&#xe9;mi Munos", "link": "http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Improving Consistency-Based Semi-Supervised Learning with Weight Averaging. (arXiv:1806.05594v1 [cs.LG])", "description": "Recent advances in deep unsupervised learning have renewed interest in semi-supervised methods, which can learn from both labeled and unlabeled data. Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. We show that consistency regularization leads to flatter but narrower optima. We also show that the test error surface for these methods is approximately convex in regions of weight space traversed by SGD. Inspired by these observations, we propose to train consistency based semi-supervised models with stochastic weight averaging (SWA), a recent method which averages weights along the trajectory of SGD. We also develop fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With fast-SWA we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100 over many different numbers of observed training labels. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to 6.28% of the previous best result in the literature. We also improve the best known result from 80% accuracy to 83% for domain adaptation from CIFAR-10 to STL. Finally, we show that with fast-SWA the simple $\\Pi$ model becomes state-of-the-art for large labeled settings. ", "link": "http://arxiv.org/abs/1806.05594", "authors": [{"name": "Ben Athiwaratkun", "link": "http://arxiv.org/find/cs/1/au:+Athiwaratkun_B/0/1/0/all/0/1"}, {"name": "Marc Finzi", "link": "http://arxiv.org/find/cs/1/au:+Finzi_M/0/1/0/all/0/1"}, {"name": "Pavel Izmailov", "link": "http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1"}, {"name": "Andrew Gordon Wilson", "link": "http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Stochastic Variance-Reduced Policy Gradient. (arXiv:1806.05618v1 [cs.LG])", "description": "In this paper, we propose a novel reinforcement- learning algorithm consisting in a stochastic variance-reduced version of policy gradient for solving Markov Decision Processes (MDPs). Stochastic variance-reduced gradient (SVRG) methods have proven to be very successful in supervised learning. However, their adaptation to policy gradient is not straightforward and needs to account for I) a non-concave objective func- tion; II) approximations in the full gradient com- putation; and III) a non-stationary sampling pro- cess. The result is SVRPG, a stochastic variance- reduced policy gradient algorithm that leverages on importance weights to preserve the unbiased- ness of the gradient estimate. Under standard as- sumptions on the MDP, we provide convergence guarantees for SVRPG with a convergence rate that is linear under increasing batch sizes. Finally, we suggest practical variants of SVRPG, and we empirically evaluate them on continuous MDPs. ", "link": "http://arxiv.org/abs/1806.05618", "authors": [{"name": "Matteo Papini", "link": "http://arxiv.org/find/cs/1/au:+Papini_M/0/1/0/all/0/1"}, {"name": "Damiano Binaghi", "link": "http://arxiv.org/find/cs/1/au:+Binaghi_D/0/1/0/all/0/1"}, {"name": "Giuseppe Canonaco", "link": "http://arxiv.org/find/cs/1/au:+Canonaco_G/0/1/0/all/0/1"}, {"name": "Matteo Pirotta", "link": "http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"}, {"name": "Marcello Restelli", "link": "http://arxiv.org/find/cs/1/au:+Restelli_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Self-Imitation Learning. (arXiv:1806.05635v1 [cs.LG])", "description": "This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks. ", "link": "http://arxiv.org/abs/1806.05635", "authors": [{"name": "Junhyuk Oh", "link": "http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"}, {"name": "Yijie Guo", "link": "http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"}, {"name": "Satinder Singh", "link": "http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"}, {"name": "Honglak Lee", "link": "http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations. (arXiv:1806.05662v1 [cs.LG])", "description": "Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels. ", "link": "http://arxiv.org/abs/1806.05662", "authors": [{"name": "Zhilin Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"}, {"name": "Jake", "link": "http://arxiv.org/find/cs/1/au:+Jake/0/1/0/all/0/1"}, {"name": "Zhao", "link": "http://arxiv.org/find/cs/1/au:+Zhao/0/1/0/all/0/1"}, {"name": "Bhuwan Dhingra", "link": "http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"}, {"name": "Kaiming He", "link": "http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"}, {"name": "William W. Cohen", "link": "http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"}, {"name": "Ruslan Salakhutdinov", "link": "http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"}, {"name": "Yann LeCun", "link": "http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Constraints and Conditions: the Lasso Oracle-inequalities. (arXiv:1603.06177v3 [math.ST] UPDATED)", "description": "We study various constraints and conditions on the true coefficient vector and on the design matrix to establish non-asymptotic oracle inequalities for the prediction error, estimation accuracy and variable selection for the Lasso estimator in high dimensional sparse regression models. We review results from the literature and we provide simpler and detailed derivation for several boundedness theorems. In addition, we complement the theory with illustrated examples. ", "link": "http://arxiv.org/abs/1603.06177", "authors": [{"name": "Niharika Gauraha", "link": "http://arxiv.org/find/math/1/au:+Gauraha_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Learning a Tree-Structured Ising Model in Order to Make Predictions. (arXiv:1604.06749v3 [math.ST] UPDATED)", "description": "We study the problem of learning a tree Ising model from samples such that subsequent predictions made using the model are accurate. The prediction task considered in this paper is that of predicting the values of a subset of variables given values of some other subset of variables. Virtually all previous work on graphical model learning has focused on recovering the true underlying graph. We define a distance (\"small set TV\" or ssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets $\\mathcal{S}$ of a given size, of the total variation between the marginals of $P$ and $Q$ on $\\mathcal{S}$; this distance captures the accuracy of the prediction task of interest. We derive non-asymptotic bounds on the number of samples needed to get a distribution (from the same class) with small ssTV relative to the one generating the samples. One of the main messages of this paper is that far fewer samples are needed than for recovering the underlying tree, which means that accurate predictions are possible using the wrong tree. ", "link": "http://arxiv.org/abs/1604.06749", "authors": [{"name": "Guy Bresler", "link": "http://arxiv.org/find/math/1/au:+Bresler_G/0/1/0/all/0/1"}, {"name": "Mina Karzand", "link": "http://arxiv.org/find/math/1/au:+Karzand_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Split-door criterion: Identification of causal effects through auxiliary outcomes. (arXiv:1611.09414v2 [stat.ME] UPDATED)", "description": "We present a method for estimating causal effects in time series data when fine-grained information about the outcome of interest is available. Specifically, we examine what we call the split-door setting, where the outcome variable can be split into two parts: one that is potentially affected by the cause being studied and another that is independent of it, with both parts sharing the same (unobserved) confounders. We show that under these conditions, the problem of identification reduces to that of testing for independence among observed variables, and present a method that uses this approach to automatically find subsets of the data that are causally identified. We demonstrate the method by estimating the causal impact of Amazon's recommender system on traffic to product pages, finding thousands of examples within the dataset that satisfy the split-door criterion. Unlike past studies based on natural experiments that were limited to a single product category, our method applies to a large and representative sample of products viewed on the site. In line with previous work, we find that the widely-used click-through rate (CTR) metric overestimates the causal impact of recommender systems; depending on the product category, we estimate that 50-80\\% of the traffic attributed to recommender systems would have happened even without any recommendations. We conclude with guidelines for using the split-door criterion as well as a discussion of other contexts where the method can be applied. ", "link": "http://arxiv.org/abs/1611.09414", "authors": [{"name": "Amit Sharma", "link": "http://arxiv.org/find/stat/1/au:+Sharma_A/0/1/0/all/0/1"}, {"name": "Jake M. Hofman", "link": "http://arxiv.org/find/stat/1/au:+Hofman_J/0/1/0/all/0/1"}, {"name": "Duncan J. Watts", "link": "http://arxiv.org/find/stat/1/au:+Watts_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Embarrassingly Parallel Inference for Gaussian Processes. (arXiv:1702.08420v5 [stat.ML] UPDATED)", "description": "Training Gaussian process-based models typically involves an $ O(N^3)$ computational bottleneck due to inverting the covariance matrix. Popular methods for overcoming this matrix inversion problem cannot adequately model all types of latent functions, and are often not parallelizable. However, judicious choice of model structure can ameliorate this problem. A mixture-of-experts model that uses a mixture of $K$ Gaussian processes offers modeling flexibility and opportunities for scalable inference. Our embarassingly parallel algorithm combines low-dimensional matrix inversions with importance sampling to yield a flexible, scalable mixture-of-experts model that offers comparable performance to Gaussian process regression at a much lower computational cost. ", "link": "http://arxiv.org/abs/1702.08420", "authors": [{"name": "Michael Minyi Zhang", "link": "http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1"}, {"name": "Sinead A. Williamson", "link": "http://arxiv.org/find/stat/1/au:+Williamson_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Convex Coupled Matrix and Tensor Completion. (arXiv:1705.05197v2 [stat.ML] UPDATED)", "description": "We propose a set of convex low rank inducing norms for a coupled matrices and tensors (hereafter coupled tensors), which shares information between matrices and tensors through common modes. More specifically, we propose a mixture of the overlapped trace norm and the latent norms with the matrix trace norm, and then, we propose a new completion algorithm based on the proposed norms. A key advantage of the proposed norms is that it is convex and can find a globally optimal solution, while existing methods for coupled learning are non-convex. Furthermore, we analyze the excess risk bounds of the completion model regularized by our proposed norms which show that our proposed norms can exploit the low rankness of coupled tensors leading to better bounds compared to uncoupled norms. Through synthetic and real-world data experiments, we show that the proposed completion algorithm compares favorably with existing completion algorithms. ", "link": "http://arxiv.org/abs/1705.05197", "authors": [{"name": "Kishan Wimalawarne", "link": "http://arxiv.org/find/stat/1/au:+Wimalawarne_K/0/1/0/all/0/1"}, {"name": "Makoto Yamada", "link": "http://arxiv.org/find/stat/1/au:+Yamada_M/0/1/0/all/0/1"}, {"name": "Hiroshi Mamitsuka", "link": "http://arxiv.org/find/stat/1/au:+Mamitsuka_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Masked Autoregressive Flow for Density Estimation. (arXiv:1705.07057v4 [stat.ML] UPDATED)", "description": "Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks. ", "link": "http://arxiv.org/abs/1705.07057", "authors": [{"name": "George Papamakarios", "link": "http://arxiv.org/find/stat/1/au:+Papamakarios_G/0/1/0/all/0/1"}, {"name": "Theo Pavlakou", "link": "http://arxiv.org/find/stat/1/au:+Pavlakou_T/0/1/0/all/0/1"}, {"name": "Iain Murray", "link": "http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "ABC model selection for spatial extremes models applied to South Australian maximum temperature data. (arXiv:1710.02961v3 [stat.AP] UPDATED)", "description": "Max-stable processes are a common choice for modelling spatial extreme data as they arise naturally as the infinite-dimensional generalisation of multivariate extreme value theory. Statistical inference for such models is complicated by the intractability of the multivariate density function. Nonparametric, composite likelihood-based, and Bayesian approaches have been proposed to address this difficulty. More recently, a simulation-based approach using approximate Bayesian computation (ABC) has been employed for estimating parameters of max-stable models. ABC algorithms rely on the evaluation of discrepancies between model simulations and the observed data rather than explicit evaluations of computationally expensive or intractable likelihood functions. The use of an ABC method to perform model selection for max-stable models is explored. Three max-stable models are considered: the extremal-t model with either a Whittle-Mat\\'ern or a powered exponential covariance function, and the Brown-Resnick model with power variogram. In addition, we also consider the non-extremal Student-t copula model with a Whittle-Mat\\'ern or a powered exponential covariance function. The method is applied to annual maximum temperature data from 25 weather stations dispersed around South Australia. ", "link": "http://arxiv.org/abs/1710.02961", "authors": [{"name": "Xing Ju Lee", "link": "http://arxiv.org/find/stat/1/au:+Lee_X/0/1/0/all/0/1"}, {"name": "Markus Hainy", "link": "http://arxiv.org/find/stat/1/au:+Hainy_M/0/1/0/all/0/1"}, {"name": "James P. McKeone", "link": "http://arxiv.org/find/stat/1/au:+McKeone_J/0/1/0/all/0/1"}, {"name": "Christopher C. Drovandi", "link": "http://arxiv.org/find/stat/1/au:+Drovandi_C/0/1/0/all/0/1"}, {"name": "Anthony N. Pettitt", "link": "http://arxiv.org/find/stat/1/au:+Pettitt_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "(Un)Conditional Sample Generation Based on Distribution Element Trees. (arXiv:1711.04632v2 [stat.ME] UPDATED)", "description": "Recently, distribution element trees (DETs) were introduced as an accurate and computationally efficient method for density estimation. In this work, we demonstrate that the DET formulation promotes an easy and inexpensive way to generate random samples similar to a smooth bootstrap. These samples can be generated unconditionally, but also, without further complications, conditionally utilizing available information about certain probability-space components. ", "link": "http://arxiv.org/abs/1711.04632", "authors": [{"name": "Daniel W. Meyer", "link": "http://arxiv.org/find/stat/1/au:+Meyer_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models. (arXiv:1801.02227v2 [stat.ML] UPDATED)", "description": "We propose a new technique that boosts the convergence of training generative adversarial networks. Generally, the rate of training deep models reduces severely after multiple iterations. A key reason for this phenomenon is that a deep network is expressed using a highly non-convex finite-dimensional model, and thus the parameter gets stuck in a local optimum. Because of this, methods often suffer not only from degeneration of the convergence speed but also from limitations in the representational power of the trained network. To overcome this issue, we propose an additional layer called the gradient layer to seek a descent direction in an infinite-dimensional space. Because the layer is constructed in the infinite-dimensional space, we are not restricted by the specific model structure of finite-dimensional models. As a result, we can get out of the local optima in finite-dimensional models and move towards the global optimal function more directly. In this paper, this phenomenon is explained from the functional gradient method perspective of the gradient layer. Interestingly, the optimization procedure using the gradient layer naturally constructs the deep structure of the network. Moreover, we demonstrate that this procedure can be regarded as a discretization method of the gradient flow that naturally reduces the objective function. Finally, the method is tested using several numerical experiments, which show its fast convergence. ", "link": "http://arxiv.org/abs/1801.02227", "authors": [{"name": "Atsushi Nitanda", "link": "http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1"}, {"name": "Taiji Suzuki", "link": "http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace. (arXiv:1801.05558v3 [stat.ML] UPDATED)", "description": "Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. Our primary contribution is the {\\em MT-net}, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on. Additionally, a task-specific learner of an {\\em MT-net} performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks. ", "link": "http://arxiv.org/abs/1801.05558", "authors": [{"name": "Yoonho Lee", "link": "http://arxiv.org/find/stat/1/au:+Lee_Y/0/1/0/all/0/1"}, {"name": "Seungjin Choi", "link": "http://arxiv.org/find/stat/1/au:+Choi_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Evaluating Predictive Models of Student Success: Closing the Methodological Gap. (arXiv:1801.08494v2 [stat.AP] UPDATED)", "description": "Model evaluation -- the process of making inferences about the performance of predictive models -- is a critical component of predictive modeling research in learning analytics. We survey the state of the practice with respect to model evaluation in learning analytics, which overwhelmingly uses only naive methods for model evaluation or statistical tests which are not appropriate for predictive model evaluation. We conduct a critical comparison of both null hypothesis significance testing (NHST) and a preferred Bayesian method for model evaluation. Finally, we apply three methods -- the na{\\\"i}ve average commonly used in learning analytics, NHST, and Bayesian -- to a predictive modeling experiment on a large set of MOOC data. We compare 96 different predictive models, including different feature sets, statistical modeling algorithms, and tuning hyperparameters for each, using this case study to demonstrate the different experimental conclusions these evaluation techniques provide. ", "link": "http://arxiv.org/abs/1801.08494", "authors": [{"name": "Josh Gardner", "link": "http://arxiv.org/find/stat/1/au:+Gardner_J/0/1/0/all/0/1"}, {"name": "Christopher Brooks", "link": "http://arxiv.org/find/stat/1/au:+Brooks_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "A Fast Proximal Point Method for Computing Wasserstein Distance. (arXiv:1802.04307v2 [stat.ML] UPDATED)", "description": "Wasserstein distance plays increasingly important roles in machine learning, stochastic programming and image processing. Major efforts have been under way to address its high computational complexity, some leading to approximate or regularized variations such as Sinkhorn distance. However, as we will demonstrate, regularized variations with large regularization parameter will degradate the performance in several important machine learning applications, and small regularization parameter will fail due to numerical stability issues with existing algorithms. We address this challenge by developing an Inexact Proximal point method for Optimal Transport (IPOT) with the proximal operator approximately evaluated at each iteration using projections to the probability simplex. We prove the algorithm has linear convergence rate. We also apply IPOT to learning generative models, and generalize the idea of IPOT to a new method for computing Wasserstein barycenter. ", "link": "http://arxiv.org/abs/1802.04307", "authors": [{"name": "Yujia Xie", "link": "http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1"}, {"name": "Xiangfeng Wang", "link": "http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1"}, {"name": "Ruijia Wang", "link": "http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1"}, {"name": "Hongyuan Zha", "link": "http://arxiv.org/find/stat/1/au:+Zha_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Learning Privacy Preserving Encodings through Adversarial Training. (arXiv:1802.05214v2 [cs.LG] UPDATED)", "description": "We present a framework to learn privacy-preserving encodings of images (or other high-dimensional data) to inhibit inference of a chosen private attribute. Rather than encoding a fixed dataset or inhibiting a fixed estimator, we aim to to learn an encoding function such that even after this function is fixed, an estimator with knowledge of the encoding is unable to learn to accurately predict the private attribute, when generalizing beyond a training set. We formulate this as adversarial optimization of an encoding function against a classifier for the private attribute, with both modeled as deep neural networks. We describe an optimization approach which successfully yields an encoder that permanently limits inference of the private attribute, while preserving either a generic notion of information, or the estimation of a different, desired, attribute. We experimentally validate the efficacy of our approach on private tasks of real-world complexity, by learning to prevent detection of scene classes from the Places-365 dataset. ", "link": "http://arxiv.org/abs/1802.05214", "authors": [{"name": "Francesco Pittaluga", "link": "http://arxiv.org/find/cs/1/au:+Pittaluga_F/0/1/0/all/0/1"}, {"name": "Sanjeev J. Koppal", "link": "http://arxiv.org/find/cs/1/au:+Koppal_S/0/1/0/all/0/1"}, {"name": "Ayan Chakrabarti", "link": "http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. (arXiv:1802.06093v3 [cs.LG] UPDATED)", "description": "We analyze algorithms for approximating a function $f(x) = \\Phi x$ mapping $\\Re^d$ to $\\Re^d$ using deep linear neural networks, i.e. that learn a function $h$ parameterized by matrices $\\Theta_1,...,\\Theta_L$ and defined by $h(x) = \\Theta_L \\Theta_{L-1} ... \\Theta_1 x$. We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic. ", "link": "http://arxiv.org/abs/1802.06093", "authors": [{"name": "Peter L. Bartlett", "link": "http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"}, {"name": "David P. Helmbold", "link": "http://arxiv.org/find/cs/1/au:+Helmbold_D/0/1/0/all/0/1"}, {"name": "Philip M. Long", "link": "http://arxiv.org/find/cs/1/au:+Long_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "A Generative Modeling Approach to Limited Channel ECG Classification. (arXiv:1802.06458v3 [stat.ML] UPDATED)", "description": "Processing temporal sequences is central to a variety of applications in health care, and in particular multi-channel Electrocardiogram (ECG) is a highly prevalent diagnostic modality that relies on robust sequence modeling. While Recurrent Neural Networks (RNNs) have led to significant advances in automated diagnosis with time-series data, they perform poorly when models are trained using a limited set of channels. A crucial limitation of existing solutions is that they rely solely on discriminative models, which tend to generalize poorly in such scenarios. In order to combat this limitation, we develop a generative modeling approach to limited channel ECG classification. This approach first uses a Seq2Seq model to implicitly generate the missing channel information, and then uses the latent representation to perform the actual supervisory task. This decoupling enables the use of unsupervised data and also provides highly robust metric spaces for subsequent discriminative learning. Our experiments with the Physionet dataset clearly evidence the effectiveness of our approach over standard RNNs in disease prediction. ", "link": "http://arxiv.org/abs/1802.06458", "authors": [{"name": "Deepta Rajan", "link": "http://arxiv.org/find/stat/1/au:+Rajan_D/0/1/0/all/0/1"}, {"name": "Jayaraman J. Thiagarajan", "link": "http://arxiv.org/find/stat/1/au:+Thiagarajan_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "On the Connection Between Learning Two-Layers Neural Networks and Tensor Decomposition. (arXiv:1802.07301v2 [cs.LG] UPDATED)", "description": "We establish connections between the problem of learning a two-layers neural network with good generalization error and tensor decomposition. We consider a model with input $\\boldsymbol x \\in \\mathbb R^d$, $r$ hidden units with weights $\\{\\boldsymbol w_i\\}_{1\\le i \\le r}$ and output $y\\in \\mathbb R$, i.e., $y=\\sum_{i=1}^r\\sigma(\\langle\\boldsymbol x, \\boldsymbol w_i\\rangle)$, where $\\langle\\cdot, \\cdot\\rangle$ denotes the scalar product and $\\sigma$ the activation function. First, we show that, if we cannot learn the weights $\\{\\boldsymbol w_i\\}_{1\\le i\\le r}$ accurately, then the neural network does not generalize well. More specifically, the generalization error is close to that of a trivial predictor with access only to the norm of the input. We prove this result in a model with separated isotropic weights and in a model with random weights. In both settings, we assume that the input distribution is Gaussian, which is common in the theoretical literature. Then, we show that the problem of learning the weights $\\{\\boldsymbol w_i\\}_{1\\le i \\le r}$ is at least as hard as the problem of tensor decomposition. We prove this result for any input distribution, and we assume that the activation function is a polynomial whose degree is related to the order of the tensor to be decomposed. Hence, we obtain that learning a two-layers neural network that generalizes well is at least as hard as tensor decomposition. It has been observed that neural network models with more parameters than training samples often generalize well, even if the problem is highly underdetermined. This means that the learning algorithm does not estimate the weights accurately and yet is able to yield a good generalization error. This paper shows that such a phenomenon cannot occur with a two-layers neural network when the input distribution is Gaussian. We also provide numerical evidence supporting our theoretical findings. ", "link": "http://arxiv.org/abs/1802.07301", "authors": [{"name": "Marco Mondelli", "link": "http://arxiv.org/find/cs/1/au:+Mondelli_M/0/1/0/all/0/1"}, {"name": "Andrea Montanari", "link": "http://arxiv.org/find/cs/1/au:+Montanari_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Learning to Explain: An Information-Theoretic Perspective on Model Interpretation. (arXiv:1802.07814v2 [cs.LG] UPDATED)", "description": "We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation. ", "link": "http://arxiv.org/abs/1802.07814", "authors": [{"name": "Jianbo Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"}, {"name": "Le Song", "link": "http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"}, {"name": "Martin J. Wainwright", "link": "http://arxiv.org/find/cs/1/au:+Wainwright_M/0/1/0/all/0/1"}, {"name": "Michael I. Jordan", "link": "http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity. (arXiv:1802.08183v4 [stat.ML] UPDATED)", "description": "Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of non-convex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized. Although online methods have been introduced, they suffer from similar problems. In this work, we propose Meta-Frank-Wolfe, the first online projection-free algorithm that uses stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves the optimal $O( \\sqrt{T})$ adversarial regret bounds for convex and continuous submodular optimization. We also propose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single stochastic gradient estimate in each round and achieves an $O(T^{2/3})$ stochastic regret bound for convex and continuous submodular optimization. We apply our methods to develop a novel \"lifting\" framework for the online discrete submodular maximization and also see that they outperform current state-of-the-art techniques on various experiments. ", "link": "http://arxiv.org/abs/1802.08183", "authors": [{"name": "Lin Chen", "link": "http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1"}, {"name": "Christopher Harshaw", "link": "http://arxiv.org/find/stat/1/au:+Harshaw_C/0/1/0/all/0/1"}, {"name": "Hamed Hassani", "link": "http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1"}, {"name": "Amin Karbasi", "link": "http://arxiv.org/find/stat/1/au:+Karbasi_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Variational Message Passing with Structured Inference Networks. (arXiv:1803.05589v2 [stat.ML] UPDATED)", "description": "Recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret. We propose a variational message-passing algorithm for variational inference in such models. We make three contributions. First, we propose structured inference networks that incorporate the structure of the graphical model in the inference network of variational auto-encoders (VAE). Second, we establish conditions under which such inference networks enable fast amortized inference similar to VAE. Finally, we derive a variational message passing algorithm to perform efficient natural-gradient inference while retaining the efficiency of the amortized inference. By simultaneously enabling structured, amortized, and natural-gradient inference for deep structured models, our method simplifies and generalizes existing methods. ", "link": "http://arxiv.org/abs/1803.05589", "authors": [{"name": "Wu Lin", "link": "http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"}, {"name": "Nicolas Hubacher", "link": "http://arxiv.org/find/stat/1/au:+Hubacher_N/0/1/0/all/0/1"}, {"name": "Mohammad Emtiyaz Khan", "link": "http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Log-moment estimators for the generalized Linnik and Mittag-Leffler distributions with applications to financial modeling. (arXiv:1803.11459v2 [stat.ME] UPDATED)", "description": "We propose formal estimation procedures for the parameters of the generalized, three-parameter Linnik $gL(\\alpha,\\mu, \\delta)$ and Mittag-Leffler $gML(\\alpha,\\mu, \\delta)$ distributions. The estimators are derived from the moments of the log-transformed random variables, and are shown to be asymptotically unbiased. The estimation algorithms are computationally efficient and the proposed procedures are tested using the daily S\\&P 500 and Dow Jones index data. The results show that the standard two-parameter Linnik and Mittag-Leffler models are not flexible enough to accurately model the current stock market data. ", "link": "http://arxiv.org/abs/1803.11459", "authors": [{"name": "Dexter O. Cahoy", "link": "http://arxiv.org/find/stat/1/au:+Cahoy_D/0/1/0/all/0/1"}, {"name": "Wojbor A. Woyczy&#x144;ski", "link": "http://arxiv.org/find/stat/1/au:+Woyczynski_W/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Contest models highlight inherent inefficiencies of scientific funding competitions. (arXiv:1804.03732v2 [physics.soc-ph] UPDATED)", "description": "Scientific research funding is allocated largely through a system of soliciting and ranking competitive grant proposals. In these competitions, the proposals themselves are not the deliverables that the funder seeks, but instead are used by the funder to screen for the most promising research ideas. Consequently, some of the funding program's impact on science is squandered because applying researchers must spend time writing proposals instead of doing science. To what extent does the community's aggregate investment in proposal preparation negate the scientific impact of the funding program? Are there alternative mechanisms for awarding funds that advance science more efficiently? We use the economic theory of contests to analyze how efficiently grant-proposal competitions advance science, and compare them to recently proposed, partially randomized alternatives such as lotteries. We find that the effort researchers waste in writing proposals may be comparable to the total scientific value of the research that the funding supports, especially when only a few proposals can be funded. Moreover, when professional pressures motivate investigators to seek funding for reasons that extend beyond the value of the proposed science (e.g., promotion, prestige), the entire program can actually hamper scientific progress when the number of awards is small. We suggest that lost efficiency may be restored either by partial lotteries for funding, or by funding researchers based on past scientific success instead of proposals for future work. ", "link": "http://arxiv.org/abs/1804.03732", "authors": [{"name": "Kevin Gross", "link": "http://arxiv.org/find/physics/1/au:+Gross_K/0/1/0/all/0/1"}, {"name": "Carl T. Bergstrom", "link": "http://arxiv.org/find/physics/1/au:+Bergstrom_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "A comparison of methods for model selection when estimating individual treatment effects. (arXiv:1804.05146v2 [stat.ML] UPDATED)", "description": "Practitioners in medicine, business, political science, and other fields are increasingly aware that decisions should be personalized to each patient, customer, or voter. A given treatment (e.g. a drug or advertisement) should be administered only to those who will respond most positively, and certainly not to those who will be harmed by it. Individual-level treatment effects can be estimated with tools adapted from machine learning, but different models can yield contradictory estimates. Unlike risk prediction models, however, treatment effect models cannot be easily evaluated against each other using a held-out test set because the true treatment effect itself is never directly observed. Besides outcome prediction accuracy, several metrics that can leverage held-out data to evaluate treatment effects models have been proposed, but they are not widely used. We provide a didactic framework that elucidates the relationships between the different approaches and compare them all using a variety of simulations of both randomized and observational data. Our results show that researchers estimating heterogenous treatment effects need not limit themselves to a single model-fitting algorithm. Instead of relying on a single method, multiple models fit by a diverse set of algorithms should be evaluated against each other using an objective function learned from the validation set. The model minimizing that objective should be used for estimating the individual treatment effect for future individuals. ", "link": "http://arxiv.org/abs/1804.05146", "authors": [{"name": "Alejandro Schuler", "link": "http://arxiv.org/find/stat/1/au:+Schuler_A/0/1/0/all/0/1"}, {"name": "Michael Baiocchi", "link": "http://arxiv.org/find/stat/1/au:+Baiocchi_M/0/1/0/all/0/1"}, {"name": "Robert Tibshirani", "link": "http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1"}, {"name": "Nigam Shah", "link": "http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Offline Evaluation of Ranking Policies with Click Models. (arXiv:1804.10488v2 [cs.LG] UPDATED)", "description": "Many web systems rank and present a list of items to users, from recommender systems to search and advertising. An important problem in practice is to evaluate new ranking policies offline and optimize them before they are deployed. We address this problem by proposing evaluation algorithms for estimating the expected number of clicks on ranked lists from historical logged data. The existing algorithms are not guaranteed to be statistically efficient in our problem because the number of recommended lists can grow exponentially with their length. To overcome this challenge, we use models of user interaction with the list of items, the so-called click models, to construct estimators that learn statistically efficiently. We analyze our estimators and prove that they are more efficient than the estimators that do not use the structure of the click model, under the assumption that the click model holds. We evaluate our estimators in a series of experiments on a real-world dataset and show that they consistently outperform prior estimators. ", "link": "http://arxiv.org/abs/1804.10488", "authors": [{"name": "Shuai Li", "link": "http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"}, {"name": "Yasin Abbasi-Yadkori", "link": "http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1"}, {"name": "Branislav Kveton", "link": "http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"}, {"name": "S. Muthukrishnan", "link": "http://arxiv.org/find/cs/1/au:+Muthukrishnan_S/0/1/0/all/0/1"}, {"name": "Vishwa Vinay", "link": "http://arxiv.org/find/cs/1/au:+Vinay_V/0/1/0/all/0/1"}, {"name": "Zheng Wen", "link": "http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Learning towards Minimum Hyperspherical Energy. (arXiv:1805.09298v3 [cs.LG] UPDATED)", "description": "Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as even as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our method, by showing the superior performance with MHE regularization. ", "link": "http://arxiv.org/abs/1805.09298", "authors": [{"name": "Weiyang Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"}, {"name": "Rongmei Lin", "link": "http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"}, {"name": "Zhen Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"}, {"name": "Lixin Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"}, {"name": "Zhiding Yu", "link": "http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"}, {"name": "Bo Dai", "link": "http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"}, {"name": "Le Song", "link": "http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Pairwise likelihood estimation of latent autoregressive count models. (arXiv:1805.10865v2 [stat.ME] UPDATED)", "description": "Latent autoregressive models are useful time series models for the analysis of infectious disease data. Evaluation of the likelihood function of latent autoregressive models is intractable and its approximation through simulation-based methods appears as a standard practice. Although simulation methods may make the inferential problem feasible, they are often computationally intensive and the quality of the numerical approximation may be difficult to assess. We consider instead a weighted pairwise likelihood approach and explore several computational and methodological aspects including selection of the pairs, estimation of robust standard errors and the role of numerical integration. The suggested approach is illustrated using monthly data on invasive meningococcal disease infection in Greece and Italy. ", "link": "http://arxiv.org/abs/1805.10865", "authors": [{"name": "Xanthi Pedeli", "link": "http://arxiv.org/find/stat/1/au:+Pedeli_X/0/1/0/all/0/1"}, {"name": "Cristiano Varin", "link": "http://arxiv.org/find/stat/1/au:+Varin_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators. (arXiv:1806.00149v2 [cs.NE] UPDATED)", "description": "We propose a new generic type of stochastic neurons, called $q$-neurons, that considers activation functions based on Jackson's $q$-derivatives with stochastic parameters $q$. Our generalization of neural network architectures with $q$-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions. ", "link": "http://arxiv.org/abs/1806.00149", "authors": [{"name": "Frank Nielsen", "link": "http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1"}, {"name": "Ke Sun", "link": "http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. (arXiv:1806.01811v4 [stat.ML] UPDATED)", "description": "Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine tune parameters such as the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization, which is quite different from the offline and nonconvex setting where adaptive gradient methods shine in practice. We bridge this gap by providing strong theoretical guarantees in batch and stochastic setting, for the convergence of AdaGrad over smooth, nonconvex landscapes, from any initialization of the stepsize, without knowledge of Lipschitz constant of the gradient. We show in the stochastic setting that AdaGrad converges to a stationary point at the optimal $O(1/\\sqrt{N})$ rate (up to a $\\log(N)$ factor), and in the batch setting, at the optimal $O(1/N)$ rate. Moreover, in both settings, the constant in the rate matches the constant obtained as if the variance of the gradient noise and Lipschitz constant of the gradient were known in advance and used to tune the stepsize, up to a logarithmic factor of the mismatch between the optimal stepsize and the stepsize used to initialize AdaGrad. In particular, our results imply that AdaGrad is robust to both the unknown Lipschitz constant and level of stochastic noise on the gradient, in a near-optimal sense. When there is noise, AdaGrad converges at the rate of $O(1/\\sqrt{N})$ with well-tuned stepsize, and when there is not noise, the same algorithm converges at the rate of $O(1/N)$ like well-tuned batch gradient descent. ", "link": "http://arxiv.org/abs/1806.01811", "authors": [{"name": "Rachel Ward", "link": "http://arxiv.org/find/stat/1/au:+Ward_R/0/1/0/all/0/1"}, {"name": "Xiaoxia Wu", "link": "http://arxiv.org/find/stat/1/au:+Wu_X/0/1/0/all/0/1"}, {"name": "Leon Bottou", "link": "http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "IVUS-Net: An Intravascular Ultrasound Segmentation Network. (arXiv:1806.03583v2 [stat.ML] UPDATED)", "description": "IntraVascular UltraSound (IVUS) is one of the most effective imaging modalities that provides assistance to experts in order to diagnose and treat cardiovascular diseases. We address a central problem in IVUS image analysis with Fully Convolutional Network (FCN): automatically delineate the lumen and media-adventitia borders in IVUS images, which is crucial to shorten the diagnosis process or benefits a faster and more accurate 3D reconstruction of the artery. Particularly, we propose an FCN architecture, called IVUS-Net, followed by a post-processing contour extraction step, in order to automatically segments the interior (lumen) and exterior (media-adventitia) regions of the human arteries. We evaluated our IVUS-Net on the test set of a standard publicly available dataset containing 326 IVUS B-mode images with two measurements, namely Jaccard Measure (JM) and Hausdorff Distances (HD). The evaluation result shows that IVUS-Net outperforms the state-of-the-art lumen and media segmentation methods by 4% to 20% in terms of HD distance. IVUS-Net performs well on images in the test set that contain a significant amount of major artifacts such as bifurcations, shadows, and side branches that are not common in the training set. Furthermore, using a modern GPU, IVUS-Net segments each IVUS frame only in 0.15 seconds. The proposed work, to the best of our knowledge, is the first deep learning based method for segmentation of both the lumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the best results without any manual intervention. Code is available at https://github.com/Kulbear/ivus-segmentation-icsm2018 ", "link": "http://arxiv.org/abs/1806.03583", "authors": [{"name": "Ji Yang", "link": "http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1"}, {"name": "Lin Tong", "link": "http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1"}, {"name": "Mehdi Faraji", "link": "http://arxiv.org/find/stat/1/au:+Faraji_M/0/1/0/all/0/1"}, {"name": "Anup Basu", "link": "http://arxiv.org/find/stat/1/au:+Basu_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Aggregating Predictions on Multiple Non-disclosed Datasets using Conformal Prediction. (arXiv:1806.04000v2 [stat.ML] UPDATED)", "description": "Conformal Prediction is a machine learning methodology that produces valid prediction regions under mild conditions. In this paper, we explore the application of making predictions over multiple data sources of different sizes without disclosing data between the sources. We propose that each data source applies a transductive conformal predictor independently using the local data, and that the individual predictions are then aggregated to form a combined prediction region. We demonstrate the method on several data sets, and show that the proposed method produces conservatively valid predictions and reduces the variance in the aggregated predictions. We also study the effect that the number of data sources and size of each source has on aggregated predictions, as compared with equally sized sources and pooled data. ", "link": "http://arxiv.org/abs/1806.04000", "authors": [{"name": "Ola Spjuth", "link": "http://arxiv.org/find/stat/1/au:+Spjuth_O/0/1/0/all/0/1"}, {"name": "Lars Carlsson", "link": "http://arxiv.org/find/stat/1/au:+Carlsson_L/0/1/0/all/0/1"}, {"name": "Niharika Gauraha", "link": "http://arxiv.org/find/stat/1/au:+Gauraha_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Deep learning to represent sub-grid processes in climate models. (arXiv:1806.04731v2 [physics.ao-ph] UPDATED)", "description": "The representation of nonlinear sub-grid processes, especially clouds, has been a major source of uncertainty in climate models for decades. Cloud-resolving models better represent many of these processes and can now be run globally but only for short-term simulations of at most a few years because of computational limitations. Here we demonstrate that deep learning can be used to capture many advantages of cloud-resolving modeling at a fraction of the computational cost. We train a deep neural network to represent all atmospheric sub-grid processes in a climate model by learning from a multi-scale model in which convection is treated explicitly. The trained neural network then replaces the traditional sub-grid parameterizations in a global general circulation model in which it freely interacts with the resolved dynamics and the surface-flux scheme. The prognostic multi-year simulations are stable and closely reproduce not only the mean climate of the cloud-resolving simulation but also key aspects of variability, including precipitation extremes and the equatorial wave spectrum. Furthermore, the neural network approximately conserves energy despite not being explicitly instructed to. Finally, we show that the neural network parameterization generalizes to new surface forcing patterns but struggles to cope with temperatures far outside its training manifold. Our results show the feasibility of using deep learning for climate model parameterization. In a broader context, we anticipate that data-driven Earth System Model development could play a key role in reducing climate prediction uncertainty in the coming decade. ", "link": "http://arxiv.org/abs/1806.04731", "authors": [{"name": "Stephan Rasp", "link": "http://arxiv.org/find/physics/1/au:+Rasp_S/0/1/0/all/0/1"}, {"name": "Michael S. Pritchard", "link": "http://arxiv.org/find/physics/1/au:+Pritchard_M/0/1/0/all/0/1"}, {"name": "Pierre Gentine", "link": "http://arxiv.org/find/physics/1/au:+Gentine_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "Only Bayes should learn a manifold (on the estimation of differential geometric structure from data). (arXiv:1806.04994v2 [stat.ML] UPDATED)", "description": "We investigate learning of the differential geometric structure of a data manifold embedded in a high-dimensional Euclidean space. We first analyze kernel-based algorithms and show that under the usual regularizations, non-probabilistic methods cannot recover the differential geometric structure, but instead find mostly linear manifolds or spaces equipped with teleports. To properly learn the differential geometric structure, non-probabilistic methods must apply regularizations that enforce large gradients, which go against common wisdom. We repeat the analysis for probabilistic methods and find that under reasonable priors, the geometric structure can be recovered. Fully exploiting the recovered structure, however, requires the development of stochastic extensions to classic Riemannian geometry. We take early steps in that regard. Finally, we partly extend the analysis to modern models based on neural networks, thereby highlighting geometric and probabilistic shortcomings of current deep generative models. ", "link": "http://arxiv.org/abs/1806.04994", "authors": [{"name": "S&#xf8;ren Hauberg", "link": "http://arxiv.org/find/stat/1/au:+Hauberg_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Statistics"}
{"title": "A Unified Framework for Generalizable Style Transfer: Style and Content Separation. (arXiv:1806.05173v1 [cs.CV])", "description": "Image style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here propose a unified style transfer framework for both character typeface transfer and neural style transfer tasks leveraging style and content separation. A key merit of such framework is its generalizability to new styles and contents. The overall framework consists of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content representations from the corresponding reference images. The mixer integrates the above two representations and feeds it into the decoder to generate images with the target style and content. During training, the encoder networks learn to extract styles and contents from limited size of style/content reference images. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special `multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. Under this framework, we design two individual networks for character typeface transfer and neural style transfer, respectively. For character typeface transfer, to separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. For neural style transfer, we leverage the statistical information of feature maps in certain layers to represent style. Extensive experimental results have demonstrated the effectiveness and robustness of the proposed methods. ", "link": "http://arxiv.org/abs/1806.05173", "authors": [{"name": "Yexun Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"}, {"name": "Ya Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"}, {"name": "Wenbin Cai", "link": "http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Maintenance of Smart Buildings using Fault Trees. (arXiv:1806.05174v1 [cs.SY])", "description": "Timely maintenance is an important means of increasing system dependability and life span. Fault Maintenance trees (FMTs) are an innovative framework incorporating both maintenance strategies and degradation models and serve as a good planning platform for balancing total costs (operational and maintenance) with dependability of a system. In this work, we apply the FMT formalism to a {Smart Building} application and propose a framework that efficiently encodes the FMT into Continuous Time Markov Chains. This allows us to obtain system dependability metrics such as system reliability and mean time to failure, as well as costs of maintenance and failures over time, for different maintenance policies. We illustrate the pertinence of our approach by evaluating various dependability metrics and maintenance strategies of a Heating, Ventilation and Air-Conditioning system. ", "link": "http://arxiv.org/abs/1806.05174", "authors": [{"name": "Nathalie Cauchi", "link": "http://arxiv.org/find/cs/1/au:+Cauchi_N/0/1/0/all/0/1"}, {"name": "Khaza Anuaral Hoque", "link": "http://arxiv.org/find/cs/1/au:+Hoque_K/0/1/0/all/0/1"}, {"name": "Marielle Stoelinga", "link": "http://arxiv.org/find/cs/1/au:+Stoelinga_M/0/1/0/all/0/1"}, {"name": "Alessandro Abate", "link": "http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "fMRI Semantic Category Decoding using Linguistic Encoding of Word Embeddings. (arXiv:1806.05177v1 [q-bio.NC])", "description": "The dispute of how the human brain represents conceptual knowledge has been argued in many scientific fields. Brain imaging studies have shown that the spatial patterns of neural activation in the brain are correlated with thinking about different semantic categories of words (for example, tools, animals, and buildings) or when viewing the related pictures. In this paper, we present a computational model that learns to predict the neural activation captured in functional magnetic resonance imaging (fMRI) data of test words. Unlike the models with hand-crafted features that have been used in the literature, in this paper we propose a novel approach wherein decoding models are built with features extracted from popular linguistic encodings of Word2Vec, GloVe, Meta-Embeddings in conjunction with the empirical fMRI data associated with viewing several dozen concrete nouns. We compared these models with several other models that use word features extracted from FastText, Randomly-generated features, Mitchell's 25 features [1]. The experimental results show that the predicted fMRI images using Meta-Embeddings meet the state-of-the-art performance. Although models with features from GloVe and Word2Vec predict fMRI images similar to the state-of-the-art model, model with features from Meta-Embeddings predicts significantly better. The proposed scheme that uses popular linguistic encoding offers a simple and easy approach for semantic decoding from fMRI experiments. ", "link": "http://arxiv.org/abs/1806.05177", "authors": [{"name": "Subba Reddy Oota", "link": "http://arxiv.org/find/q-bio/1/au:+Oota_S/0/1/0/all/0/1"}, {"name": "Naresh Manwani", "link": "http://arxiv.org/find/q-bio/1/au:+Manwani_N/0/1/0/all/0/1"}, {"name": "Bapi Raju S", "link": "http://arxiv.org/find/q-bio/1/au:+S_B/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Generating Sentences Using a Dynamic Canvas. (arXiv:1806.05178v1 [cs.CL])", "description": "We introduce the Attentive Unsupervised Text (W)riter (AUTR), which is a word level generative model for natural language. It uses a recurrent neural network with a dynamic attention and canvas memory mechanism to iteratively construct sentences. By viewing the state of the memory at intermediate stages and where the model is placing its attention, we gain insight into how it constructs sentences. We demonstrate that AUTR learns a meaningful latent representation for each sentence, and achieves competitive log-likelihood lower bounds whilst being computationally efficient. It is effective at generating and reconstructing sentences, as well as imputing missing words. ", "link": "http://arxiv.org/abs/1806.05178", "authors": [{"name": "Harshil Shah", "link": "http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1"}, {"name": "Bowen Zheng", "link": "http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"}, {"name": "David Barber", "link": "http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "SafeSpec: Banishing the Spectre of a Meltdown with Leakage-Free Speculation. (arXiv:1806.05179v1 [cs.CR])", "description": "Speculative execution which is used pervasively in modern CPUs can leave side effects in the processor caches and other structures even when the speculated instructions do not commit and their direct effect is not visible. The recent Meltdown and Spectre attacks have shown that this behavior can be exploited to expose privileged information to an unprivileged attacker. In particular, the attack forces the speculative execution of a code gadget that will carry out the illegal read, which eventually gets squashed, but which leaves a side-channel trail that can be used by the attacker to infer the value. Several attack variations are possible, allowing arbitrary exposure of the full kernel memory to an unprivileged attacker. In this paper, we introduce a new model (SafeSpec) for supporting speculation in a way that is immune to side-channel leakage necessary for attacks such as Meltdown and Spectre. In particular, SafeSpec stores side effects of speculation in a way that is not visible to the attacker while the instructions are speculative. The speculative state is then either committed to the main CPU structures if the branch commits, or squashed if it does not, making all direct side effects of speculative code invisible. The solution must also address the possibility of a covert channel from speculative instructions to committed instructions before these instructions are committed. We show that SafeSpec prevents all three variants of Spectre and Meltdown, as well as new variants that we introduce. We also develop a cycle accurate model of modified design of an x86-64 processor and show that the performance impact is negligible. We build prototypes of the hardware support in a hardware description language to show that the additional overhead is small. We believe that SafeSpec completely closes this class of attacks, and that it is practical to implement. ", "link": "http://arxiv.org/abs/1806.05179", "authors": [{"name": "Khaled N. Khasawneh", "link": "http://arxiv.org/find/cs/1/au:+Khasawneh_K/0/1/0/all/0/1"}, {"name": "Esmaeil Mohammadian Koruyeh", "link": "http://arxiv.org/find/cs/1/au:+Koruyeh_E/0/1/0/all/0/1"}, {"name": "Chengyu Song", "link": "http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"}, {"name": "Dmitry Evtyushkin", "link": "http://arxiv.org/find/cs/1/au:+Evtyushkin_D/0/1/0/all/0/1"}, {"name": "Dmitry Ponomarev", "link": "http://arxiv.org/find/cs/1/au:+Ponomarev_D/0/1/0/all/0/1"}, {"name": "Nael Abu-Ghazaleh", "link": "http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Retrospective Analysis of the Fake News Challenge Stance Detection Task. (arXiv:1806.05180v1 [cs.IR])", "description": "The 2017 Fake News Challenge Stage 1 (FNC-1) shared task addressed a stance classification task as a crucial first step towards detecting fake news. To date, there is no in-depth analysis paper to critically discuss FNC-1's experimental setup, reproduce the results, and draw conclusions for next-generation stance classification methods. In this paper, we provide such an in-depth analysis for the three top-performing systems. We first find that FNC-1's proposed evaluation metric favors the majority class, which can be easily classified, and thus overestimates the true discriminative power of the methods. Therefore, we propose a new F1-based metric yielding a changed system ranking. Next, we compare the features and architectures used, which leads to a novel feature-rich stacked LSTM model that performs on par with the best systems, but is superior in predicting minority classes. To understand the methods' ability to generalize, we derive a new dataset and perform both in-domain and cross-domain experiments. Our qualitative and quantitative study helps interpreting the original FNC-1 scores and understand which features help improving performance and why. Our new dataset and all source code used during the reproduction study are publicly available for future research. ", "link": "http://arxiv.org/abs/1806.05180", "authors": [{"name": "Andreas Hanselowski", "link": "http://arxiv.org/find/cs/1/au:+Hanselowski_A/0/1/0/all/0/1"}, {"name": "Avinesh PVS", "link": "http://arxiv.org/find/cs/1/au:+PVS_A/0/1/0/all/0/1"}, {"name": "Benjamin Schiller", "link": "http://arxiv.org/find/cs/1/au:+Schiller_B/0/1/0/all/0/1"}, {"name": "Felix Caspelherr", "link": "http://arxiv.org/find/cs/1/au:+Caspelherr_F/0/1/0/all/0/1"}, {"name": "Debanjan Chaudhuri", "link": "http://arxiv.org/find/cs/1/au:+Chaudhuri_D/0/1/0/all/0/1"}, {"name": "Christian M. Meyer", "link": "http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1"}, {"name": "Iryna Gurevych", "link": "http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Fully Convolutional Network for Automatic Road Extraction from Satellite Imagery. (arXiv:1806.05182v1 [cs.CV])", "description": "Analysis of high-resolution satellite images has been an important research topic for traffic management, city planning, and road monitoring. One of the problems here is automatic and precise road extraction. From an original image, it is difficult and computationally expensive to extract roads due to presences of other road-like features with straight edges. In this paper, we propose an approach for automatic road extraction based on a fully convolutional neural network of U-net family. This network consists of ResNet-34 pre-trained on ImageNet and decoder adapted from vanilla U-Net. Based on validation results, leaderboard and our own experience this network shows superior results for the DEEPGLOBE - CVPR 2018 road extraction sub-challenge. Moreover, this network uses moderate memory that allows using just one GTX 1080 or 1080ti video cards to perform whole training and makes pretty fast predictions. ", "link": "http://arxiv.org/abs/1806.05182", "authors": [{"name": "Alexander V. Buslaev", "link": "http://arxiv.org/find/cs/1/au:+Buslaev_A/0/1/0/all/0/1"}, {"name": "Selim S. Seferbekov", "link": "http://arxiv.org/find/cs/1/au:+Seferbekov_S/0/1/0/all/0/1"}, {"name": "Vladimir I. Iglovikov", "link": "http://arxiv.org/find/cs/1/au:+Iglovikov_V/0/1/0/all/0/1"}, {"name": "Alexey A. Shvets", "link": "http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Automatic counting of fission tracks in apatite and muscovite using image processing. (arXiv:1806.05199v1 [cs.CV])", "description": "One of the major difficulties of automatic track counting using photomicrographs is separating overlapped tracks. We address this issue combining image processing algorithms such as skeletonization, and we test our algorithm with several binarization techniques. The counting algorithm was successfully applied to determine the efficiency factor GQR, necessary for standardless fission-track dating, involving counting induced tracks in apatite and muscovite with superficial densities of about $6 \\times 10^5$ tracks/$cm^2$. ", "link": "http://arxiv.org/abs/1806.05199", "authors": [{"name": "Alexandre Fioravante de Siqueira", "link": "http://arxiv.org/find/cs/1/au:+Siqueira_A/0/1/0/all/0/1"}, {"name": "Wagner Massayuki Nakasuga", "link": "http://arxiv.org/find/cs/1/au:+Nakasuga_W/0/1/0/all/0/1"}, {"name": "Sandro Guedes", "link": "http://arxiv.org/find/cs/1/au:+Guedes_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Enabling End-To-End Machine Learning Replicability: A Case Study in Educational Data Mining. (arXiv:1806.05208v1 [cs.CY])", "description": "The use of machine learning techniques has expanded in education research, driven by the rich data from digital learning environments and institutional data warehouses. However, replication of machine learned models in the domain of the learning sciences is particularly challenging due to a confluence of experimental, methodological, and data barriers. We discuss the challenges of end-to-end machine learning replication in this context, and present an open-source software toolkit, the MOOC Replication Framework (MORF), to address them. We demonstrate the use of MORF by conducting a replication at scale, and provide a complete executable container, with unique DOIs documenting the configurations of each individual trial, for replication or future extension at https://github.com/educational-technology-collective/fy2015-replication. This work demonstrates an approach to end-to-end machine learning replication which is relevant to any domain with large, complex or multi-format, privacy-protected data with a consistent schema. ", "link": "http://arxiv.org/abs/1806.05208", "authors": [{"name": "Josh Gardner", "link": "http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1"}, {"name": "Yuming Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"}, {"name": "Ryan Baker", "link": "http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1"}, {"name": "Christopher Brooks", "link": "http://arxiv.org/find/cs/1/au:+Brooks_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "An Evaluation of Neural Machine Translation Models on Historical Spelling Normalization. (arXiv:1806.05210v1 [cs.CL])", "description": "In this paper, we apply different NMT models to the problem of historical spelling normalization for five languages: English, German, Hungarian, Icelandic, and Swedish. The NMT models are at different levels, have different attention mechanisms, and different neural network architectures. Our results show that NMT models are much better than SMT models in terms of character error rate. The vanilla RNNs are competitive to GRUs/LSTMs in historical spelling normalization. Transformer models perform better only when provided with more training data. We also find that subword-level models with a small subword vocabulary are better than character-level models. In addition, we propose a hybrid method which further improves the performance of historical spelling normalization. ", "link": "http://arxiv.org/abs/1806.05210", "authors": [{"name": "Gongbo Tang", "link": "http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"}, {"name": "Fabienne Cap", "link": "http://arxiv.org/find/cs/1/au:+Cap_F/0/1/0/all/0/1"}, {"name": "Eva Pettersson", "link": "http://arxiv.org/find/cs/1/au:+Pettersson_E/0/1/0/all/0/1"}, {"name": "Joakim Nivre", "link": "http://arxiv.org/find/cs/1/au:+Nivre_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Impostor Networks for Fast Fine-Grained Recognition. (arXiv:1806.05217v1 [cs.CV])", "description": "In this work we introduce impostor networks, an architecture that allows to perform fine-grained recognition with high accuracy and using a light-weight convolutional network, making it particularly suitable for fine-grained applications on low-power and non-GPU enabled platforms. Impostor networks compensate for the lightness of its `backend' network by combining it with a lightweight non-parametric classifier. The combination of a convolutional network and such non-parametric classifier is trained in an end-to-end fashion. Similarly to convolutional neural networks, impostor networks can fit large-scale training datasets very well, while also being able to generalize to new data points. At the same time, the bulk of computations within impostor networks happen through nearest neighbor search in high-dimensions. Such search can be performed efficiently on a variety of architectures including standard CPUs, where deep convolutional networks are inefficient. In a series of experiments with three fine-grained datasets, we show that impostor networks are able to boost the classification accuracy of a moderate-sized convolutional network considerably at a very small computational cost. ", "link": "http://arxiv.org/abs/1806.05217", "authors": [{"name": "Vadim Lebedev", "link": "http://arxiv.org/find/cs/1/au:+Lebedev_V/0/1/0/all/0/1"}, {"name": "Artem Babenko", "link": "http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1"}, {"name": "Victor Lempitsky", "link": "http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis. (arXiv:1806.05219v1 [cs.CL])", "description": "Lack of repeatability and generalisability are two significant threats to continuing scientific development in Natural Language Processing. Language models and learning methods are so complex that scientific conference papers no longer contain enough space for the technical depth required for replication or reproduction. Taking Target Dependent Sentiment Analysis as a case study, we show how recent work in the field has not consistently released code, or described settings for learning methods in enough detail, and lacks comparability and generalisability in train, test or validation data. To investigate generalisability and to enable state of the art comparative evaluations, we carry out the first reproduction studies of three groups of complementary methods and perform the first large-scale mass evaluation on six different English datasets. Reflecting on our experiences, we recommend that future replication or reproduction experiments should always consider a variety of datasets alongside documenting and releasing their methods and published code in order to minimise the barriers to both repeatability and generalisability. We have released our code with a model zoo on GitHub with Jupyter Notebooks to aid understanding and full documentation, and we recommend that others do the same with their papers at submission time through an anonymised GitHub account. ", "link": "http://arxiv.org/abs/1806.05219", "authors": [{"name": "Andrew Moore", "link": "http://arxiv.org/find/cs/1/au:+Moore_A/0/1/0/all/0/1"}, {"name": "Paul Rayson", "link": "http://arxiv.org/find/cs/1/au:+Rayson_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Decentralized Ergodic Control: Distribution-Driven Sensing and Exploration for Multi-Agent Systems. (arXiv:1806.05220v1 [cs.RO])", "description": "We present a decentralized ergodic control policy for time-varying area coverage problems for multiple agents with nonlinear dynamics. Ergodic control allows us to specify distributions as objectives for area coverage problems for nonlinear robotic systems as a closed-form controller. We derive a variation to the ergodic control policy that can be used with consensus to enable a fully decentralized multi-agent control policy. Examples are presented to illustrate the applicability of our method for multi-agent terrain mapping as well as target localization. An analysis on ergodic policies as a Nash equilibrium is provided for game theoretic applications. ", "link": "http://arxiv.org/abs/1806.05220", "authors": [{"name": "Ian Abraham", "link": "http://arxiv.org/find/cs/1/au:+Abraham_I/0/1/0/all/0/1"}, {"name": "Todd D. Murphey", "link": "http://arxiv.org/find/cs/1/au:+Murphey_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Human Activity Recognition Based on Wearable Sensor Data: A Standardization of the State-of-the-Art. (arXiv:1806.05226v1 [cs.CV])", "description": "Human activity recognition based on wearable sensor data has been an attractive research topic due to its application in areas such as healthcare, homeland security and smart environments. In this context, many works have presented remarkable results using accelerometer, gyroscope and magnetometer data to represent the categories of activities. However, the current studies do not consider important issues that lead to skewed results, making hard to measure how well sensor-based human activity recognition is and preventing a direct comparison of previous works. These issues include the employed metrics, the validation protocol used, the samples generation process, and the quality of the dataset (i.e., the sampling rate and the number of activities to be recognized). We emphasize that in other research areas, such as image classification and object detection, these issues are well-defined, which brings more efforts towards the application. Inspired by this, in this work, we conduct an extensive set of experiments to indicate the vulnerable points in human activity recognition based on wearable sensor data. To this purpose, we implement and evaluate several state-of-the-art approaches, ranging from handcrafted-based methods to convolutional neural networks. Furthermore, we standardize a large number of datasets, which vary in terms of sampling rate, number of sensors, activities and subjects. According to our study, the most of evaluation types applied in the literature are not adequate to perform the activity recognition in the context of wearable sensor data, in which the recognition accuracy drops around ten percentage points when compared to the appropriate validation. ", "link": "http://arxiv.org/abs/1806.05226", "authors": [{"name": "Artur Jordao", "link": "http://arxiv.org/find/cs/1/au:+Jordao_A/0/1/0/all/0/1"}, {"name": "Antonio C. Nazare Jr.", "link": "http://arxiv.org/find/cs/1/au:+Nazare_A/0/1/0/all/0/1"}, {"name": "Jessica Sena", "link": "http://arxiv.org/find/cs/1/au:+Sena_J/0/1/0/all/0/1"}, {"name": "William Robson Schwartz", "link": "http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Shape correspondences from learnt template-based parametrization. (arXiv:1806.05228v1 [cs.CV])", "description": "We present a new deep learning approach for matching deformable shapes by using a model which jointly encodes 3D shapes and correspondences. This is achieved by factoring the surface representation into (i) a template, that parameterizes the surface, and (ii) a learnt feature vector that parameterizes the function which transforms the template into the input surface. We show that our network can directly predict the feature vector and thus correspondences for a new input shape, but also that correspondence quality can be significantly improved by an additional regression step. This additional step improves the shape feature vector by minimizing the Chamfer distance between the input and parameterized shape. We show that this produces both a better shape representation and better correspondences. We demonstrate that our simple approach improves state of the art results on the difficult FAUST inter challenge, with an average correspondence error of 2.88cm. We also show results on the real scans from the SCAPE dataset and the synthetically perturbed shapes from the TOSCA dataset, including non-human shapes. ", "link": "http://arxiv.org/abs/1806.05228", "authors": [{"name": "Thibault Groueix", "link": "http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1"}, {"name": "Matthew Fisher", "link": "http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1"}, {"name": "Vladimir G. Kim", "link": "http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1"}, {"name": "Bryan C. Russell", "link": "http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1"}, {"name": "Mathieu Aubry", "link": "http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Identifying Recurring Patterns with Deep Neural Networks for Natural Image Denoising. (arXiv:1806.05229v1 [cs.CV])", "description": "While there is a vast diversity in the patterns and textures that occur across different varieties of natural images, the variance of such patterns within a single image is far more limited. A variety of traditional methods have exploited this self-similarity or recurrence with considerable success for image modeling, estimation, and restoration. A key challenge, however, is in accurately identifying recurring patterns within degraded image observations. This work proposes a new method for natural image denoising, that trains a deep neural network to determine whether noisy patches share common underlying patterns. Specifically, given a pair of noisy patches, the network predicts whether different transform sub-band coefficients of the original noise-free patches are the same. The denoising algorithm averages these matched coefficients to obtain an initial estimate of the clean image, with much higher quality than traditional approaches. This estimate is then refined with a second post-processing network, yielding state-of-the-art denoising performance. ", "link": "http://arxiv.org/abs/1806.05229", "authors": [{"name": "Zhihao Xia", "link": "http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1"}, {"name": "Ayan Chakrabarti", "link": "http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Dependently Typed Folds for Nested Data Types. (arXiv:1806.05230v1 [cs.LO])", "description": "We present an approach to develop folds for nested data types using dependent types. We call such folds $\\textit{dependently typed folds}$, they have the following properties. (1) Dependently typed folds are defined by well-founded recursion and they can be defined in a total dependently typed language. (2) Dependently typed folds do not depend on maps, map functions and many terminating functions can be defined using dependently typed folds. (3) The induction principles for nested data types follow from the definitions of dependently typed folds and the programs defined by dependently typed folds can be formally verified. (4) Dependently typed folds exist for any nested data types and they can be specialized to the traditional $\\textit{higher-order folds}$. Using various of examples, we show how to program and reason about dependently typed folds. We also show how to obtain dependently typed folds in general and how to specialize them to the corresponding higher-order folds. ", "link": "http://arxiv.org/abs/1806.05230", "authors": [{"name": "Peng Fu", "link": "http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1"}, {"name": "Peter Selinger", "link": "http://arxiv.org/find/cs/1/au:+Selinger_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Beyond Bags of Words: Inferring Systemic Nets. (arXiv:1806.05231v1 [cs.CL])", "description": "Textual analytics based on representations of documents as bags of words have been reasonably successful. However, analysis that requires deeper insight into language, into author properties, or into the contexts in which documents were created requires a richer representation. Systemic nets are one such representation. They have not been extensively used because they required human effort to construct. We show that systemic nets can be algorithmically inferred from corpora, that the resulting nets are plausible, and that they can provide practical benefits for knowledge discovery problems. This opens up a new class of practical analysis techniques for textual analytics. ", "link": "http://arxiv.org/abs/1806.05231", "authors": [{"name": "D.B. Skillicorn", "link": "http://arxiv.org/find/cs/1/au:+Skillicorn_D/0/1/0/all/0/1"}, {"name": "N. Alsadhan", "link": "http://arxiv.org/find/cs/1/au:+Alsadhan_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "End-to-End Parkinson Disease Diagnosis using Brain MR-Images by 3D-CNN. (arXiv:1806.05233v1 [cs.CV])", "description": "In this work, we use a deep learning framework for simultaneous classification and regression of Parkinson disease diagnosis based on MR-Images and personal information (i.e. age, gender). We intend to facilitate and increase the confidence in Parkinson disease diagnosis through our deep learning framework. ", "link": "http://arxiv.org/abs/1806.05233", "authors": [{"name": "Soheil Esmaeilzadeh", "link": "http://arxiv.org/find/cs/1/au:+Esmaeilzadeh_S/0/1/0/all/0/1"}, {"name": "Yao Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"}, {"name": "Ehsan Adeli", "link": "http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Understanding the Meaning of Understanding. (arXiv:1806.05234v1 [cs.AI])", "description": "Can we train a machine to detect if another machine has understood a concept? In principle, this is possible by conducting tests on the subject of that concept. However we want this procedure to be done by avoiding direct questions. In other words, we would like to isolate the absolute meaning of an abstract idea by putting it into a class of equivalence, hence without adopting straight definitions or showing how this idea \"works\" in practice. We discuss the metaphysical implications hidden in the above question, with the aim of providing a plausible reference framework. ", "link": "http://arxiv.org/abs/1806.05234", "authors": [{"name": "Daniele Funaro", "link": "http://arxiv.org/find/cs/1/au:+Funaro_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Manifold Mixup: Encouraging Meaningful On-Manifold Interpolation as a Regularizer. (arXiv:1806.05236v1 [stat.ML])", "description": "Deep networks often perform well on the data manifold on which they are trained, yet give incorrect (and often very confident) answers when evaluated on points from off of the training distribution. This is exemplified by the adversarial examples phenomenon but can also be seen in terms of model generalization and domain shift. We propose Manifold Mixup which encourages the network to produce more reasonable and less confident predictions at points with combinations of attributes not seen in the training set. This is accomplished by training on convex combinations of the hidden state representations of data samples. Using this method, we demonstrate improved semi-supervised learning, learning with limited labeled data, and robustness to adversarial examples. Manifold Mixup requires no (significant) additional computation. Analytical experiments on both real data and synthetic data directly support our hypothesis for why the Manifold Mixup method improves results. ", "link": "http://arxiv.org/abs/1806.05236", "authors": [{"name": "Vikas Verma", "link": "http://arxiv.org/find/stat/1/au:+Verma_V/0/1/0/all/0/1"}, {"name": "Alex Lamb", "link": "http://arxiv.org/find/stat/1/au:+Lamb_A/0/1/0/all/0/1"}, {"name": "Christopher Beckham", "link": "http://arxiv.org/find/stat/1/au:+Beckham_C/0/1/0/all/0/1"}, {"name": "Aaron Courville", "link": "http://arxiv.org/find/stat/1/au:+Courville_A/0/1/0/all/0/1"}, {"name": "Ioannis Mitliagkis", "link": "http://arxiv.org/find/stat/1/au:+Mitliagkis_I/0/1/0/all/0/1"}, {"name": "Yoshua Bengio", "link": "http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Weighted Tanimoto Coefficient for 3D Molecule Structure Similarity Measurement. (arXiv:1806.05237v1 [cs.CV])", "description": "Similarity searching of molecular structure has been an important application in the Chemoinformatics, especially in drug discovery. Similarity searching is a common method used for identification of molecular structure. It involve three main principal component of similarity searching: structure representation; weighting scheme; and similarity coefficient. In this paper, we introduces Weighted Tanimoto Coefficient based on weighted Euclidean distance in order to investigate the effect of weight function on the result for similarity searching. The Tanimoto coefficient is one of the popular similarity coefficients used to measure the similarity between pairs of the molecule. The most of research area found that the similarity searching is based on binary or fingerprint data. Meanwhile, we used non-binary data and was set amphetamine structure as a reference or targeted structure and the rest of the dataset becomes a database structure. Throughout this study, it showed that there is definitely gives a different result between a similarity searching with and without weight. ", "link": "http://arxiv.org/abs/1806.05237", "authors": [{"name": "Siti Asmah Bero", "link": "http://arxiv.org/find/cs/1/au:+Bero_S/0/1/0/all/0/1"}, {"name": "Azah Kamilah Muda", "link": "http://arxiv.org/find/cs/1/au:+Muda_A/0/1/0/all/0/1"}, {"name": "Yun-Huoy Choo", "link": "http://arxiv.org/find/cs/1/au:+Choo_Y/0/1/0/all/0/1"}, {"name": "Noor Azilah Muda", "link": "http://arxiv.org/find/cs/1/au:+Muda_N/0/1/0/all/0/1"}, {"name": "Satrya Fajri Pratama", "link": "http://arxiv.org/find/cs/1/au:+Pratama_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Quasi-tight Framelets with Directionality or High Vanishing Moments Derived from Arbitrary Refinable Functions. (arXiv:1806.05241v1 [cs.IT])", "description": "Construction of multivariate tight framelets is known to be a challenging problem. Multivariate dual framelets with vanishing moments generalize tight framelets and are not easy to be constructed either. Compactly supported multivariate framelets with directionality or high vanishing moments are of interest and importance in both theory and applications. In this paper we introduce the notion of a quasi-tight framelet, which is a dual framelet, but behaves almost like a tight framelet. Let $\\phi\\in L_2(R^d)$ be an arbitrary compactly supported $M$-refinable function such that its underlying low-pass filter satisfies the basic sum rule. We first constructively prove by a step-by-step algorithm that we can always easily derive from the arbitrary $M$-refinable function $\\phi$ a directional compactly supported quasi-tight $M$-framelet in $L_2(R^d)$ associated with a directional quasi-tight $M$-framelet filter bank, each of whose high-pass filters has only two nonzero coefficients with opposite signs. If in addition all the coefficients of its low-pass filter are nonnegative, such a quasi-tight $M$-framelet becomes a directional tight $M$-framelet in $L_2(R^d)$. Furthermore, we show by a constructive algorithm that we can always derive from the arbitrary $M$-refinable function $\\phi$ a compactly supported quasi-tight $M$-framelet in $L_2(R^d)$ with the highest possible order of vanishing moments. We shall also present a result on quasi-tight framelets whose associated high-pass filters are purely differencing filters with the highest order of vanishing moments. Several examples will be provided to illustrate our main theoretical results and algorithms in this paper. ", "link": "http://arxiv.org/abs/1806.05241", "authors": [{"name": "Chenzhe Diao", "link": "http://arxiv.org/find/cs/1/au:+Diao_C/0/1/0/all/0/1"}, {"name": "Bin Han", "link": "http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Blockchain Enabled Enhanced IoT Ecosystem Security. (arXiv:1806.05242v1 [cs.CR])", "description": "Blockchain (BC), the technology behind the Bitcoin cryptocurrency system, is starting to be adopted for ensuring enhanced security and privacy in the Internet of Things (IoT) ecosystem. Fervent research is currently being focused in both academia and industry in this domain. Proof of Work (PoW), a cryptographic puzzle, plays a vital role in ensuring BC security by maintaining a digital ledger of transactions, which are considered to be incorruptible. Furthermore, BC uses a changeable Public Key (PK) to record the identity of users, thus providing an extra layer of privacy. Not only in cryptocurrency has the successful adoption of the BC been implemented, but also in multifaceted non-monetary systems, such as in: distributed storage systems, proof of location and healthcare. Recent research articles and projects or applications were surveyed to assess the implementation of the BC for IoT Security and identify associated challenges and propose solutions for BC enabled enhanced security for the IoT ecosystem. ", "link": "http://arxiv.org/abs/1806.05242", "authors": [{"name": "Mahdi H. Miraz", "link": "http://arxiv.org/find/cs/1/au:+Miraz_M/0/1/0/all/0/1"}, {"name": "Maaruf Ali", "link": "http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "What About Applied Fairness?. (arXiv:1806.05250v1 [cs.AI])", "description": "Machine learning practitioners are often ambivalent about the ethical aspects of their products. We believe anything that gets us from that current state to one in which our systems are achieving some degree of fairness is an improvement that should be welcomed. This is true even when that progress does not get us 100% of the way to the goal of \"complete\" fairness or perfectly align with our personal belief on which measure of fairness is used. Some measure of fairness being built would still put us in a better position than the status quo. Impediments to getting fairness and ethical concerns applied in real applications, whether they are abstruse philosophical debates or technical overhead such as the introduction of ever more hyper-parameters, should be avoided. In this paper we further elaborate on our argument for this viewpoint and its importance. ", "link": "http://arxiv.org/abs/1806.05250", "authors": [{"name": "Jared Sylvester", "link": "http://arxiv.org/find/cs/1/au:+Sylvester_J/0/1/0/all/0/1"}, {"name": "Edward Raff", "link": "http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Finding your Lookalike: Measuring Face Similarity Rather than Face Identity. (arXiv:1806.05252v1 [cs.CV])", "description": "Face images are one of the main areas of focus for computer vision, receiving on a wide variety of tasks. Although face recognition is probably the most widely researched, many other tasks such as kinship detection, facial expression classification and facial aging have been examined. In this work we propose the new, subjective task of quantifying perceived face similarity between a pair of faces. That is, we predict the perceived similarity between facial images, given that they are not of the same person. Although this task is clearly correlated with face recognition, it is different and therefore justifies a separate investigation. Humans often remark that two persons look alike, even in cases where the persons are not actually confused with one another. In addition, because face similarity is different than traditional image similarity, there are challenges in data collection and labeling, and dealing with diverging subjective opinions between human labelers. We present evidence that finding facial look-alikes and recognizing faces are two distinct tasks. We propose a new dataset for facial similarity and introduce the Lookalike network, directed towards similar face classification, which outperforms the ad hoc usage of a face recognition network directed at the same task. ", "link": "http://arxiv.org/abs/1806.05252", "authors": [{"name": "Amir Sadovnik", "link": "http://arxiv.org/find/cs/1/au:+Sadovnik_A/0/1/0/all/0/1"}, {"name": "Wassim Gharbi", "link": "http://arxiv.org/find/cs/1/au:+Gharbi_W/0/1/0/all/0/1"}, {"name": "Thanh Vu", "link": "http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1"}, {"name": "Andrew Gallagher", "link": "http://arxiv.org/find/cs/1/au:+Gallagher_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "SMHD: A Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions. (arXiv:1806.05258v1 [cs.CL])", "description": "Mental health is a significant and growing public health concern. As language usage can be leveraged to obtain crucial insights into mental health conditions, there is a need for large-scale, labeled, mental health-related datasets of users who have been diagnosed with one or more of such conditions. In this paper, we investigate the creation of high-precision patterns to identify self-reported diagnoses of nine different mental health conditions, and obtain high-quality labeled data without the need for manual labelling. We introduce the SMHD (Self-reported Mental Health Diagnoses) dataset and make it available. SMHD is a novel large dataset of social media posts from users with one or multiple mental health conditions along with matched control users. We examine distinctions in users' language, as measured by linguistic and psychological variables. We further explore text classification methods to identify individuals with mental conditions through their language. ", "link": "http://arxiv.org/abs/1806.05258", "authors": [{"name": "Arman Cohan", "link": "http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1"}, {"name": "Bart Desmet", "link": "http://arxiv.org/find/cs/1/au:+Desmet_B/0/1/0/all/0/1"}, {"name": "Andrew Yates", "link": "http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1"}, {"name": "Luca Soldaini", "link": "http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1"}, {"name": "Sean MacAvaney", "link": "http://arxiv.org/find/cs/1/au:+MacAvaney_S/0/1/0/all/0/1"}, {"name": "Nazli Goharian", "link": "http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Analysis of Search Stratagem Utilisation. (arXiv:1806.05259v1 [cs.IR])", "description": "In Interactive IR, researchers consider the user behaviour towards systems and search tasks in order to adapt search results and to improve the search experience of users. Analysing the users' past interactions with the system is one typical approach. In this paper, we analyse the user behaviour in retrieval sessions towards Marcia Bates' search stratagems such as Footnote Chasing, Citation Searching, Keyword Searching, Author Searching and Journal Run in a real-life academic search engine. In fact, search stratagems represent high-level search behaviour as the users go beyond simple execution of queries and investigate more of the system functionalities. We performed analyses of these five search stratagems using two datasets extracted from the social sciences search engine sowiport. A specific focus was the detection of the search phase and frequency of the usage of these stratagems. In addition, we explored the impact of these stratagems on the whole search process performance. We addressed mainly the usage patterns' observation of the stratagems, their impact on the conduct of retrieval sessions and explore whether they are used similarly in both datasets. From the observation and metrics proposed, we can conclude that the utilisation of search stratagems in real retrieval sessions leads to an improvement of the precision in terms of positive interactions. However, the difference is that Footnote Chasing, Citation Searching and Journal Run appear mostly at the end of a session while Keyword and Author Searching appear typically at the beginning. Thus, we can conclude from the log analysis that the improvement of search functionalities including personalisation and/or recommendation could be achieved by considering references, citations, and journals in the ranking process. ", "link": "http://arxiv.org/abs/1806.05259", "authors": [{"name": "Ameni Kacem", "link": "http://arxiv.org/find/cs/1/au:+Kacem_A/0/1/0/all/0/1"}, {"name": "Philipp Mayr", "link": "http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Hybrid RF-VLC System for Energy Efficient Wireless Access. (arXiv:1806.05265v1 [cs.NI])", "description": "In this paper, we propose a new paradigm in designing and realizing energy efficient wireless indoor access networks, namely, a hybrid system enabled by traditional RF access, such as WiFi, as well as the emerging visible light communication (VLC). VLC facilitates the great advantage of being able to jointly perform illumination and communications, and little extra power beyond illumination is required to empower communications, thus rendering wireless access with almost zero power consumption. On the other hand, when illumination is not required from the light source, the energy consumed by VLC could be more than that consumed by the RF. By capitalizing on the above properties, the proposed hybrid RF-VLC system is more energy efficient and more adaptive to the illumination conditions than the individual VLC or RF systems. To demonstrate the viability of the proposed system, we first formulate the problem of minimizing the power consumption of the hybrid RF-VLC system while satisfying the users requests and maintaining acceptable level of illumination, which is NP-complete. Therefore, we divide the problem into two subproblems. In the first subproblems, we determine the set of VLC access points (AP) that needs to be turned on to satisfy the illumination requirements. Given this set, we turn our attention to satisfying the users' requests for real-time communications, and we propose a randomized online algorithm that, against an oblivious adversary, achieves a competitive ratio of $\\log(N)\\log(M)$ with probability of success $1 - \\frac{1}{N}$, where $N$ is the number of users and $M$ is the number of VLC and RF APs. We also show that the best online algorithm to solve this problem can achieve a competitive ratio of $\\log(M)$. Simulation results further demonstrate the advantages of the hybrid system. ", "link": "http://arxiv.org/abs/1806.05265", "authors": [{"name": "Abdallah Khreishah", "link": "http://arxiv.org/find/cs/1/au:+Khreishah_A/0/1/0/all/0/1"}, {"name": "Sihua Shao", "link": "http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1"}, {"name": "Ammar Gharaibeh", "link": "http://arxiv.org/find/cs/1/au:+Gharaibeh_A/0/1/0/all/0/1"}, {"name": "Moussa Ayyash", "link": "http://arxiv.org/find/cs/1/au:+Ayyash_M/0/1/0/all/0/1"}, {"name": "Hany Elgala", "link": "http://arxiv.org/find/cs/1/au:+Elgala_H/0/1/0/all/0/1"}, {"name": "Nirwan Ansari", "link": "http://arxiv.org/find/cs/1/au:+Ansari_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Online Self-supervised Scene Segmentation for Micro Aerial Vehicles. (arXiv:1806.05269v1 [cs.RO])", "description": "Recently, there have been numerous advances in the development of payload and power constrained lightweight Micro Aerial Vehicles (MAVs). As these robots aspire for high-speed autonomous flights in complex dynamic environments, robust scene understanding at long-range becomes critical. The problem is heavily characterized by either the limitations imposed by sensor capabilities for geometry-based methods, or the need for large-amounts of manually annotated training data required by data-driven methods. This motivates the need to build systems that have the capability to alleviate these problems by exploiting the complimentary strengths of both geometry and data-driven methods. In this paper, we take a step in this direction and propose a generic framework for adaptive scene segmentation using self-supervised online learning. We present this in the context of vision-based autonomous MAV flight, and demonstrate the efficacy of our proposed system through extensive experiments on benchmark datasets and real-world field tests. ", "link": "http://arxiv.org/abs/1806.05269", "authors": [{"name": "Shreyansh Daftry", "link": "http://arxiv.org/find/cs/1/au:+Daftry_S/0/1/0/all/0/1"}, {"name": "Yashasvi Agrawal", "link": "http://arxiv.org/find/cs/1/au:+Agrawal_Y/0/1/0/all/0/1"}, {"name": "Larry Matthies", "link": "http://arxiv.org/find/cs/1/au:+Matthies_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Benchmarks for Image Classification and Other High-dimensional Pattern Recognition Problems. (arXiv:1806.05272v1 [stat.ML])", "description": "A good classification method should yield more accurate results than simple heuristics. But there are classification problems, especially high-dimensional ones like the ones based on image/video data, for which simple heuristics can work quite accurately; the structure of the data in such problems is easy to uncover without any sophisticated or computationally expensive method. On the other hand, some problems have a structure that can only be found with sophisticated pattern recognition methods. We are interested in quantifying the difficulty of a given high-dimensional pattern recognition problem. We consider the case where the patterns come from two pre-determined classes and where the objects are represented by points in a high-dimensional vector space. However, the framework we propose is extendable to an arbitrarily large number of classes. We propose classification benchmarks based on simple random projection heuristics. Our benchmarks are 2D curves parameterized by the classification error and computational cost of these simple heuristics. Each curve divides the plane into a \"positive- gain\" and a \"negative-gain\" region. The latter contains methods that are ill-suited for the given classification problem. The former is divided into two by the curve asymptote; methods that lie in the small region under the curve but right of the asymptote merely provide a computational gain but no structural advantage over the random heuristics. We prove that the curve asymptotes are optimal (i.e. at Bayes error) in some cases, and thus no sophisticated method can provide a structural advantage over the random heuristics. Such classification problems, an example of which we present in our numerical experiments, provide poor ground for testing new pattern classification methods. ", "link": "http://arxiv.org/abs/1806.05272", "authors": [{"name": "Tarun Yellamraju", "link": "http://arxiv.org/find/stat/1/au:+Yellamraju_T/0/1/0/all/0/1"}, {"name": "Jonas Hepp", "link": "http://arxiv.org/find/stat/1/au:+Hepp_J/0/1/0/all/0/1"}, {"name": "Mireille Boutin", "link": "http://arxiv.org/find/stat/1/au:+Boutin_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "How Predictable is Your State? Leveraging Lexical and Contextual Information for Predicting Legislative Floor Action at the State Level. (arXiv:1806.05284v1 [cs.CL])", "description": "Modeling U.S. Congressional legislation and roll-call votes has received significant attention in previous literature. However, while legislators across 50 state governments and D.C. propose over 100,000 bills each year, and on average enact over 30% of them, state level analysis has received relatively less attention due in part to the difficulty in obtaining the necessary data. Since each state legislature is guided by their own procedures, politics and issues, however, it is difficult to qualitatively asses the factors that affect the likelihood of a legislative initiative succeeding. Herein, we present several methods for modeling the likelihood of a bill receiving floor action across all 50 states and D.C. We utilize the lexical content of over 1 million bills, along with contextual legislature and legislator derived features to build our predictive models, allowing a comparison of the factors that are important to the lawmaking process. Furthermore, we show that these signals hold complementary predictive power, together achieving an average improvement in accuracy of 18% over state specific baselines. ", "link": "http://arxiv.org/abs/1806.05284", "authors": [{"name": "Vlad Eidelman", "link": "http://arxiv.org/find/cs/1/au:+Eidelman_V/0/1/0/all/0/1"}, {"name": "Anastassia Kornilova", "link": "http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1"}, {"name": "Daniel Argyle", "link": "http://arxiv.org/find/cs/1/au:+Argyle_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Flexible Convolutional Solver with Application to Photorealistic Style Transfer. (arXiv:1806.05285v1 [cs.CV])", "description": "We propose a new flexible deep convolutional neural network (convnet) to perform fast visual style transfer. In contrast to existing convnets that address the same task, our architecture derives directly from the structure of the gradient descent originally used to solve the style transfer problem [Gatys et al., 2016]. Like existing convnets, ours approximately solves the original problem much faster than the gradient descent. However, our network is uniquely flexible by design: it can be manipulated at runtime to enforce new constraints on the final solution. In particular, we show how to modify it to obtain a photorealistic result with no retraining. We study the modifications made by [Luan et al., 2017] to the original cost function of [Gatys et al., 2016] to achieve photorealistic style transfer. These modifications affect directly the gradient descent and can be reported on-the-fly in our network. These modifications are possible as the proposed architecture stems from unrolling the gradient descent. ", "link": "http://arxiv.org/abs/1806.05285", "authors": [{"name": "Gilles Puy", "link": "http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1"}, {"name": "Patrick P&#xe9;rez", "link": "http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Bounds on the localization number. (arXiv:1806.05286v1 [math.CO])", "description": "We consider the localization game played on graphs, wherein a set of cops attempt to determine the exact location of an invisible robber by exploiting distance probes. The corresponding optimization parameter for a graph $G$ is called the localization number and is written $\\zeta (G)$. We settle a conjecture of \\cite{nisse1} by providing an upper bound on the localization number as a function of the chromatic number. In particular, we show that every graph with $\\zeta (G) \\le k$ has degeneracy less than $3^k$ and, consequently, satisfies $\\chi(G) \\le 3^{\\zeta (G)}$. We show further that this degeneracy bound is tight. We also prove that the localization number is at most 2 in outerplanar graphs, and we determine, up to an additive constant, the localization number of hypercubes. ", "link": "http://arxiv.org/abs/1806.05286", "authors": [{"name": "Anthony Bonato", "link": "http://arxiv.org/find/math/1/au:+Bonato_A/0/1/0/all/0/1"}, {"name": "William B. Kinnersley", "link": "http://arxiv.org/find/math/1/au:+Kinnersley_W/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Automatic formation of the structure of abstract machines in hierarchical reinforcement learning with state clustering. (arXiv:1806.05292v1 [cs.AI])", "description": "We introduce a new approach to hierarchy formation and task decomposition in hierarchical reinforcement learning. Our method is based on the Hierarchy Of Abstract Machines (HAM) framework because HAM approach is able to design efficient controllers that will realize specific behaviors in real robots. The key to our algorithm is the introduction of the internal or \"mental\" environment in which the state represents the structure of the HAM hierarchy. The internal action in this environment leads to changes the hierarchy of HAMs. We propose the classical Q-learning procedure in the internal environment which allows the agent to obtain an optimal hierarchy. We extends the HAM framework by adding on-model approach to select the appropriate sub-machine to execute action sequences for certain class of external environment states. Preliminary experiments demonstrated the prospects of the method. ", "link": "http://arxiv.org/abs/1806.05292", "authors": [{"name": "Aleksandr I. Panov", "link": "http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1"}, {"name": "Aleksey Skrynnik", "link": "http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Multi-View Networks for Denoising of Arbitrary Numbers of Channels. (arXiv:1806.05296v1 [eess.AS])", "description": "We propose a set of denoising neural networks capable of operating on an arbitrary number of channels at runtime, irrespective of how many channels they were trained on. We coin the proposed models multi-view networks since they operate using multiple views of the same data. We explore two such architectures and show how they outperform traditional denoising models in multi-channel scenarios. Additionally, we demonstrate how multi-view networks can leverage information provided by additional recordings to make better predictions, and how they are able to generalize to a number of recordings not seen in training. ", "link": "http://arxiv.org/abs/1806.05296", "authors": [{"name": "Jonah Casebeer", "link": "http://arxiv.org/find/eess/1/au:+Casebeer_J/0/1/0/all/0/1"}, {"name": "Brian Luc", "link": "http://arxiv.org/find/eess/1/au:+Luc_B/0/1/0/all/0/1"}, {"name": "Paris Smaragdis", "link": "http://arxiv.org/find/eess/1/au:+Smaragdis_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Pattern Dependence Detection using n-TARP Clustering. (arXiv:1806.05297v1 [stat.ML])", "description": "Consider an experiment involving a potentially small number of subjects. Some random variables are observed on each subject: a high-dimensional one called the \"observed\" random variable, and a one-dimensional one called the \"outcome\" random variable. We are interested in the dependencies between the observed random variable and the outcome random variable. We propose a method to quantify and validate the dependencies of the outcome random variable on the various patterns contained in the observed random variable. Different degrees of relationship are explored (linear, quadratic, cubic, ...). This work is motivated by the need to analyze educational data, which often involves high-dimensional data representing a small number of students. Thus our implementation is designed for a small number of subjects; however, it can be easily modified to handle a very large dataset. As an illustration, the proposed method is used to study the influence of certain skills on the course grade of students in a signal processing class. A valid dependency of the grade on the different skill patterns is observed in the data. ", "link": "http://arxiv.org/abs/1806.05297", "authors": [{"name": "Tarun Yellamraju", "link": "http://arxiv.org/find/stat/1/au:+Yellamraju_T/0/1/0/all/0/1"}, {"name": "Mireille Boutin", "link": "http://arxiv.org/find/stat/1/au:+Boutin_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Apuntes de Redes Neuronales Artificiales. (arXiv:1806.05298v1 [cs.NE])", "description": "These handouts are designed for people who is just starting involved with the topic artificial neural networks. We show how it works a single artificial neuron (McCulloch & Pitt model), mathematically and graphically. We do explain the delta rule, a learning algorithm to find the neuron weights. We also present some examples in MATLAB/Octave. There are examples for classification task for lineal and non-lineal problems. At the end, we present an artificial neural network, a feed-forward neural network along its learning algorithm backpropagation. ", "link": "http://arxiv.org/abs/1806.05298", "authors": [{"name": "J.C. Cuevas-Tello", "link": "http://arxiv.org/find/cs/1/au:+Cuevas_Tello_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Shape Features Extraction Using a Partial Differential Equation. (arXiv:1806.05299v1 [cs.CV])", "description": "This paper presents a unified method for extraction of geometrical shape features in binary image data using a linear partial differential equation (PDE). The PDE and functions are formulated to extract geometrical shape features, which are thickness, shape orientation and skeleton, all at once. The main advantages of the proposed method are it is free of any computation with respect to distance, it has no topological constraint of target image data and surfaces do not have to be distinguished to inside or outside. A one dimensional analytical solution is provided to validate the proposed method. Additionally, two- and three-dimensional numerical examples are shown to confirm the validity and usefulness of the proposed method. ", "link": "http://arxiv.org/abs/1806.05299", "authors": [{"name": "Takayuki Yamada", "link": "http://arxiv.org/find/cs/1/au:+Yamada_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Graphical Interactive Debugger for Distributed Systems. (arXiv:1806.05300v1 [cs.DC])", "description": "Designing and debugging distributed systems is notoriously difficult. The correctness of a distributed system is largely determined by its handling of failure scenarios. The sequence of events leading to a bug can be long and complex, and it is likely to include message reorderings and failures. On single-node systems, interactive debuggers enable stepping through an execution of the program, but they lack the ability to easily simulate failure scenarios and control the order in which messages are delivered. ", "link": "http://arxiv.org/abs/1806.05300", "authors": [{"name": "Doug Woos", "link": "http://arxiv.org/find/cs/1/au:+Woos_D/0/1/0/all/0/1"}, {"name": "Zachary Tatlock", "link": "http://arxiv.org/find/cs/1/au:+Tatlock_Z/0/1/0/all/0/1"}, {"name": "Michael D. Ernst", "link": "http://arxiv.org/find/cs/1/au:+Ernst_M/0/1/0/all/0/1"}, {"name": "Thomas E. Anderson", "link": "http://arxiv.org/find/cs/1/au:+Anderson_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Deep Reinforcement Learning for Dynamic Urban Transportation Problems. (arXiv:1806.05310v1 [stat.ML])", "description": "We explore the use of deep learning and deep reinforcement learning for optimization problems in transportation. Many transportation system analysis tasks are formulated as an optimization problem - such as optimal control problems in intelligent transportation systems and long term urban planning. Often transportation models used to represent dynamics of a transportation system involve large data sets with complex input-output interactions and are difficult to use in the context of optimization. Use of deep learning metamodels can produce a lower dimensional representation of those relations and allow to implement optimization and reinforcement learning algorithms in an efficient manner. In particular, we develop deep learning models for calibrating transportation simulators and for reinforcement learning to solve the problem of optimal scheduling of travelers on the network. ", "link": "http://arxiv.org/abs/1806.05310", "authors": [{"name": "Laura Schultz", "link": "http://arxiv.org/find/stat/1/au:+Schultz_L/0/1/0/all/0/1"}, {"name": "Vadim Sokolov", "link": "http://arxiv.org/find/stat/1/au:+Sokolov_V/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Rate-Splitting Robustness in Multi-Pair Massive MIMO Relay Systems. (arXiv:1806.05318v1 [cs.IT])", "description": "Relay systems improve both coverage and system capacity. Towards this direction, full-duplex (FD) technology, being able to boost the spectral efficiency by transmitting and receiving simultaneously on the same frequency and time resources, is envisaged to play a key role in future networks. However, its benefits come at the expense of self-interference (SI) from their own transmit signal. At the same time, massive MIMO systems, bringing unconventionally many antennas, emerge as a promising technology with huge degrees-of-freedom (DoF). To this end, this paper considers a multi-pair decode-and-forward FD relay channel, where the relay station is deployed with a large number of antennas. Moreover, the rate-splitting (RS) transmission has recently been shown to provide significant performance benefits in various multi-user scenarios with imperfect channel state information at the transmitter (CSIT). Engaging the RS approach, we employ the deterministic equivalent (DE) analysis to derive the corresponding sum-rates in the presence of interferences. Initially, numerical results demonstrate the robustness of RS in half-duplex (HD) systems, since the achievable sum-rate increases without bound, i.e., it does not saturate at high signal-to-noise ratio (SNR). Next, we tackle the detrimental effect of SI in FD. In particular, and most importantly, not only FD outperforms HD, but also RS enables increasing the range of SI over which FD outperforms HD. Furthermore, increasing the number of relay station antennas, RS appears to be more efficacious due to imperfect CSIT, since SI decreases. Interestingly, increasing the number of users, the efficiency of RS worsens and its implementation becomes less favorable under these conditions. Finally, we verify that the proposed DEs, being accurate for a large number of relay station antennas, are tight approximations even for realistic system dimensions. ", "link": "http://arxiv.org/abs/1806.05318", "authors": [{"name": "Anastasios Papazafeiropoulos", "link": "http://arxiv.org/find/cs/1/au:+Papazafeiropoulos_A/0/1/0/all/0/1"}, {"name": "Tharmalingam Ratnarajah", "link": "http://arxiv.org/find/cs/1/au:+Ratnarajah_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "SCSP: Spectral Clustering Filter Pruning with Soft Self-adaption Manners. (arXiv:1806.05320v1 [cs.CV])", "description": "Deep Convolutional Neural Networks (CNN) has achieved significant success in computer vision field. However, the high computational cost of the deep complex models prevents the deployment on edge devices with limited memory and computational resource. In this paper, we proposed a novel filter pruning for convolutional neural networks compression, namely spectral clustering filter pruning with soft self-adaption manners (SCSP). We first apply spectral clustering on filters layer by layer to explore their intrinsic connections and only count on efficient groups. By self-adaption manners, the pruning operations can be done in few epochs to let the network gradually choose meaningful groups. According to this strategy, we not only achieve model compression while keeping considerable performance, but also find a novel angle to interpret the model compression process. ", "link": "http://arxiv.org/abs/1806.05320", "authors": [{"name": "Huiyuan Zhuo", "link": "http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1"}, {"name": "Xuelin Qian", "link": "http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"}, {"name": "Yanwei Fu", "link": "http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"}, {"name": "Heng Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"}, {"name": "Xiangyang Xue", "link": "http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Base Station Cooperation in Millimeter Wave Cellular Networks: Performance Enhancement of Cell-Edge Users. (arXiv:1806.05325v1 [cs.IT])", "description": "Millimeter wave (mmWave) signals are much more sensitive to blockage, which results in a significant increase of the outage probability, especially for the users at the edge of the cells. In this paper, we exploit the technique of base station (BS) cooperation to improve the performance of the cell-edge users in the downlink transmission of mmWave cellular networks. We design two cooperative schemes, which are referred to as fixed-number BS cooperation (FNC) scheme and fixed-region BS cooperation (FRC) scheme, respectively. In FNC scheme, the cooperative BSs consist of the M nearest BSs around the served cell-edge users, and in FRC scheme, the cooperative BSs include all the BSs located within a given region. We derive the expressions for the average rate and outage probability of a typical cell-edge user located at the origin based on the stochastic geometry framework. To reduce the computational complexity of our analytical results for the outage probability, we further propose a Gamma approximation based method to provide approximations with satisfying accuracy. Our analytical results incorporate the critical characteristics of mmWave channels, i.e., the blockage effects, the different path loss of LOS and NLOS links and the highly directional antenna arrays. Simulation results show that the performance of the cell-edge users is greatly improved when mmWave networks are combined with the technique of BS cooperation. ", "link": "http://arxiv.org/abs/1806.05325", "authors": [{"name": "Hui-Ming Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"}, {"name": "Ke-Wen Huang", "link": "http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"}, {"name": "Theodoros A. Tsiftsis", "link": "http://arxiv.org/find/cs/1/au:+Tsiftsis_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Identifying the Fake Base Station: A Location Based Approach. (arXiv:1806.05326v1 [cs.IT])", "description": "Fake base station (FBS) attack is a great security challenge to wireless user equipment (UE). During the cell selection stage, the UE receives multiple synchronization signals (SSs) from multiple nearby base stations (BSs), and then synchronizes itself with the strongest SS. A FBS also can transmit a SS with sufficient power to confuse the UE, which makes the UE connect to the FBS, and may lead to the leakage of private information. In this letter, countermeasure to the FBS attack by utilizing the location information is investigated. Two location awareness based FBS-resistance schemes are proposed by checking the received signal strength according to the position of the UE and a legitimate BS map. The successful cheating rate (SCR) definded as the probability that the UE will connect to the FBS is investigated. Numeric results show that with the two proposed schemes, the SCR can be greatly reduced especially when the transmit power of the FBS is large. Beyond that, a cooperation aided method is further proposed to improve the performance, and we show that the cooperation aided method can further suppress the SCR when the signal strength from the FBS is similar to that from the legitimate BS. ", "link": "http://arxiv.org/abs/1806.05326", "authors": [{"name": "Ke-Wen Huang", "link": "http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"}, {"name": "Hui-Ming Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Creating and understanding email communication networks to aid digital forensic investigations. (arXiv:1806.05327v1 [cs.CR])", "description": "Digital forensic analysts depend on the ability to understand the social networks of the individuals they investigate. We develop a novel method for automatically constructing these networks from collected hard drives. We accomplish this by scanning the raw storage media for email addresses, constructing co-reference networks based on the proximity of email addresses to each other, then selecting connected components that correspond to real communication networks. We validate our analysis against a tagged data-set of networks for which we determined ground truth through interviews with the drive owners. In the resulting social networks, we find that classical measures of centrality and community detection algorithms are effective for identifying important nodes and close associates. ", "link": "http://arxiv.org/abs/1806.05327", "authors": [{"name": "Michael McCarrin", "link": "http://arxiv.org/find/cs/1/au:+McCarrin_M/0/1/0/all/0/1"}, {"name": "Janina Green", "link": "http://arxiv.org/find/cs/1/au:+Green_J/0/1/0/all/0/1"}, {"name": "Ralucca Gera", "link": "http://arxiv.org/find/cs/1/au:+Gera_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "o-glasses: Visualizing x86 Code from Binary Using a 1d-CNN. (arXiv:1806.05328v1 [cs.CR])", "description": "Malicious document files used in targeted attacks often contain a small program called shellcode. It is often hard to prepare a runnable environment for dynamic analysis of these document files because they exploit specific vulnerabilities. In these cases, it is necessary to identify the position of the shellcode in each document file to analyze it. If the exploit code uses executable scripts such as JavaScript and Flash, it is not so hard to locate the shellcode. On the other hand, it is sometimes almost impossible to locate the shellcode when it does not contain any JavaScript or Flash but consists of native x86 code only. ", "link": "http://arxiv.org/abs/1806.05328", "authors": [{"name": "Yuhei Otsubo", "link": "http://arxiv.org/find/cs/1/au:+Otsubo_Y/0/1/0/all/0/1"}, {"name": "Akira Otsuka", "link": "http://arxiv.org/find/cs/1/au:+Otsuka_A/0/1/0/all/0/1"}, {"name": "Mamoru Mimura", "link": "http://arxiv.org/find/cs/1/au:+Mimura_M/0/1/0/all/0/1"}, {"name": "Takeshi Sakaki", "link": "http://arxiv.org/find/cs/1/au:+Sakaki_T/0/1/0/all/0/1"}, {"name": "Atsuhiro Goto", "link": "http://arxiv.org/find/cs/1/au:+Goto_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Towards Implementation of Robust and Low-Cost Security Primitives for Resource-Constrained IoT Devices. (arXiv:1806.05332v1 [cs.CR])", "description": "In recent years, due to the trend in globalization, system integrators have had to deal with integrated circuit (IC)/intellectual property (IP) counterfeiting more than ever. These counterfeit hardware issues counterfeit hardware that have driven the need for more secure chip authentication. High entropy random numbers from physical sources are a critical component in authentication and encryption processes within secure systems [6]. Secure encryption is dependent on sources of truly random numbers for generating keys, and there is a need for an on chip random number generator to achieve adequate security. Furthermore, the Internet of Things (IoT) adopts a large number of these hardware-based security and prevention solutions in order to securely exchange data in resource efficient manner. In this work, we have developed several methodologies of hardware-based random functions in order to address the issues and enhance the security and trust of ICs: a novel DRAM-based intrinsic Physical Unclonable Function (PUF) [13] for system level security and authentication along with analysis of the impact of various environmental conditions, particularly silicon aging; a DRAM remanence based True Random Number Generation (TRNG) to produce random sequences with a very low overhead; a DRAM TRNG model using its startup value behavior for creating random bit streams; an efficient power supply noise based TRNG model for generating an infinite number of random bits which has been evaluated as a cost effective technique; architectures and hardware security solutions for the Internet of Things (IoT) environment. Since IoT devices are heavily resource constrained, our proposed designs can alleviate the concerns of establishing trustworthy and security in an efficient and low-cost manner. ", "link": "http://arxiv.org/abs/1806.05332", "authors": [{"name": "Fatemeh Tehranipoor", "link": "http://arxiv.org/find/cs/1/au:+Tehranipoor_F/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Hierarchical interpretations for neural network predictions. (arXiv:1806.05337v1 [cs.LG])", "description": "Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise. ", "link": "http://arxiv.org/abs/1806.05337", "authors": [{"name": "Chandan Singh", "link": "http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1"}, {"name": "W. James Murdoch", "link": "http://arxiv.org/find/cs/1/au:+Murdoch_W/0/1/0/all/0/1"}, {"name": "Bin Yu", "link": "http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "From Trailers to Storylines: An Efficient Way to Learn from Movies. (arXiv:1806.05341v1 [cs.CV])", "description": "The millions of movies produced in the human history are valuable resources for computer vision research. However, learning a vision model from movie data would meet with serious difficulties. A major obstacle is the computational cost -- the length of a movie is often over one hour, which is substantially longer than the short video clips that previous study mostly focuses on. In this paper, we explore an alternative approach to learning vision models from movies. Specifically, we consider a framework comprised of a visual module and a temporal analysis module. Unlike conventional learning methods, the proposed approach learns these modules from different sets of data -- the former from trailers while the latter from movies. This allows distinctive visual features to be learned within a reasonable budget while still preserving long-term temporal structures across an entire movie. We construct a large-scale dataset for this study and define a series of tasks on top. Experiments on this dataset showed that the proposed method can substantially reduce the training time while obtaining highly effective features and coherent temporal structures. ", "link": "http://arxiv.org/abs/1806.05341", "authors": [{"name": "Qingqiu Huang", "link": "http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"}, {"name": "Yuanjun Xiong", "link": "http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"}, {"name": "Yu Xiong", "link": "http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"}, {"name": "Yuqi Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"}, {"name": "Dahua Lin", "link": "http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Convex Class Model on Symmetric Positive Definite Manifolds. (arXiv:1806.05343v1 [cs.CV])", "description": "The effectiveness of Symmetric Positive Definite (SPD) manifold features has been proven in various computer vision tasks. However, due to the non-Euclidean geometry of these features, existing Euclidean machineries cannot be directly used. In this paper, we tackle the classification tasks with limited training data on SPD manifolds. Our proposed framework, named Manifold Convex Class Model, represents each class on SPD manifolds using a convex model and classification can be performed by computing distances to the convex models. We provide three methods based on different metrics to address the optimization problem of the smallest distance of a point to the convex model on SPD manifold. The efficacy of our proposed framework is demonstrated both on synthetic data and several computer vision tasks including object recognition, texture classification, person re-identification and traffic scene classification. ", "link": "http://arxiv.org/abs/1806.05343", "authors": [{"name": "Kun Zhao", "link": "http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"}, {"name": "Arnold Wiliem", "link": "http://arxiv.org/find/cs/1/au:+Wiliem_A/0/1/0/all/0/1"}, {"name": "Shaokang Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"}, {"name": "Brian C. Lovell", "link": "http://arxiv.org/find/cs/1/au:+Lovell_B/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Scalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization. (arXiv:1806.05355v1 [stat.ML])", "description": "We propose a simple and easy to implement neural network compression algorithm that achieves results competitive with more complicated state-of-the-art methods. The key idea is to modify the original optimization problem by adding K independent Gaussian priors (corresponding to the k-means objective) over the network parameters to achieve parameter quantization, as well as an L1 penalty to achieve pruning. Unlike many existing quantization-based methods, our method uses hard clustering assignments of network parameters, which adds minimal change or overhead to standard network training. We also demonstrate experimentally that tying neural network parameters provides less gain in generalization performance than changing network architecture and connectivity patterns entirely. ", "link": "http://arxiv.org/abs/1806.05355", "authors": [{"name": "Yibo Yang", "link": "http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1"}, {"name": "Nicholas Ruozzi", "link": "http://arxiv.org/find/stat/1/au:+Ruozzi_N/0/1/0/all/0/1"}, {"name": "Vibhav Gogate", "link": "http://arxiv.org/find/stat/1/au:+Gogate_V/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Finding GEMS: Multi-Scale Dictionaries for High-Dimensional Graph Signals. (arXiv:1806.05356v1 [cs.LG])", "description": "Modern data introduces new challenges to classic signal processing approaches, leading to a growing interest in the field of graph signal processing. A powerful and well established model for real world signals in various domains is sparse representation over a dictionary, combined with the ability to train the dictionary from signal examples. This model has been successfully applied to graph signals as well by integrating the underlying graph topology into the learned dictionary. Nonetheless, dictionary learning methods for graph signals are typically restricted to small dimensions due to the computational constraints that the dictionary learning problem entails, and due to the direct use of the graph Laplacian matrix. In this paper, we propose a dictionary learning algorithm that applies to a broader class of graph signals, and is capable of handling much higher dimensional data. We incorporate the underlying graph topology both implicitly, by forcing the learned dictionary atoms to be sparse combinations of graph-wavelet functions, and explicitly, by adding direct graph constraints to promote smoothness in both the feature and manifold domains. The resulting atoms are thus adapted to the data of interest while adhering to the underlying graph structure and possessing a desired multi-scale property. Experimental results on several datasets, representing both synthetic and real network data of different nature, demonstrate the effectiveness of the proposed algorithm for graph signal processing even in high dimensions. ", "link": "http://arxiv.org/abs/1806.05356", "authors": [{"name": "Yael Yankelevsky", "link": "http://arxiv.org/find/cs/1/au:+Yankelevsky_Y/0/1/0/all/0/1"}, {"name": "Michael Elad", "link": "http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Deep Multi-Output Forecasting: Learning to Accurately Predict Blood Glucose Trajectories. (arXiv:1806.05357v1 [cs.LG])", "description": "In many forecasting applications, it is valuable to predict not only the value of a signal at a certain time point in the future, but also the values leading up to that point. This is especially true in clinical applications, where the future state of the patient can be less important than the patient's overall trajectory. This requires multi-step forecasting, a forecasting variant where one aims to predict multiple values in the future simultaneously. Standard methods to accomplish this can propagate error from prediction to prediction, reducing quality over the long term. In light of these challenges, we propose multi-output deep architectures for multi-step forecasting in which we explicitly model the distribution of future values of the signal over a prediction horizon. We apply these techniques to the challenging and clinically relevant task of blood glucose forecasting. Through a series of experiments on a real-world dataset consisting of 550K blood glucose measurements, we demonstrate the effectiveness of our proposed approaches in capturing the underlying signal dynamics. Compared to existing shallow and deep methods, we find that our proposed approaches improve performance individually and capture complementary information, leading to a large improvement over the baseline when combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, the results suggest the efficacy of our proposed approach in predicting blood glucose level and multi-step forecasting more generally. ", "link": "http://arxiv.org/abs/1806.05357", "authors": [{"name": "Ian Fox", "link": "http://arxiv.org/find/cs/1/au:+Fox_I/0/1/0/all/0/1"}, {"name": "Lynn Ang", "link": "http://arxiv.org/find/cs/1/au:+Ang_L/0/1/0/all/0/1"}, {"name": "Mamta Jaiswal", "link": "http://arxiv.org/find/cs/1/au:+Jaiswal_M/0/1/0/all/0/1"}, {"name": "Rodica Pop-Busui", "link": "http://arxiv.org/find/cs/1/au:+Pop_Busui_R/0/1/0/all/0/1"}, {"name": "Jenna Wiens", "link": "http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning. (arXiv:1806.05358v1 [cs.LG])", "description": "In this paper, we study robust large-scale distributed learning in the presence of saddle points in non-convex loss functions. We consider the Byzantine setting where some worker machines may have abnormal or even arbitrary and adversarial behavior. We argue that in the Byzantine setting, optimizing a non-convex function and escaping saddle points become much more challenging, even when robust gradient estimators are used. We develop ByzantinePGD, a robust and communication-efficient algorithm that can provably escape saddle points and converge to approximate local minimizers. The iteration complexity of our algorithm in the Byzantine setting matches that of standard gradient descent in the usual setting. We further provide three robust aggregation subroutines that can be used in ByzantinePGD, including median, trimmed mean, and iterative filtering. We characterize their performance in statistical settings, and argue for their near-optimality in different regimes including the high dimensional setting. ", "link": "http://arxiv.org/abs/1806.05358", "authors": [{"name": "Dong Yin", "link": "http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"}, {"name": "Yudong Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"}, {"name": "Kannan Ramchandran", "link": "http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1"}, {"name": "Peter Bartlett", "link": "http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Game Theoretic Approach to Learning and Dynamics in Information Retrieval. (arXiv:1806.05359v1 [cs.GT])", "description": "We consider a game-theoretic model of information retrieval with strategic authors. We examine two different utility schemes: authors who aim at maximizing exposure and authors who want to maximize active selection of their content (i.e. the number of clicks). We introduce the study of author learning dynamics in such contexts. We prove that under the probability ranking principle (PRP), which forms the basis of the current state of the art ranking methods, any better-response learning dynamics converges to a pure Nash equilibrium. We also show that other ranking methods induce a strategic environment under which such a convergence may not occur. ", "link": "http://arxiv.org/abs/1806.05359", "authors": [{"name": "Omer Ben-Porat", "link": "http://arxiv.org/find/cs/1/au:+Ben_Porat_O/0/1/0/all/0/1"}, {"name": "Itay Rosenberg", "link": "http://arxiv.org/find/cs/1/au:+Rosenberg_I/0/1/0/all/0/1"}, {"name": "Moshe Tennenholtz", "link": "http://arxiv.org/find/cs/1/au:+Tennenholtz_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "View-volume Network for Semantic Scene Completion from a Single Depth Image. (arXiv:1806.05361v1 [cs.CV])", "description": "We introduce a View-Volume convolutional neural network (VVNet) for inferring the occupancy and semantic labels of a volumetric 3D scene from a single depth image. The VVNet concatenates a 2D view CNN and a 3D volume CNN with a differentiable projection layer. Given a single RGBD image, our method extracts the detailed geometric features from the input depth image with a 2D view CNN and then projects the features into a 3D volume according to the input depth map via a projection layer. After that, we learn the 3D context information of the scene with a 3D volume CNN for computing the result volumetric occupancy and semantic labels. With combined 2D and 3D representations, the VVNet efficiently reduces the computational cost, enables feature extraction from multi-channel high resolution inputs, and thus significantly improves the result accuracy. We validate our method and demonstrate its efficiency and effectiveness on both synthetic SUNCG and real NYU dataset. ", "link": "http://arxiv.org/abs/1806.05361", "authors": [{"name": "Yu-Xiao Guo", "link": "http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"}, {"name": "Xin Tong", "link": "http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Fire SSD: Wide Fire Modules based Single Shot Detector on Edge Device. (arXiv:1806.05363v1 [cs.CV])", "description": "With the emergence of edge computing, there is an increasing need for running convolutional neural network based object detection on small form factor edge computing devices with limited compute and thermal budget for applications such as video surveillance. To address this problem, efficient object detection frameworks such as YOLO and SSD were proposed. However, SSD based object detection that uses VGG16 as backend network is insufficient to achieve real time speed on edge devices. To further improve the detection speed, the backend network is replaced by more efficient networks such as SqueezeNet and MobileNet. Although the speed is greatly improved, it comes with a price of lower accuracy. In this paper, we propose an efficient SSD named Fire SSD. Fire SSD achieves 70.7mAP on Pascal VOC 2007 test set. Fire SSD achieves the speed of 30.6FPS on low power mainstream CPU and is about 6 times faster than SSD300 and has about 4 times smaller model size. Fire SSD also achieves 22.2FPS on integrated GPU. ", "link": "http://arxiv.org/abs/1806.05363", "authors": [{"name": "Hengfui Liau", "link": "http://arxiv.org/find/cs/1/au:+Liau_H/0/1/0/all/0/1"}, {"name": "Nimmagadda Yamini", "link": "http://arxiv.org/find/cs/1/au:+Yamini_N/0/1/0/all/0/1"}, {"name": "YengLiong Wong", "link": "http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition. (arXiv:1806.05372v1 [cs.CV])", "description": "Attention-based learning for fine-grained image recognition remains a challenging task, where most of the existing methods treat each object part in isolation, while neglecting the correlations among them. In addition, the multi-stage or multi-scale mechanisms involved make the existing methods less efficient and hard to be trained end-to-end. In this paper, we propose a novel attention-based convolutional neural network (CNN) which regulates multiple object parts among different input images. Our method first learns multiple attention region features of each input image through the one-squeeze multi-excitation (OSME) module, and then apply the multi-attention multi-class constraint (MAMC) in a metric learning framework. For each anchor feature, the MAMC functions by pulling same-attention same-class features closer, while pushing different-attention or different-class features away. Our method can be easily trained end-to-end, and is highly efficient which requires only one training stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog species dataset that surpasses similar existing datasets by category coverage, data volume and annotation quality. This dataset will be released upon acceptance to facilitate the research of fine-grained image recognition. Extensive experiments are conducted to show the substantial improvements of our method on four benchmark datasets. ", "link": "http://arxiv.org/abs/1806.05372", "authors": [{"name": "Ming Sun", "link": "http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"}, {"name": "Yuchen Yuan", "link": "http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"}, {"name": "Feng Zhou", "link": "http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"}, {"name": "Errui Ding", "link": "http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Single Image Reflection Separation with Perceptual Losses. (arXiv:1806.05376v1 [cs.CV])", "description": "We present an approach to separating reflection from a single image. The approach uses a fully convolutional network trained end-to-end with losses that exploit low-level and high-level image information. Our loss function includes two perceptual losses: a feature loss from a visual perception network, and an adversarial loss that encodes characteristics of images in the transmission layers. We also propose a novel exclusion loss that enforces pixel-level layer separation. We create a dataset of real-world images with reflection and corresponding ground-truth transmission layers for quantitative evaluation and model training. We validate our method through comprehensive quantitative experiments and show that our approach outperforms state-of-the-art reflection removal methods in PSNR, SSIM, and perceptual user study. We also extend our method to two other image enhancement tasks to demonstrate the generality of our approach. ", "link": "http://arxiv.org/abs/1806.05376", "authors": [{"name": "Xuaner Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"}, {"name": "Ren Ng", "link": "http://arxiv.org/find/cs/1/au:+Ng_R/0/1/0/all/0/1"}, {"name": "Qifeng Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Performance of Caching-Based D2D Video Distribution with Measured Popularity Distributions. (arXiv:1806.05380v1 [cs.NI])", "description": "On-demand video accounts for the majority of wireless data traffic. Video distribution schemes based on caching combined with device-to-device (D2D) communications promise order-of-magnitude greater spectral efficiency for video delivery, but hinge on the principle of \"concentrated demand distributions.\" This letter presents, for the first time, evaluations of the spectral efficiency of such schemes based on measured cellular demand distributions. In particular, we use a database with more than 100 million requests (689,461 for cellular users) from the BBC iPlayer, a popular video streaming service in the U.K., to evaluate the throughput-outage tradeoff of a random caching D2D based scheme, and find that also for this realistic case, order-of-magnitude improvements can be achieved. The gains depend on the size of the local cache in the devices; e.g., with a cache size of 32 GB, a throughput increase of two orders of magnitude at an outage probability between 0.01 and 0.1 can be obtained. ", "link": "http://arxiv.org/abs/1806.05380", "authors": [{"name": "Ming-Chun Lee", "link": "http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"}, {"name": "Mingyue Ji", "link": "http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1"}, {"name": "Andreas F. Molisch", "link": "http://arxiv.org/find/cs/1/au:+Molisch_A/0/1/0/all/0/1"}, {"name": "Nishanth Sastry", "link": "http://arxiv.org/find/cs/1/au:+Sastry_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "PCAS: Pruning Channels with Attention Statistics. (arXiv:1806.05382v1 [stat.ML])", "description": "To implement deep neural networks on small embedded devices, conventional techniques use channel pruning looking considering manual compression rate per layer to reduce parameters. Besides it is difficult to consider the relationships between layers and it takes a lot of time for deeper models. For addressing these issues, we propose a new channel pruning technique based on attention that can evaluate the importance of channels. We improved the method with the criterion to allow the automatic channel selection using a single compression rate for the entire model. Experimental results showed that a parameter reduction of 90.8% and FLOPs reduction of 79.4% was achieved with an accuracy degradation of around 1% for the compressed ResNet-50 model on the CIFAR-10 benchmark. ", "link": "http://arxiv.org/abs/1806.05382", "authors": [{"name": "Kohei Yamamoto", "link": "http://arxiv.org/find/stat/1/au:+Yamamoto_K/0/1/0/all/0/1"}, {"name": "Kurato Maeno", "link": "http://arxiv.org/find/stat/1/au:+Maeno_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Perceptual Rasterization for Head-mounted Display Image Synthesis. (arXiv:1806.05385v1 [cs.GR])", "description": "We suggest a rasterization pipeline tailored towards the need of head-mounted displays (HMD), where latency and field-of-view requirements pose new challenges beyond those of traditional desktop displays. Instead of rendering and warping for low latency, or using multiple passes for foveation, we show how both can be produced directly in a single perceptual rasterization pass. We do this with per-fragment ray-casting. This is enabled by derivations of tight space-time-fovea pixel bounds, introducing just enough flexibility for requisite geometric tests, but retaining most of the the simplicity and efficiency of the traditional rasterizaton pipeline. To produce foveated images, we rasterize to an image with spatially varying pixel density. To reduce latency, we extend the image formation model to directly produce \"rolling\" images where the time at each pixel depends on its display location. Our approach overcomes limitations of warping with respect to disocclusions, object motion and view-dependent shading, as well as geometric aliasing artifacts in other foveated rendering techniques. A set of perceptual user studies demonstrates the efficacy of our approach. ", "link": "http://arxiv.org/abs/1806.05385", "authors": [{"name": "Tobias Ritschel", "link": "http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1"}, {"name": "Sebastian Friston", "link": "http://arxiv.org/find/cs/1/au:+Friston_S/0/1/0/all/0/1"}, {"name": "Anthony Steed", "link": "http://arxiv.org/find/cs/1/au:+Steed_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Parameter Learning and Change Detection Using a Particle Filter With Accelerated Adaptation. (arXiv:1806.05387v1 [stat.ML])", "description": "This paper presents the construction of a particle filter, which incorporates elements inspired by genetic algorithms, in order to achieve accelerated adaptation of the estimated posterior distribution to changes in model parameters. Specifically, the filter is designed for the situation where the subsequent data in online sequential filtering does not match the model posterior filtered based on data up to a current point in time. The examples considered encompass parameter regime shifts and stochastic volatility. The filter adapts to regime shifts extremely rapidly and delivers a clear heuristic for distinguishing between regime shifts and stochastic volatility, even though the model dynamics assumed by the filter exhibit neither of those features. ", "link": "http://arxiv.org/abs/1806.05387", "authors": [{"name": "Karol Gellert", "link": "http://arxiv.org/find/stat/1/au:+Gellert_K/0/1/0/all/0/1"}, {"name": "Erik Schl&#xf6;gl", "link": "http://arxiv.org/find/stat/1/au:+Schlogl_E/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Theory of Estimation-of-Distribution Algorithms. (arXiv:1806.05392v1 [cs.NE])", "description": "Estimation-of-distribution algorithms (EDAs) are general metaheuristics used in optimization that represent a more recent alternative to classical approaches like evolutionary algorithms. In a nutshell, EDAs typically do not directly evolve populations of search points but build probabilistic models of promising solutions by repeatedly sampling and selecting points from the underlying search space. Recently, there has been made significant progress in the theoretical understanding of EDAs. This article provides an up-to-date overview of the most commonly analyzed EDAs and the most recent theoretical results in this area. In particular, emphasis is put on the runtime analysis of simple univariate EDAs, including a description of typical benchmark functions and tools for the analysis. Along the way, open problems and directions for future research are described. ", "link": "http://arxiv.org/abs/1806.05392", "authors": [{"name": "Martin S. Krejca", "link": "http://arxiv.org/find/cs/1/au:+Krejca_M/0/1/0/all/0/1"}, {"name": "Carsten Witt", "link": "http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks. (arXiv:1806.05393v1 [stat.ML])", "description": "In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures. ", "link": "http://arxiv.org/abs/1806.05393", "authors": [{"name": "Lechao Xiao", "link": "http://arxiv.org/find/stat/1/au:+Xiao_L/0/1/0/all/0/1"}, {"name": "Yasaman Bahri", "link": "http://arxiv.org/find/stat/1/au:+Bahri_Y/0/1/0/all/0/1"}, {"name": "Jascha Sohl-Dickstein", "link": "http://arxiv.org/find/stat/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1"}, {"name": "Samuel S. Schoenholz", "link": "http://arxiv.org/find/stat/1/au:+Schoenholz_S/0/1/0/all/0/1"}, {"name": "Jeffrey Pennington", "link": "http://arxiv.org/find/stat/1/au:+Pennington_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks. (arXiv:1806.05394v1 [stat.ML])", "description": "Recurrent neural networks have gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an input. We show that this theory predicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings. Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly improvement in training dynamics. Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task. ", "link": "http://arxiv.org/abs/1806.05394", "authors": [{"name": "Minmin Chen", "link": "http://arxiv.org/find/stat/1/au:+Chen_M/0/1/0/all/0/1"}, {"name": "Jeffrey Pennington", "link": "http://arxiv.org/find/stat/1/au:+Pennington_J/0/1/0/all/0/1"}, {"name": "Samuel S. Schoenholz", "link": "http://arxiv.org/find/stat/1/au:+Schoenholz_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "On the Perceptron's Compression. (arXiv:1806.05403v1 [cs.LG])", "description": "We study and provide exposition to several phenomena that are related to the perceptron's compression. One theme concerns modifications of the perceptron algorithm that yield better guarantees on the margin of the hyperplane it outputs. These modifications can be useful in training neural networks as well, and we demonstrate them with some experimental data. In a second theme, we deduce conclusions from the perceptron's compression in various contexts. ", "link": "http://arxiv.org/abs/1806.05403", "authors": [{"name": "Shay Moran", "link": "http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1"}, {"name": "Ido Nachum", "link": "http://arxiv.org/find/cs/1/au:+Nachum_I/0/1/0/all/0/1"}, {"name": "Itai Panasoff", "link": "http://arxiv.org/find/cs/1/au:+Panasoff_I/0/1/0/all/0/1"}, {"name": "Amir Yehudayoff", "link": "http://arxiv.org/find/cs/1/au:+Yehudayoff_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Micro Congestion Control: Every Flow Deserves a Second Chance. (arXiv:1806.05406v1 [cs.NI])", "description": "Today, a considerable Internet traffic is sent from the datacenter and headed for users. With continuous upgrades in network infrastructure and user devices, the characteristics of connections served by servers in the datacenter are usually diverse and varied over time. As a result, one particular congestion control algorithm is hard to accommodate the heterogeneity and perform well in all scenarios. In this paper, we present Micro Congestion Control (MCC) --- a novel framework for Internet congestion control. With MCC, diverse algorithms can be assigned to connections in one server to cover the heterogeneity, and multiple algorithms can be applied in each connection's life cycle to keep pace with the dynamic of the Internet. We design and implement MCC in Linux, and experiments show that MCC is capable of smoothly switching among various candidate algorithms on the fly to achieve potential performance gain. Meanwhile, the overheads introduced by MCC are moderate and acceptable. ", "link": "http://arxiv.org/abs/1806.05406", "authors": [{"name": "Kefan Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"}, {"name": "Fengyuan Ren", "link": "http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning Dynamics of Linear Denoising Autoencoders. (arXiv:1806.05413v1 [stat.ML])", "description": "Denoising autoencoders (DAEs) have proven useful for unsupervised representation learning, but a thorough theoretical understanding is still lacking of how the input noise influences learning. Here we develop theory for how noise influences learning in DAEs. By focusing on linear DAEs, we are able to derive analytic expressions that exactly describe their learning dynamics. We verify our theoretical predictions with simulations as well as experiments on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise allows DAEs to ignore low variance directions in the inputs while learning to reconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs to standard regularised autoencoders, we show that noise has a similar regularisation effect to weight decay, but with faster training dynamics. We also show that our theoretical predictions approximate learning dynamics on real-world data and qualitatively match observed dynamics in nonlinear DAEs. ", "link": "http://arxiv.org/abs/1806.05413", "authors": [{"name": "Arnu Pretorius", "link": "http://arxiv.org/find/stat/1/au:+Pretorius_A/0/1/0/all/0/1"}, {"name": "Steve Kroon", "link": "http://arxiv.org/find/stat/1/au:+Kroon_S/0/1/0/all/0/1"}, {"name": "Herman Kamper", "link": "http://arxiv.org/find/stat/1/au:+Kamper_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Configurable Markov Decision Processes. (arXiv:1806.05415v1 [cs.AI])", "description": "In many real-world problems, there is the possibility to configure, to a limited extent, some environmental parameters to improve the performance of a learning agent. In this paper, we propose a novel framework, Configurable Markov Decision Processes (Conf-MDPs), to model this new type of interaction with the environment. Furthermore, we provide a new learning algorithm, Safe Policy-Model Iteration (SPMI), to jointly and adaptively optimize the policy and the environment configuration. After having introduced our approach and derived some theoretical results, we present the experimental evaluation in two explicative problems to show the benefits of the environment configurability on the performance of the learned policy. ", "link": "http://arxiv.org/abs/1806.05415", "authors": [{"name": "Alberto Maria Metelli", "link": "http://arxiv.org/find/cs/1/au:+Metelli_A/0/1/0/all/0/1"}, {"name": "Mirco Mutti", "link": "http://arxiv.org/find/cs/1/au:+Mutti_M/0/1/0/all/0/1"}, {"name": "Marcello Restelli", "link": "http://arxiv.org/find/cs/1/au:+Restelli_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Ranking Recovery from Limited Comparisons using Low-Rank Matrix Completion. (arXiv:1806.05419v1 [stat.ML])", "description": "This paper proposes a new method for solving the well-known rank aggregation problem from pairwise comparisons using the method of low-rank matrix completion. The partial and noisy data of pairwise comparisons is transformed into a matrix form. We then use tools from matrix completion, which has served as a major component in the low-rank completion solution of the Netflix challenge, to construct the preference of the different objects. In our approach, the data of multiple comparisons is used to create an estimate of the probability of object i to win (or be chosen) over object j, where only a partial set of comparisons between N objects is known. The data is then transformed into a matrix form for which the noiseless solution has a known rank of one. An alternating minimization algorithm, in which the target matrix takes a bilinear form, is then used in combination with maximum likelihood estimation for both factors. The reconstructed matrix is used to obtain the true underlying preference intensity. This work demonstrates the improvement of our proposed algorithm over the current state-of-the-art in both simulated scenarios and real data. ", "link": "http://arxiv.org/abs/1806.05419", "authors": [{"name": "Tal Levy", "link": "http://arxiv.org/find/stat/1/au:+Levy_T/0/1/0/all/0/1"}, {"name": "Alireza Vahid", "link": "http://arxiv.org/find/stat/1/au:+Vahid_A/0/1/0/all/0/1"}, {"name": "Raja Giryes", "link": "http://arxiv.org/find/stat/1/au:+Giryes_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Selfless Sequential Learning. (arXiv:1806.05421v1 [stat.ML])", "description": "Sequential learning studies the problem of learning tasks in a sequence with restricted access to only the data of the current task. In the setting with a fixed model capacity, the learning process should not be selfish and account for later tasks to be added and therefore aim at utilizing a minimum number of neurons, leaving enough capacity for future needs. We explore different regularization strategies and activation functions that could lead to less interference between the different tasks. We show that learning a sparse representation is more beneficial for sequential learning than encouraging parameter sparsity regardless of their corresponding neurons. We particularly propose a novel regularizer that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. We combine our regularizer with state-of-the-art lifelong learning methods that penalize changes on important previously learned parts of the network. We show that increased sparsity translates in a performance improvement on the different tasks that are learned in a sequence. ", "link": "http://arxiv.org/abs/1806.05421", "authors": [{"name": "Rahaf Aljundi", "link": "http://arxiv.org/find/stat/1/au:+Aljundi_R/0/1/0/all/0/1"}, {"name": "Marcus Rohrbach", "link": "http://arxiv.org/find/stat/1/au:+Rohrbach_M/0/1/0/all/0/1"}, {"name": "Tinne Tuytelaars", "link": "http://arxiv.org/find/stat/1/au:+Tuytelaars_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "How to design browser security and privacy alerts. (arXiv:1806.05426v1 [cs.HC])", "description": "It is important to design browser security and privacy alerts so as to maximise their value to the end user, and their efficacy in terms of communicating risk. We derived a list of design guidelines from the research literature by carrying out a systematic review. We analysed the papers both quantitatively and qualitatively to arrive at a comprehensive set of guidelines. Our findings aim to to provide designers and developers with guidance as to how to construct privacy and security alerts. We conclude by providing an alert template,highlighting its adherence to the derived guidelines. ", "link": "http://arxiv.org/abs/1806.05426", "authors": [{"name": "Lynsay A. Shepherd", "link": "http://arxiv.org/find/cs/1/au:+Shepherd_L/0/1/0/all/0/1"}, {"name": "Karen Renaud", "link": "http://arxiv.org/find/cs/1/au:+Renaud_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Maximum weight spectrum codes with reduced length. (arXiv:1806.05427v1 [cs.IT])", "description": "A q-ary linear code of dimension k is called a maximum weight spectrum (MWS) code if it has the maximum possible number (viz. (q^k-1)/(q-1)) of different non-zero weights. We construct MWS codes from quasi-minimal codes, thus obtaining of much shorter length than hitherto known. By an averaging argument, we show the existence of MWS codes of even shorter length. ", "link": "http://arxiv.org/abs/1806.05427", "authors": [{"name": "Gerard D Cohen", "link": "http://arxiv.org/find/cs/1/au:+Cohen_G/0/1/0/all/0/1"}, {"name": "Ludo Tolhuizen", "link": "http://arxiv.org/find/cs/1/au:+Tolhuizen_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "An Effective Privacy-Preserving Data Coding in Peer-To-Peer Network. (arXiv:1806.05430v1 [cs.CR])", "description": "Coding Opportunistically (COPE) is a simple but very effective data coding mechanism in the wireless network. However, COPE leaves risks for attackers easily getting the private information saved in the packets, when they move through the network to their destination nodes. Hence in our work, a lightweight cryptographic approach, namely SCOPE, is proposed to consolidate COPE against the honest-but-curious and malicious attacks. Honest-but-curious attack serves adversaries who accurately obey the protocol but try to learn as much private information as possible for their curiosity. Additionally, this kind of attack is not destructive consequently. However, it may leave the backdoor for the more dangerous attacks carrying catastrophes to the system. Malicious attack tries to learn not only the private information but also modifies the packet on harmful purposes. In our work, the SCOPE protocol is defensive to the both attacks. The private information in the COPE packet are encrypted by Elliptic Curve Cryptography (ECC), and an additional information is inserted into SCOPE packets served for the authentication process using the lightweight hash Elliptic Curve Digital Signature Algorithm (ECDSA). We then prove our new protocol is still guaranteed to be a secure method of data coding, and to be light to effectively operate in the peer-to-peer wireless network ", "link": "http://arxiv.org/abs/1806.05430", "authors": [{"name": "Ngoc Hong Tran", "link": "http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1"}, {"name": "Cao-Vien Phung", "link": "http://arxiv.org/find/cs/1/au:+Phung_C/0/1/0/all/0/1"}, {"name": "Binh Quoc Nguyen", "link": "http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"}, {"name": "Leila Bahri", "link": "http://arxiv.org/find/cs/1/au:+Bahri_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Urdu Word Segmentation using Conditional Random Fields (CRFs). (arXiv:1806.05432v1 [cs.CL])", "description": "State-of-the-art Natural Language Processing algorithms rely heavily on efficient word segmentation. Urdu is amongst languages for which word segmentation is a complex task as it exhibits space omission as well as space insertion issues. This is partly due to the Arabic script which although cursive in nature, consists of characters that have inherent joining and non-joining attributes regardless of word boundary. This paper presents a word segmentation system for Urdu which uses a Conditional Random Field sequence modeler with orthographic, linguistic and morphological features. Our proposed model automatically learns to predict white space as word boundary as well as Zero Width Non-Joiner (ZWNJ) as sub-word boundary. Using a manually annotated corpus, our model achieves F1 score of 0.97 for word boundary identification and 0.85 for sub-word boundary identification tasks. We have made our code and corpus publicly available to make our results reproducible. ", "link": "http://arxiv.org/abs/1806.05432", "authors": [{"name": "Haris Bin Zia", "link": "http://arxiv.org/find/cs/1/au:+Zia_H/0/1/0/all/0/1"}, {"name": "Agha Ali Raza", "link": "http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1"}, {"name": "Awais Athar", "link": "http://arxiv.org/find/cs/1/au:+Athar_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce. (arXiv:1806.05434v1 [cs.CL])", "description": "Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multi-turn conversation model based on convolutional neural networks. After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our model in an industrial chatbot called AliMe Assist (https://consumerservice.taobao.com/online-help) and observed a significant improvement over the existing online model. ", "link": "http://arxiv.org/abs/1806.05434", "authors": [{"name": "Minghui Qiu", "link": "http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1"}, {"name": "Liu Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"}, {"name": "Feng Ji", "link": "http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1"}, {"name": "Weipeng Zhao", "link": "http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"}, {"name": "Wei Zhou", "link": "http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"}, {"name": "Jun Huang", "link": "http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"}, {"name": "Haiqing Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"}, {"name": "W. Bruce Croft", "link": "http://arxiv.org/find/cs/1/au:+Croft_W/0/1/0/all/0/1"}, {"name": "Wei Lin", "link": "http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "ServeNet: A Deep Neural Network for Web Service Classification. (arXiv:1806.05437v1 [cs.LG])", "description": "Automated service classification plays a crucial role in service management such as service discovery, selection, and composition. In recent years, machine learning techniques have been used for service classification. However, they can only predict around 10 to 20 service categories due to the quality of feature engineering and the imbalance problem of service dataset. In this paper, we present a deep neural network ServeNet with a novel dataset splitting algorithm to deal with these issues. ServeNet can automatically abstract low-level representation to high-level features, and then predict service classification based on the service datasets produced by the proposed splitting algorithm. To demonstrate the effectiveness of our approach, we conducted a comprehensive experimental study on 10,000 real-world services in 50 categories. The result shows that ServeNet can achieve higher accuracy than other machine learning methods. ", "link": "http://arxiv.org/abs/1806.05437", "authors": [{"name": "Yilong Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"}, {"name": "Peng Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"}, {"name": "Lianchao Ding", "link": "http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"}, {"name": "Bingqing Shen", "link": "http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"}, {"name": "Weiru Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors. (arXiv:1806.05438v1 [stat.ML])", "description": "We consider stochastic gradient descent for binary classification problems in a reproducing kernel Hilbert space. In traditional analysis, it is known that the expected classification error converges more slowly than the expected risk even when assuming a low-noise condition on the conditional label probabilities. Consequently, the resulting rate is sublinear. Therefore, it is important to consider whether much faster convergence of the expected classification error can be achieved. In recent research, an exponential convergence rate for stochastic gradient descent was shown under a strong low-noise condition, but theoretical analysis of this was limited to the square loss function, which is somewhat inadequate for binary classification tasks. In this paper, we show an exponential convergence rate of the expected classification error in the final phase of learning for a wide class of differentiable convex loss functions under similar assumptions. ", "link": "http://arxiv.org/abs/1806.05438", "authors": [{"name": "Atsushi Nitanda", "link": "http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1"}, {"name": "Taiji Suzuki", "link": "http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Scalable load balancing in networked systems: A survey of recent advances. (arXiv:1806.05444v1 [math.PR])", "description": "The basic load balancing scenario involves a single dispatcher where tasks arrive that must immediately be forwarded to one of $N$ single-server queues. We discuss recent advances on scalable load balancing schemes which provide favorable delay performance when $N$ grows large, and yet only require minimal implementation overhead. ", "link": "http://arxiv.org/abs/1806.05444", "authors": [{"name": "Mark van der Boor", "link": "http://arxiv.org/find/math/1/au:+Boor_M/0/1/0/all/0/1"}, {"name": "Sem C. Borst", "link": "http://arxiv.org/find/math/1/au:+Borst_S/0/1/0/all/0/1"}, {"name": "Johan S.H. van Leeuwaarden", "link": "http://arxiv.org/find/math/1/au:+Leeuwaarden_J/0/1/0/all/0/1"}, {"name": "Debankur Mukherjee", "link": "http://arxiv.org/find/math/1/au:+Mukherjee_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Asymptotic maximal order statistic for SIR in $\\kappa-\\mu$ shadowed fading. (arXiv:1806.05450v1 [cs.IT])", "description": "Using tools from extreme value theory (EVT), it is proved that the limiting distribution of the maximum of L independent and identically distributed (i.i.d.) signal-to-interference ratio (SIR) random variables (RVs) is a Frechet distribution, when the user and the interferer signals undergo independent and non-identically distributed (i.n.i.d.) $\\kappa-\\mu$ shadowed fading. This limiting distribution is used to analyze the outage probability for selection combining (SC). Further, the moments of the maximum is shown to converge to the moments of the Frechet RV. This is used in deriving results for the asymptotic rate for SC. Finally, the rate of convergence of the actual maximum distribution to the Frechet distribution is derived and is analyzed for different $\\kappa$ and $\\mu$ parameters. Further, results from stochastic ordering are used to analyze the variations in the limiting distribution with respect to variations in the source fading parameters. A close match is observed between Monte-Carlo simulations and the limiting distributions for outage probability and rate. ", "link": "http://arxiv.org/abs/1806.05450", "authors": [{"name": "Athira Subhash", "link": "http://arxiv.org/find/cs/1/au:+Subhash_A/0/1/0/all/0/1"}, {"name": "Muralikrishnan Srinivasan", "link": "http://arxiv.org/find/cs/1/au:+Srinivasan_M/0/1/0/all/0/1"}, {"name": "Sheetal Kalyani", "link": "http://arxiv.org/find/cs/1/au:+Kalyani_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "The committee machine: Computational to statistical gaps in learning a two-layers neural network. (arXiv:1806.05451v1 [cs.LG])", "description": "Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it, strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap. ", "link": "http://arxiv.org/abs/1806.05451", "authors": [{"name": "Benjamin Aubin", "link": "http://arxiv.org/find/cs/1/au:+Aubin_B/0/1/0/all/0/1"}, {"name": "Antoine Maillard", "link": "http://arxiv.org/find/cs/1/au:+Maillard_A/0/1/0/all/0/1"}, {"name": "Jean Barbier", "link": "http://arxiv.org/find/cs/1/au:+Barbier_J/0/1/0/all/0/1"}, {"name": "Florent Krzakala", "link": "http://arxiv.org/find/cs/1/au:+Krzakala_F/0/1/0/all/0/1"}, {"name": "Nicolas Macris", "link": "http://arxiv.org/find/cs/1/au:+Macris_N/0/1/0/all/0/1"}, {"name": "Lenka Zdeborov&#xe1;", "link": "http://arxiv.org/find/cs/1/au:+Zdeborova_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Deep Generative Models in the Real-World: An Open Challenge from Medical Imaging. (arXiv:1806.05452v1 [cs.CV])", "description": "Recent advances in deep learning led to novel generative modeling techniques that achieve unprecedented quality in generated samples and performance in learning complex distributions in imaging data. These new models in medical image computing have important applications that form clinically relevant and very challenging unsupervised learning problems. In this paper, we explore the feasibility of using state-of-the-art auto-encoder-based deep generative models, such as variational and adversarial auto-encoders, for one such task: abnormality detection in medical imaging. We utilize typical, publicly available datasets with brain scans from healthy subjects and patients with stroke lesions and brain tumors. We use the data from healthy subjects to train different auto-encoder based models to learn the distribution of healthy images and detect pathologies as outliers. Models that can better learn the data distribution should be able to detect outliers more accurately. We evaluate the detection performance of deep generative models and compare them with non-deep learning based approaches to provide a benchmark of the current state of research. We conclude that abnormality detection is a challenging task for deep generative models and large room exists for improvement. In order to facilitate further research, we aim to provide carefully pre-processed imaging data available to the research community. ", "link": "http://arxiv.org/abs/1806.05452", "authors": [{"name": "Xiaoran Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"}, {"name": "Nick Pawlowski", "link": "http://arxiv.org/find/cs/1/au:+Pawlowski_N/0/1/0/all/0/1"}, {"name": "Martin Rajchl", "link": "http://arxiv.org/find/cs/1/au:+Rajchl_M/0/1/0/all/0/1"}, {"name": "Ben Glocker", "link": "http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1"}, {"name": "Ender Konukoglu", "link": "http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Low-rank geometric mean metric learning. (arXiv:1806.05454v1 [cs.LG])", "description": "We propose a low-rank approach to learning a Mahalanobis metric from data. Inspired by the recent geometric mean metric learning (GMML) algorithm, we propose a low-rank variant of the algorithm. This allows to jointly learn a low-dimensional subspace where the data reside and the Mahalanobis metric that appropriately fits the data. Our results show that we compete effectively with GMML at lower ranks. ", "link": "http://arxiv.org/abs/1806.05454", "authors": [{"name": "Mukul Bhutani", "link": "http://arxiv.org/find/cs/1/au:+Bhutani_M/0/1/0/all/0/1"}, {"name": "Pratik Jawanpuria", "link": "http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1"}, {"name": "Hiroyuki Kasai", "link": "http://arxiv.org/find/cs/1/au:+Kasai_H/0/1/0/all/0/1"}, {"name": "Bamdev Mishra", "link": "http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Analysis of the Effect of Unexpected Outliers in the Classification of Spectroscopy Data. (arXiv:1806.05455v1 [cs.CV])", "description": "Multi-class classification algorithms are very widely used, but we argue that they are not always ideal from a theoretical perspective, because they assume all classes are characterized by the data, whereas in many applications, training data for some classes may be entirely absent, rare, or statistically unrepresentative. We evaluate one-sided classifiers as an alternative, since they assume that only one class (the target) is well characterized. We consider a task of identifying whether a substance contains a chlorinated solvent, based on its chemical spectrum. For this application, it is not really feasible to collect a statistically representative set of outliers, since that group may contain \\emph{anything} apart from the target chlorinated solvents. Using a new one-sided classification toolkit, we compare a One-Sided k-NN algorithm with two well-known binary classification algorithms, and conclude that the one-sided classifier is more robust to unexpected outliers. ", "link": "http://arxiv.org/abs/1806.05455", "authors": [{"name": "Frank G. Glavin", "link": "http://arxiv.org/find/cs/1/au:+Glavin_F/0/1/0/all/0/1"}, {"name": "Michael G. Madden", "link": "http://arxiv.org/find/cs/1/au:+Madden_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning Cross-lingual Distributed Logical Representations for Semantic Parsing. (arXiv:1806.05461v1 [cs.CL])", "description": "With the development of several multilingual datasets used for semantic parsing, recent research efforts have looked into the problem of learning semantic parsers in a multilingual setup. However, how to improve the performance of a monolingual semantic parser for a specific language by leveraging data annotated in different languages remains a research question that is under-explored. In this work, we present a study to show how learning distributed representations of the logical forms from data annotated in different languages can be used for improving the performance of a monolingual semantic parser. We extend two existing monolingual semantic parsers to incorporate such cross-lingual distributed logical representations as features. Experiments show that our proposed approach is able to yield improved semantic parsing results on the standard multilingual GeoQuery dataset. ", "link": "http://arxiv.org/abs/1806.05461", "authors": [{"name": "Yanyan Zou", "link": "http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"}, {"name": "Wei Lu", "link": "http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "An Input-Delay Event-Triggered Control Design for Nonlinear Systems. (arXiv:1806.05464v1 [cs.SY])", "description": "The paper proposes a novel event-triggered control scheme for nonlinear systems based on the input-delay method. Specifically, the closed-loop system is associated with a pair of auxiliary input and output. The auxiliary output is defined as the derivative of the continuous-time input function, while the auxiliary input is defined as the input disturbance caused by the sampling or equivalently the integral of the auxiliary output over the sampling period. As a result, a cyclic mapping forms from the input to the output via the system dynamics and back from the output to the input via the integral. The event-triggering law is constructed to make the mapping contractive such that the stabilization is achieved and an easy-to-check Zeno-free condition is provided. With this idea, we develop a theorem for the event-triggered control of interconnected nonlinear systems which is employed to solve the event-triggered control for lower-triangular systems with dynamic uncertainties. ", "link": "http://arxiv.org/abs/1806.05464", "authors": [{"name": "Lijun Zhu", "link": "http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"}, {"name": "Zhiyong Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"}, {"name": "David J. Hill", "link": "http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1"}, {"name": "Shengli Du", "link": "http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Stabilization with a Specified External Gain for Linear MIMO Systems and Its Applications to Control of Networked Systems. (arXiv:1806.05472v1 [cs.SY])", "description": "This paper studies a stabilization problem for linear MIMO systems subject to external perturbation that further requires the closed-loop system render a specified gain from the external perturbation to the output. The problem arises from control of networked systems, in particular, robust output synchronization of heterogeneous linear MIMO multi-agent systems via output feedback/communication. We propose a new approach that converts a class of MIMO systems into a normal form via repeated singular value decomposition and prove that a stabilization controller with a specified external gain can be explicitly constructed for the normal form.Two scenarios with static state feedback and dynamic output feedback are investigated. By integrating the reference model and internal model techniques, the robust output synchronization problem for MIMO multi-agent systems is converted into a stabilization problem with a specified externalgain and solved by the developed approach. ", "link": "http://arxiv.org/abs/1806.05472", "authors": [{"name": "Lijun Zhu", "link": "http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"}, {"name": "Zhiyong Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"}, {"name": "Xi Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"}, {"name": "David J. Hill", "link": "http://arxiv.org/find/cs/1/au:+Hill_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Efficient Active Learning for Image Classification and Segmentation using a Sample Selection and Conditional Generative Adversarial Network. (arXiv:1806.05473v1 [cs.CV])", "description": "Training robust deep learning (DL) systems for medical image classification or segmentation is challenging due to limited images covering different disease types and severity. We propose an active learning (AL) framework to select most informative samples and add to the training data. We use conditional generative adversarial networks (cGANs) to generate realistic chest xray images with different disease characteristics by conditioning its generation on a real image sample. Informative samples to add to the training set are identified using a Bayesian neural network. Experiments show our proposed AL framework is able to achieve state of the art performance by using about 35% of the full dataset, thus saving significant time and effort over conventional methods. ", "link": "http://arxiv.org/abs/1806.05473", "authors": [{"name": "Dwarikanath Mahapatra", "link": "http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1"}, {"name": "Behzad Bozorgtabar", "link": "http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1"}, {"name": "Jean-Philippe Thiran", "link": "http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1"}, {"name": "Mauricio Reyes", "link": "http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data. (arXiv:1806.05476v1 [cs.CV])", "description": "In the past few years, Convolutional Neural Networks (CNNs) have been achieving state-of-the-art performance on a variety of problems. Many companies employ resources and money to generate these models and provide them as an API, therefore it is in their best interest to protect them, i.e., to avoid that someone else copies them. Recent studies revealed that state-of-the-art CNNs are vulnerable to adversarial examples attacks, and this weakness indicates that CNNs do not need to operate in the problem domain (PD). Therefore, we hypothesize that they also do not need to be trained with examples of the PD in order to operate in it. ", "link": "http://arxiv.org/abs/1806.05476", "authors": [{"name": "Jacson Rodrigues Correia-Silva", "link": "http://arxiv.org/find/cs/1/au:+Correia_Silva_J/0/1/0/all/0/1"}, {"name": "Rodrigo F. Berriel", "link": "http://arxiv.org/find/cs/1/au:+Berriel_R/0/1/0/all/0/1"}, {"name": "Claudine Badue", "link": "http://arxiv.org/find/cs/1/au:+Badue_C/0/1/0/all/0/1"}, {"name": "Alberto F. de Souza", "link": "http://arxiv.org/find/cs/1/au:+Souza_A/0/1/0/all/0/1"}, {"name": "Thiago Oliveira-Santos", "link": "http://arxiv.org/find/cs/1/au:+Oliveira_Santos_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Securing Majority-Attack In Blockchain Using Machine Learning And Algorithmic Game Theory: A Proof of Work. (arXiv:1806.05477v1 [cs.CR])", "description": "Recently we could see several institutions coming together to create consortium based blockchain networks such as Hyperledger. Although for applications of blockchain such as Bitcoin, Litcoin, etc. the majority-attack might not be a great threat but for consortium based blockchain networks where we could see several institutions such as public, private, government, etc. are collaborating, the majority-attack might just prove to be a prevalent threat if collusion among these institutions takes place. This paper proposes a methodology where we can use intelligent software agents to monitor the activity of stakeholders in the blockchain networks to detect anomaly such as collusion, using supervised machine learning algorithm and algorithmic game theory and stop the majority-attack from taking place. ", "link": "http://arxiv.org/abs/1806.05477", "authors": [{"name": "Somdip Dey", "link": "http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Automatic Language Identification for Romance Languages using Stop Words and Diacritics. (arXiv:1806.05480v1 [cs.CL])", "description": "Automatic language identification is a natural language processing problem that tries to determine the natural language of a given content. In this paper we present a statistical method for automatic language identification of written text using dictionaries containing stop words and diacritics. We propose different approaches that combine the two dictionaries to accurately determine the language of textual corpora. This method was chosen because stop words and diacritics are very specific to a language, although some languages have some similar words and special characters they are not all common. The languages taken into account were romance languages because they are very similar and usually it is hard to distinguish between them from a computational point of view. We have tested our method using a Twitter corpus and a news article corpus. Both corpora consists of UTF-8 encoded text, so the diacritics could be taken into account, in the case that the text has no diacritics only the stop words are used to determine the language of the text. The experimental results show that the proposed method has an accuracy of over 90% for small texts and over 99.8% for ", "link": "http://arxiv.org/abs/1806.05480", "authors": [{"name": "Ciprian-Octavian Truic&#x103;", "link": "http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1"}, {"name": "Julien Velcin", "link": "http://arxiv.org/find/cs/1/au:+Velcin_J/0/1/0/all/0/1"}, {"name": "Alexandru Boicea", "link": "http://arxiv.org/find/cs/1/au:+Boicea_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Simultaneous Sensor and Actuator Selection/Placement through Output Feedback Control. (arXiv:1806.05481v1 [cs.SY])", "description": "In most dynamic networks, it is impractical to measure all of the system states; instead, only a subset of the states are measured through sensors. Consequently, and unlike full state feedback controllers, output feedback control utilizes only the measured states to obtain a stable closed-loop performance. This paper explores the interplay between the selection of minimal number of sensors and actuators (SaA) that yield a stable closed-loop system performance. Through the formulation of the static output feedback control problem, we show that the simultaneous selection of minimal set of SaA is a combinatorial optimization problem with mixed-integer nonlinear matrix inequality constraints. To address the computational complexity, we develop two approaches: The first approach relies on integer/disjunctive programming principles, while the second approach is a simple algorithm that is akin to binary search routines. The optimality of the two approaches is also discussed. Numerical experiments are included showing the performance of the developed approaches. ", "link": "http://arxiv.org/abs/1806.05481", "authors": [{"name": "Sebastian Nugroho", "link": "http://arxiv.org/find/cs/1/au:+Nugroho_S/0/1/0/all/0/1"}, {"name": "Ahmad F. Taha", "link": "http://arxiv.org/find/cs/1/au:+Taha_A/0/1/0/all/0/1"}, {"name": "Tyler Summers", "link": "http://arxiv.org/find/cs/1/au:+Summers_T/0/1/0/all/0/1"}, {"name": "Nikolaos Gatsis", "link": "http://arxiv.org/find/cs/1/au:+Gatsis_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Morphological and Language-Agnostic Word Segmentation for NMT. (arXiv:1806.05482v1 [cs.CL])", "description": "The state of the art of handling rich morphology in neural machine translation (NMT) is to break word forms into subword units, so that the overall vocabulary size of these units fits the practical limits given by the NMT model and GPU memory capacity. In this paper, we compare two common but linguistically uninformed methods of subword construction (BPE and STE, the method implemented in Tensor2Tensor toolkit) and two linguistically-motivated methods: Morfessor and one novel method, based on a derivational dictionary. Our experiments with German-to-Czech translation, both morphologically rich, document that so far, the non-motivated methods perform better. Furthermore, we iden- tify a critical difference between BPE and STE and show a simple pre- processing step for BPE that considerably increases translation quality as evaluated by automatic measures. ", "link": "http://arxiv.org/abs/1806.05482", "authors": [{"name": "Dominik Mach&#xe1;&#x10d;ek", "link": "http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1"}, {"name": "Jon&#xe1;&#x161; Vidra", "link": "http://arxiv.org/find/cs/1/au:+Vidra_J/0/1/0/all/0/1"}, {"name": "Ond&#x159;ej Bojar", "link": "http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Nearly Zero-Shot Learning for Semantic Decoding in Spoken Dialogue Systems. (arXiv:1806.05484v1 [cs.CL])", "description": "This paper presents two ways of dealing with scarce data in semantic decoding using N-Best speech recognition hypotheses. First, we learn features by using a deep learning architecture in which the weights for the unknown and known categories are jointly optimised. Second, an unsupervised method is used for further tuning the weights. Sharing weights injects prior knowledge to unknown categories. The unsupervised tuning (i.e. the risk minimisation) improves the F-Measure when recognising nearly zero-shot data on the DSTC3 corpus. This unsupervised method can be applied subject to two assumptions: the rank of the class marginal is assumed to be known and the class-conditional scores of the classifier are assumed to follow a Gaussian distribution. ", "link": "http://arxiv.org/abs/1806.05484", "authors": [{"name": "Lina M.Rojas-Barahona", "link": "http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1"}, {"name": "I&#xf1;igo Casanueva", "link": "http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1"}, {"name": "Pawel Budzianowski", "link": "http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1"}, {"name": "Stefan Ultes", "link": "http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1"}, {"name": "Milica Gasic", "link": "http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1"}, {"name": "Bo-Hsiang Tseng", "link": "http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1"}, {"name": "Steve Young", "link": "http://arxiv.org/find/cs/1/au:+Young_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:1806.05490v1 [stat.ML])", "description": "Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Pro- cesses that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to directly sample from it. To efficiently optimize the hyperparameters, we intro- duce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs. ", "link": "http://arxiv.org/abs/1806.05490", "authors": [{"name": "Marton Havasi", "link": "http://arxiv.org/find/stat/1/au:+Havasi_M/0/1/0/all/0/1"}, {"name": "Jos&#xe9; Miguel Hern&#xe1;ndez Lobato", "link": "http://arxiv.org/find/stat/1/au:+Lobato_J/0/1/0/all/0/1"}, {"name": "Juan Jos&#xe9; Murillo Fuentes", "link": "http://arxiv.org/find/stat/1/au:+Fuentes_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Aspect Sentiment Model for Micro Reviews. (arXiv:1806.05499v1 [cs.CL])", "description": "This paper aims at an aspect sentiment model for aspect-based sentiment analysis (ABSA) focused on micro reviews. This task is important in order to understand short reviews majority of the users write, while existing topic models are targeted for expert-level long reviews with sufficient co-occurrence patterns to observe. Current methods on aggregating micro reviews using metadata information may not be effective as well due to metadata absence, topical heterogeneity, and cold start problems. To this end, we propose a model called Micro Aspect Sentiment Model (MicroASM). MicroASM is based on the observation that short reviews 1) are viewed with sentiment-aspect word pairs as building blocks of information, and 2) can be clustered into larger reviews. When compared to the current state-of-the-art aspect sentiment models, experiments show that our model provides better performance on aspect-level tasks such as aspect term extraction and document-level tasks such as sentiment classification. ", "link": "http://arxiv.org/abs/1806.05499", "authors": [{"name": "Reinald Kim Amplayo", "link": "http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1"}, {"name": "Seung-won Hwang", "link": "http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Neural Stethoscopes: Unifying Analytic, Auxiliary and Adversarial Network Probing. (arXiv:1806.05502v1 [stat.ML])", "description": "Model interpretability and systematic, targeted model adaptation present central tenets in machine learning for addressing limited or biased datasets. In this paper, we introduce neural stethoscopes as a framework for quantifying the degree of importance of specific factors of influence in deep networks as well as for actively promoting and suppressing information as appropriate. In doing so we unify concepts from multitask learning as well as training with auxiliary and adversarial losses. We showcase the efficacy of neural stethoscopes in an intuitive physics domain. Specifically, we investigate the challenge of visually predicting stability of block towers and demonstrate that the network uses visual cues which makes it susceptible to biases in the dataset. Through the use of stethoscopes we interrogate the accessibility of specific information throughout the network stack and show that we are able to actively de-bias network predictions as well as enhance performance via suitable auxiliary and adversarial stethoscope losses. ", "link": "http://arxiv.org/abs/1806.05502", "authors": [{"name": "Fabian B. Fuchs", "link": "http://arxiv.org/find/stat/1/au:+Fuchs_F/0/1/0/all/0/1"}, {"name": "Oliver Groth", "link": "http://arxiv.org/find/stat/1/au:+Groth_O/0/1/0/all/0/1"}, {"name": "Adam R. Kosoriek", "link": "http://arxiv.org/find/stat/1/au:+Kosoriek_A/0/1/0/all/0/1"}, {"name": "Alex Bewley", "link": "http://arxiv.org/find/stat/1/au:+Bewley_A/0/1/0/all/0/1"}, {"name": "Markus Wulfmeier", "link": "http://arxiv.org/find/stat/1/au:+Wulfmeier_M/0/1/0/all/0/1"}, {"name": "Andrea Vedaldi", "link": "http://arxiv.org/find/stat/1/au:+Vedaldi_A/0/1/0/all/0/1"}, {"name": "Ingmar Posner", "link": "http://arxiv.org/find/stat/1/au:+Posner_I/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Entity Commonsense Representation for Neural Abstractive Summarization. (arXiv:1806.05504v1 [cs.CL])", "description": "A major proportion of a text summary includes important entities found in the original text. These entities build up the topic of the summary. Moreover, they hold commonsense information once they are linked to a knowledge base. Based on these observations, this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries. To this end, we leverage on an off-the-shelf entity linking system (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily attachable to a sequence-to-sequence model that transforms a list of entities into a vector representation of the topic of the summary. Current available ELS's are still not sufficiently effective, possibly introducing unresolved ambiguities and irrelevant entities. We resolve the imperfections of the ELS by (a) encoding entities with selective disambiguation, and (b) pooling entity vectors using firm attention. By applying E2T to a simple sequence-to-sequence model with attention mechanism as base model, we see significant improvements of the performance in the Gigaword (sentence to title) and CNN (long document to multi-sentence highlights) summarization datasets by at least 2 ROUGE points. ", "link": "http://arxiv.org/abs/1806.05504", "authors": [{"name": "Reinald Kim Amplayo", "link": "http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1"}, {"name": "Seonjae Lim", "link": "http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"}, {"name": "Seung-won Hwang", "link": "http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Dense Light Field Reconstruction From Sparse Sampling Using Residual Network. (arXiv:1806.05506v1 [cs.CV])", "description": "Light field records numerous light rays from real-world scene. However, capturing a dense light field by existing devices is a time-consuming process. Reconstructing a large amount of light rays equivalent to multiple light fields using sparse sampling arises a severe challenge for existing methods. In this paper, we present a learning based approach to reconstruct multiple light fields between two mutually independent light fields. We indicate that light rays distributed in different light fields have some consistent constraints under a certain condition. The most significant constraint is a depth related correlation between angular and spatial dimensions. Our method avoids working out the error-sensitive constraints by employing a deep neural network. We solve residual values of pixels on the epipolar plane image (EPI) to reconstruct novel light fields. Our method is able to reconstruct 4X up-sampling, i.e., extrapolating four novel light fields between two mutually independent light fields. We also compare our results with those yielded by a number of alternatives elsewhere in the literature, which shows our reconstructed light fields have better structure similarity and occlusion relationship. ", "link": "http://arxiv.org/abs/1806.05506", "authors": [{"name": "Mantang Guo", "link": "http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"}, {"name": "Hao Zhu", "link": "http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"}, {"name": "Guoqing Zhou", "link": "http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"}, {"name": "Qing Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Cold-Start Aware User and Product Attention for Sentiment Classification. (arXiv:1806.05507v1 [cs.CL])", "description": "The use of user/product information in sentiment analysis is important, especially for cold-start users/products, whose number of reviews are very limited. However, current models do not deal with the cold-start problem which is typical in review websites. In this paper, we present Hybrid Contextualized Sentiment Classifier (HCSC), which contains two modules: (1) a fast word encoder that returns word vectors embedded with short and long range dependency features; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors. HCSC introduces shared vectors that are constructed from similar users/products, and are used when the original distinct vectors do not have sufficient information (i.e. cold-start). This is decided by a frequency-guided selective gate vector. Our experiments show that in terms of RMSE, HCSC performs significantly better when compared with on famous datasets, despite having less complexity, and thus can be trained much faster. More importantly, our model performs significantly better than previous models when the training data is sparse and has cold-start problems. ", "link": "http://arxiv.org/abs/1806.05507", "authors": [{"name": "Reinald Kim Amplayo", "link": "http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1"}, {"name": "Jihyeok Kim", "link": "http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"}, {"name": "Sua Sung", "link": "http://arxiv.org/find/cs/1/au:+Sung_S/0/1/0/all/0/1"}, {"name": "Seung-won Hwang", "link": "http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "ReConvNet: Video Object Segmentation with Spatio-Temporal Features Modulation. (arXiv:1806.05510v1 [cs.CV])", "description": "We introduce ReConvNet, a recurrent convolutional architecture for semi-supervised video object segmentation that is able to fast adapt its features to focus on the object of interest at inference time. Generalizing to new objects not observed during training is known to be an hard task for supervised approaches that need to be retrained on the new instances. To tackle this problem, we propose a more efficient solution that learns spatio-temporal features that can be adapted by the model itself through affine transformations conditioned on the object in the first frame of the sequence. This approach is simple, it can be trained end-to-end and does not require extra training steps at inference time. Our method shows comparable results on DAVIS2016 with respect to state-of-the art approaches that use online finetuning, and outperform them on DAVIS2017. ReConvNet shows also promising results on the DAVIS-Challenge 2018 placing in $10$-th position. ", "link": "http://arxiv.org/abs/1806.05510", "authors": [{"name": "Francesco Lattari", "link": "http://arxiv.org/find/cs/1/au:+Lattari_F/0/1/0/all/0/1"}, {"name": "Marco Ciccone", "link": "http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1"}, {"name": "Matteo Matteucci", "link": "http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1"}, {"name": "Jonathan Masci", "link": "http://arxiv.org/find/cs/1/au:+Masci_J/0/1/0/all/0/1"}, {"name": "Francesco Visin", "link": "http://arxiv.org/find/cs/1/au:+Visin_F/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "NetScore: Towards Universal Metrics for Large-scale Performance Analysis of Deep Neural Networks for Practical Usage. (arXiv:1806.05512v1 [cs.CV])", "description": "Much of the focus in the design of deep neural networks has been on improving accuracy, leading to more powerful yet highly complex network architectures that are difficult to deploy in practical scenarios, particularly on edge devices such as mobile and other consumer devices, given their high computational and memory requirements. As a result, there has been a recent interest in the design of quantitative metrics for evaluating deep neural networks that accounts for more than just model accuracy as the sole indicator of network performance. In this study, we continue the conversation towards universal metrics for evaluating the performance of deep neural networks for practical usage. In particular, we propose a new balanced metric called NetScore, which is designed specifically to provide a quantitative assessment of the balance between accuracy, computational complexity, and network architecture complexity of a deep neural network. In what is one of the largest comparative analysis between deep neural networks in literature, the NetScore metric, the top-1 accuracy metric, and the popular information density metric were compared across a diverse set of 50 different deep convolutional neural networks for image classification on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2012) dataset. The evaluation results across these three metrics for this diverse set of networks are presented in this study to act as a reference guide for practitioners in the field. The proposed NetScore metric, along with the other tested metrics, are by no means perfect, but the hope is to push the conversation towards better universal metrics for evaluating deep neural networks for use in practical scenarios to help guide practitioners in model design. ", "link": "http://arxiv.org/abs/1806.05512", "authors": [{"name": "Alexander Wong", "link": "http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Humor Detection in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System. (arXiv:1806.05513v1 [cs.CL])", "description": "The tremendous amount of user generated data through social networking sites led to the gaining popularity of automatic text classification in the field of computational linguistics over the past decade. Within this domain, one problem that has drawn the attention of many researchers is automatic humor detection in texts. In depth semantic understanding of the text is required to detect humor which makes the problem difficult to automate. With increase in the number of social media users, many multilingual speakers often interchange between languages while posting on social media which is called code-mixing. It introduces some challenges in the field of linguistic analysis of social media content (Barman et al., 2014), like spelling variations and non-grammatical structures in a sentence. Past researches include detecting puns in texts (Kao et al., 2016) and humor in one-lines (Mihalcea et al., 2010) in a single language, but with the tremendous amount of code-mixed data available online, there is a need to develop techniques which detects humor in code-mixed tweets. In this paper, we analyze the task of humor detection in texts and describe a freely available corpus containing English-Hindi code-mixed tweets annotated with humorous(H) or non-humorous(N) tags. We also tagged the words in the tweets with Language tags (English/Hindi/Others). Moreover, we describe the experiments carried out on the corpus and provide a baseline classification system which distinguishes between humorous and non-humorous texts. ", "link": "http://arxiv.org/abs/1806.05513", "authors": [{"name": "Ankush Khandelwal", "link": "http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1"}, {"name": "Sahil Swami", "link": "http://arxiv.org/find/cs/1/au:+Swami_S/0/1/0/all/0/1"}, {"name": "Syed S. Akhtar", "link": "http://arxiv.org/find/cs/1/au:+Akhtar_S/0/1/0/all/0/1"}, {"name": "Manish Shrivastava", "link": "http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "The Exact Equivalence of Distance and Kernel Methods for Hypothesis Testing. (arXiv:1806.05514v1 [stat.ML])", "description": "Distance-based methods, also called \"energy statistics\", are leading methods for two-sample and independence tests from the statistics community. Kernel methods, developed from \"kernel mean embeddings\", are leading methods for two-sample and independence tests from the machine learning community. Previous works demonstrated the equivalence of distance and kernel methods only at the population level, for each kind of test, requiring an embedding theory of kernels. We propose a simple, bijective transformation between semimetrics and nondegenerate kernels. We prove that for finite samples, two-sample tests are special cases of independence tests, and the distance-based statistic is equivalent to the kernel-based statistic, including the biased, unbiased, and normalized versions. In other words, upon setting the kernel or metric to be bijective of each other, running any of the four algorithms will yield the exact same answer up to numerical precision. This deepens and unifies our understanding of interpoint comparison based methods. ", "link": "http://arxiv.org/abs/1806.05514", "authors": [{"name": "Cencheng Shen", "link": "http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1"}, {"name": "Joshua T. Vogelstein", "link": "http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Translations as Additional Contexts for Sentence Classification. (arXiv:1806.05516v1 [cs.CL])", "description": "In sentence classification tasks, additional contexts, such as the neighboring sentences, may improve the accuracy of the classifier. However, such contexts are domain-dependent and thus cannot be used for another classification task with an inappropriate domain. In contrast, we propose the use of translated sentences as context that is always available regardless of the domain. We find that naive feature expansion of translations gains only marginal improvements and may decrease the performance of the classifier, due to possible inaccurate translations thus producing noisy sentence vectors. To this end, we present multiple context fixing attachment (MCFA), a series of modules attached to multiple sentence vectors to fix the noise in the vectors using the other sentence vectors as context. We show that our method performs competitively compared to previous models, achieving best classification performance on multiple data sets. We are the first to use translations as domain-free contexts for sentence classification. ", "link": "http://arxiv.org/abs/1806.05516", "authors": [{"name": "Reinald Kim Amplayo", "link": "http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1"}, {"name": "Kyungjae Lee", "link": "http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"}, {"name": "Jinyeong Yeo", "link": "http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1"}, {"name": "Seung-won Hwang", "link": "http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Are My EHRs Private Enough? -Event-level Privacy Protection. (arXiv:1806.05520v1 [cs.IR])", "description": "Privacy is a major concern in sharing human subject data to researchers for secondary analyses. A simple binary consent (opt-in or not) may significantly reduce the amount of sharable data, since many patients might only be concerned about a few sensitive medical conditions rather than the entire medical records. We propose event-level privacy protection, and develop a feature ablation method to protect event-level privacy in electronic medical records. Using a list of 13 sensitive diagnoses, we evaluate the feasibility and the efficacy of the proposed method. As feature ablation progresses, the identifiability of a sensitive medical condition decreases with varying speeds on different diseases. We find that these sensitive diagnoses can be divided into 3 categories: (1) 5 diseases have fast declining identifiability (AUC below 0.6 with less than 400 features excluded); (2) 7 diseases with progressively declining identifiability (AUC below 0.7 with between 200 and 700 features excluded); and (3) 1 disease with slowly declining identifiability (AUC above 0.7 with 1000 features excluded). The fact that the majority (12 out of 13) of the sensitive diseases fall into the first two categories suggests the potential of the proposed feature ablation method as a solution for event-level record privacy protection. ", "link": "http://arxiv.org/abs/1806.05520", "authors": [{"name": "Chengsheng Mao", "link": "http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1"}, {"name": "Yuan Zhao", "link": "http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"}, {"name": "Mengxin Sun", "link": "http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"}, {"name": "Yuan Luo", "link": "http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment. (arXiv:1806.05521v1 [cs.CL])", "description": "Because word semantics can substantially change across communities and contexts, capturing domain-specific word semantics is an important challenge. Here, we propose SEMAXIS, a simple yet powerful framework to characterize word semantics using many semantic axes in word- vector spaces beyond sentiment. We demonstrate that SEMAXIS can capture nuanced semantic representations in multiple online communities. We also show that, when the sentiment axis is examined, SEMAXIS outperforms the state-of-the-art approaches in building domain-specific sentiment lexicons. ", "link": "http://arxiv.org/abs/1806.05521", "authors": [{"name": "Jisun An", "link": "http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1"}, {"name": "Haewoon Kwak", "link": "http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1"}, {"name": "Yong-Yeol Ahn", "link": "http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Improved Density-Based Spatio--Textual Clustering on Social Media. (arXiv:1806.05522v1 [cs.SI])", "description": "DBSCAN may not be sufficient when the input data type is heterogeneous in terms of textual description. When we aim to discover clusters of geo-tagged records relevant to a particular point-of-interest (POI) on social media, examining only one type of input data (e.g., the tweets relevant to a POI) may draw an incomplete picture of clusters due to noisy regions. To overcome this problem, we introduce DBSTexC, a newly defined density-based clustering algorithm using spatio--textual information. We first characterize POI-relevant and POI-irrelevant tweets as the texts that include and do not include a POI name or its semantically coherent variations, respectively. By leveraging the proportion of POI-relevant and POI-irrelevant tweets, the proposed algorithm demonstrates much higher clustering performance than the DBSCAN case in terms of $\\mathcal{F}_1$ score and its variants. While DBSTexC performs exactly as DBSCAN with the textually homogeneous inputs, it far outperforms DBSCAN with the textually heterogeneous inputs. Furthermore, to further improve the clustering quality by fully capturing the geographic distribution of tweets, we present fuzzy DBSTexC (F-DBSTexC), an extension of DBSTexC, which incorporates the notion of fuzzy clustering into the DBSTexC. We then demonstrate the robustness of F-DBSTexC via intensive experiments. The computational complexity of our algorithms is also analytically and numerically shown. ", "link": "http://arxiv.org/abs/1806.05522", "authors": [{"name": "Minh D. Nguyen", "link": "http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"}, {"name": "Won-Yong Shin", "link": "http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Bounds and algorithms for $k$-truss. (arXiv:1806.05523v1 [math.CO])", "description": "A $k$-truss is a relaxation of a $k$-clique developed by Cohen (2005), specifically a connected graph in which every edge is incident to at least $k$ triangles. The $k$-truss has proved to be a useful tool in identifying cohesive networks in real-world graphs such as social networks. Despite its simplicity and its utility, the combinatorial and algorithmic aspects of $k$-truss have not been thoroughly explored. ", "link": "http://arxiv.org/abs/1806.05523", "authors": [{"name": "Paul Burkhardt", "link": "http://arxiv.org/find/math/1/au:+Burkhardt_P/0/1/0/all/0/1"}, {"name": "Vance Faber", "link": "http://arxiv.org/find/math/1/au:+Faber_V/0/1/0/all/0/1"}, {"name": "David G. Harris", "link": "http://arxiv.org/find/math/1/au:+Harris_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Fast Decoding of Low Density Lattice Codes. (arXiv:1806.05524v1 [cs.IT])", "description": "Low density lattice codes (LDLC) are a family of lattice codes that can be decoded efficiently using a message-passing algorithm. In the original LDLC decoder, the message exchanged between variable nodes and check nodes are continuous functions, which must be approximated in practice. A promising method is Gaussian approximation (GA), where the messages are approximated by Gaussian functions. However, current GA-based decoders share two weaknesses: firstly, the convergence of these approximate decoders is unproven; secondly, the best known decoder requires $O(2^d)$ operations at each variable node, where $d$ is the degree of LDLC. It means that existing decoders are very slow for long codes with large $d$. The contribution of this paper is twofold: firstly, we prove that all GA-based LDLC decoders converge sublinearly (or faster) in the high signal-to-noise ratio (SNR) region; secondly, we propose a novel GA-based LDLC decoder which requires only $O(d)$ operations at each variable node. Simulation results confirm that the error correcting performance of proposed decoder is the same as the best known decoder, but with a much lower decoding complexity. ", "link": "http://arxiv.org/abs/1806.05524", "authors": [{"name": "Shuiyin Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"}, {"name": "Yi Hong", "link": "http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"}, {"name": "Emanuele Viterbo", "link": "http://arxiv.org/find/cs/1/au:+Viterbo_E/0/1/0/all/0/1"}, {"name": "Alessia Marelli", "link": "http://arxiv.org/find/cs/1/au:+Marelli_A/0/1/0/all/0/1"}, {"name": "Rino Micheloni", "link": "http://arxiv.org/find/cs/1/au:+Micheloni_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection. (arXiv:1806.05525v1 [cs.CV])", "description": "Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are frequently formulated as semantic segmentation. This ill-posed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing. ", "link": "http://arxiv.org/abs/1806.05525", "authors": [{"name": "Mohsen Ghafoorian", "link": "http://arxiv.org/find/cs/1/au:+Ghafoorian_M/0/1/0/all/0/1"}, {"name": "Cedric Nugteren", "link": "http://arxiv.org/find/cs/1/au:+Nugteren_C/0/1/0/all/0/1"}, {"name": "N&#xf3;ra Baka", "link": "http://arxiv.org/find/cs/1/au:+Baka_N/0/1/0/all/0/1"}, {"name": "Olaf Booij", "link": "http://arxiv.org/find/cs/1/au:+Booij_O/0/1/0/all/0/1"}, {"name": "Michael Hofmann", "link": "http://arxiv.org/find/cs/1/au:+Hofmann_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Corner-Sharing Tetrahedra for Modeling Micro-Structure. (arXiv:1806.05528v1 [cs.CG])", "description": "State-of-the-art representations of volumetric multi-scale shape and structure can be classified into three broad categories: continuous, continuous-from-discrete, and discrete representations. We propose modeling micro-structure with a class of discrete Corner-Sharing Tetrahedra (CoSTs). CoSTs can represent bar-joint, tensegrity, line-incidence, and similar constraint systems that capture local physical constraints and global multi-scale properties for design and analysis. The paper develops a palette of simple geometry processing operations on CoSTs including graph manipulation, hierarchical refinement, randomization, and generating associated continuous representations. ", "link": "http://arxiv.org/abs/1806.05528", "authors": [{"name": "Meera Sitharam", "link": "http://arxiv.org/find/cs/1/au:+Sitharam_M/0/1/0/all/0/1"}, {"name": "Jeremy Youngquist", "link": "http://arxiv.org/find/cs/1/au:+Youngquist_J/0/1/0/all/0/1"}, {"name": "Maxwell Nolan", "link": "http://arxiv.org/find/cs/1/au:+Nolan_M/0/1/0/all/0/1"}, {"name": "J&#xf6;rg Peters", "link": "http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Correlation Tracking via Robust Region Proposals. (arXiv:1806.05530v1 [cs.CV])", "description": "Recently, correlation filter-based trackers have received extensive attention due to their simplicity and superior speed. However, such trackers perform poorly when the target undergoes occlusion, viewpoint change or other challenging attributes due to pre-defined sampling strategy. To tackle these issues, in this paper, we propose an adaptive region proposal scheme to facilitate visual tracking. To be more specific, a novel tracking monitoring indicator is advocated to forecast tracking failure. Afterwards, we incorporate detection and scale proposals respectively, to recover from model drift as well as handle aspect ratio variation. We test the proposed algorithm on several challenging sequences, which have demonstrated that the proposed tracker performs favourably against state-of-the-art trackers. ", "link": "http://arxiv.org/abs/1806.05530", "authors": [{"name": "Yuqi Han", "link": "http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"}, {"name": "Jinghong Nan", "link": "http://arxiv.org/find/cs/1/au:+Nan_J/0/1/0/all/0/1"}, {"name": "Zengshuo Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"}, {"name": "Jingjing Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"}, {"name": "Baojun Zhao", "link": "http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Distributed Hypothesis Testing based on Unequal-Error Protection Codes. (arXiv:1806.05533v1 [cs.IT])", "description": "Coding and testing schemes for binary hypothesis testing over noisy networks are proposed and their corresponding type-II error exponents are derived. When communication is over a discrete memoryless channel (DMC), our scheme combines Shimokawa-Han-Amari's hypothesis testing scheme with Borade's unequal error protection (UEP) for channel coding. A separate source channel coding architecture is employed. The resulting exponent is optimal for the newly introduced class of \\emph{generalized testing against conditional independence}. When communication is over a MAC or a BC, our scheme combines hybrid coding with UEP. The resulting error exponent over the MAC is optimal in the case of generalized testing against conditional independence with independent observations at the two sensors, when the MAC decomposes into two individual DMCs. In this case, separate source-channel coding is sufficient; this same conclusion holds also under arbitrarily correlated sensor observations when testing is against independence. For the BC, the error exponents region of hybrid coding with UEP exhibits a tradeoff between the exponents attained at the two decision centers. When both receivers aim at maximizing the error exponents under different hypotheses and the marginal distributions of the sensors' observations are different under these hypotheses, then this tradeoff can be mitigated with the following strategy. The sensor makes a tentative guess on the hypothesis, submits this guess, and applies our coding and testing scheme for the DMC only for the decision center that is not interested in maximizing the exponent under the guessed hypothesis. ", "link": "http://arxiv.org/abs/1806.05533", "authors": [{"name": "Sadaf Salehkalaibar", "link": "http://arxiv.org/find/cs/1/au:+Salehkalaibar_S/0/1/0/all/0/1"}, {"name": "Michele Wigger", "link": "http://arxiv.org/find/cs/1/au:+Wigger_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Status maximization as a source of fairness in a networked dictator game. (arXiv:1806.05542v1 [cs.MA])", "description": "Human behavioural patterns exhibit selfish or competitive, as well as selfless or altruistic tendencies, both of which have demonstrable effects on human social and economic activity. In behavioural economics, such effects have traditionally been illustrated experimentally via simple games like the dictator and ultimatum games. Experiments with these games suggest that, beyond rational economic thinking, human decision-making processes are influenced by social preferences, such as an inclination to fairness. In this study we suggest that the apparent gap between competitive and altruistic human tendencies can be bridged by assuming that people are primarily maximising their status, i.e., a utility function different from simple profit maximisation. To this end we analyse a simple agent-based model, where individuals play the repeated dictator game in a social network they can modify. As model parameters we consider the living costs and the rate at which agents forget infractions by others. We find that individual strategies used in the game vary greatly, from selfish to selfless, and that both of the above parameters determine when individuals form complex and cohesive social networks. ", "link": "http://arxiv.org/abs/1806.05542", "authors": [{"name": "Jan E. Snellman", "link": "http://arxiv.org/find/cs/1/au:+Snellman_J/0/1/0/all/0/1"}, {"name": "Gerardo I&#xf1;iguez", "link": "http://arxiv.org/find/cs/1/au:+Iniguez_G/0/1/0/all/0/1"}, {"name": "J&#xe1;nos Kert&#xe9;sz", "link": "http://arxiv.org/find/cs/1/au:+Kertesz_J/0/1/0/all/0/1"}, {"name": "R. A. Barrio", "link": "http://arxiv.org/find/cs/1/au:+Barrio_R/0/1/0/all/0/1"}, {"name": "Kimmo K. Kaski", "link": "http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Constrained existence problem for weak subgame perfect equilibria with omega-regular Boolean objectives. (arXiv:1806.05544v1 [cs.GT])", "description": "We study multiplayer turn-based games played on a finite directed graph such that each player aims at satisfying an omega-regular Boolean objective. Instead of the well-known notions of Nash equilibrium (NE) and subgame perfect equilibrium (SPE), we focus on the recent notion of weak subgame perfect equilibrium (weak SPE), a refinement of SPE. In this setting, players who deviate can only use the subclass of strategies that differ from the original one on a finite number of histories. We are interested in the constrained existence problem for weak SPEs. We provide a complete characterization of the computational complexity of this problem: it is P-complete for Explicit Muller objectives, NP-complete for Co-B\\\"uchi, Parity, Muller, Rabin, and Streett objectives, and PSPACE-complete for Reachability and Safety objectives (we only prove NP-membership for B\\\"uchi objectives). We also show that the constrained existence problem is fixed parameter tractable and is polynomial when the number of players is fixed. All these results are based on a fine analysis of a fixpoint algorithm that computes the set of possible payoff profiles underlying weak SPEs. ", "link": "http://arxiv.org/abs/1806.05544", "authors": [{"name": "Thomas Brihaye", "link": "http://arxiv.org/find/cs/1/au:+Brihaye_T/0/1/0/all/0/1"}, {"name": "V&#xe9;ronique Bruy&#xe8;re", "link": "http://arxiv.org/find/cs/1/au:+Bruyere_V/0/1/0/all/0/1"}, {"name": "Aline Goeminne", "link": "http://arxiv.org/find/cs/1/au:+Goeminne_A/0/1/0/all/0/1"}, {"name": "Jean-Fran&#xe7;ois Raskin", "link": "http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Adaptive Shooting for Bots in First Person Shooter Games Using Reinforcement Learning. (arXiv:1806.05554v1 [cs.AI])", "description": "In current state-of-the-art commercial first person shooter games, computer controlled bots, also known as non player characters, can often be easily distinguishable from those controlled by humans. Tell-tale signs such as failed navigation, \"sixth sense\" knowledge of human players' whereabouts and deterministic, scripted behaviors are some of the causes of this. We propose, however, that one of the biggest indicators of non humanlike behavior in these games can be found in the weapon shooting capability of the bot. Consistently perfect accuracy and \"locking on\" to opponents in their visual field from any distance are indicative capabilities of bots that are not found in human players. Traditionally, the bot is handicapped in some way with either a timed reaction delay or a random perturbation to its aim, which doesn't adapt or improve its technique over time. We hypothesize that enabling the bot to learn the skill of shooting through trial and error, in the same way a human player learns, will lead to greater variation in game-play and produce less predictable non player characters. This paper describes a reinforcement learning shooting mechanism for adapting shooting over time based on a dynamic reward signal from the amount of damage caused to opponents. ", "link": "http://arxiv.org/abs/1806.05554", "authors": [{"name": "Frank G. Glavin", "link": "http://arxiv.org/find/cs/1/au:+Glavin_F/0/1/0/all/0/1"}, {"name": "Michael G. Madden", "link": "http://arxiv.org/find/cs/1/au:+Madden_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation. (arXiv:1806.05559v1 [cs.CL])", "description": "Parallel sentence extraction is a task addressing the data sparsity problem found in multilingual natural language processing applications. We propose a bidirectional recurrent neural network based approach to extract parallel sentences from collections of multilingual texts. Our experiments with noisy parallel corpora show that we can achieve promising results against a competitive baseline by removing the need of specific feature engineering or additional external resources. To justify the utility of our approach, we extract sentence pairs from Wikipedia articles to train machine translation systems and show significant improvements in translation performance. ", "link": "http://arxiv.org/abs/1806.05559", "authors": [{"name": "Francis Gr&#xe9;goire", "link": "http://arxiv.org/find/cs/1/au:+Gregoire_F/0/1/0/all/0/1"}, {"name": "Philippe Langlais", "link": "http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Cardiac Motion Scoring with Segment- and Subject-level Non-Local Modeling. (arXiv:1806.05569v1 [cs.CV])", "description": "Motion scoring of cardiac myocardium is of paramount importance for early detection and diagnosis of various cardiac disease. It aims at identifying regional wall motions into one of the four types including normal, hypokinetic, akinetic, and dyskinetic, and is extremely challenging due to the complex myocardium deformation and subtle inter-class difference of motion patterns. All existing work on automated motion analysis are focused on binary abnormality detection to avoid the much more demanding motion scoring, which is urgently required in real clinical practice yet has never been investigated before. In this work, we propose Cardiac-MOS, the first powerful method for cardiac motion scoring from cardiac MR sequences based on deep convolution neural network. Due to the locality of convolution, the relationship between distant non-local responses of the feature map cannot be explored, which is closely related to motion difference between segments. In Cardiac-MOS, such non-local relationship is modeled with non-local neural network within each segment and across all segments of one subject, i.e., segment- and subject-level non-local modeling, and lead to obvious performance improvement. Besides, Cardiac-MOS can effectively extract motion information from MR sequences of various lengths by interpolating the convolution kernel along the temporal dimension, therefore can be applied to MR sequences of multiple sources. Experiments on 1440 myocardium segments of 90 subjects from short axis MR sequences of multiple lengths prove that Cardiac-MOS achieves reliable performance, with correlation of 0.926 for motion score index estimation and accuracy of 77.4\\% for motion scoring. Cardiac-MOS also outperforms all existing work for binary abnormality detection. As the first automatic motion scoring solution, Cardiac-MOS demonstrates great potential in future clinical application. ", "link": "http://arxiv.org/abs/1806.05569", "authors": [{"name": "Wufeng Xue", "link": "http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1"}, {"name": "Gary Brahm", "link": "http://arxiv.org/find/cs/1/au:+Brahm_G/0/1/0/all/0/1"}, {"name": "Stephanie Leung", "link": "http://arxiv.org/find/cs/1/au:+Leung_S/0/1/0/all/0/1"}, {"name": "Ogla Shmuilovich", "link": "http://arxiv.org/find/cs/1/au:+Shmuilovich_O/0/1/0/all/0/1"}, {"name": "Shuo Li", "link": "http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Direct Automated Quantitative Measurement of Spine via Cascade Amplifier Regression Network. (arXiv:1806.05570v1 [cs.CV])", "description": "Automated quantitative measurement of the spine (i.e., multiple indices estimation of heights, widths, areas, and so on for the vertebral body and disc) is of the utmost importance in clinical spinal disease diagnoses, such as osteoporosis, intervertebral disc degeneration, and lumbar disc herniation, yet still an unprecedented challenge due to the variety of spine structure and the high dimensionality of indices to be estimated. In this paper, we propose a novel cascade amplifier regression network (CARN), which includes the CARN architecture and local shape-constrained manifold regularization (LSCMR) loss function, to achieve accurate direct automated multiple indices estimation. The CARN architecture is composed of a cascade amplifier network (CAN) for expressive feature embedding and a linear regression model for multiple indices estimation. The CAN consists of cascade amplifier units (AUs), which are used for selective feature reuse by stimulating effective feature and suppressing redundant feature during propagating feature map between adjacent layers, thus an expressive feature embedding is obtained. During training, the LSCMR is utilized to alleviate overfitting and generate realistic estimation by learning the multiple indices distribution. Experiments on MR images of 195 subjects show that the proposed CARN achieves impressive performance with mean absolute errors of 1.2496 mm, 1.2887 mm, and 1.2692 mm for estimation of 15 heights of discs, 15 heights of vertebral bodies, and total indices respectively. The proposed method has great potential in clinical spinal disease diagnoses. ", "link": "http://arxiv.org/abs/1806.05570", "authors": [{"name": "Shumao Pang", "link": "http://arxiv.org/find/cs/1/au:+Pang_S/0/1/0/all/0/1"}, {"name": "Stephanie Leung", "link": "http://arxiv.org/find/cs/1/au:+Leung_S/0/1/0/all/0/1"}, {"name": "Ilanit Ben Nachum", "link": "http://arxiv.org/find/cs/1/au:+Nachum_I/0/1/0/all/0/1"}, {"name": "Qianjin Feng", "link": "http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1"}, {"name": "Shuo Li", "link": "http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Weakly-Supervised Learning for Tool Localization in Laparoscopic Videos. (arXiv:1806.05573v1 [cs.CV])", "description": "Surgical tool localization is an essential task for the automatic analysis of endoscopic videos. In the literature, existing methods for tool localization, tracking and segmentation require training data that is fully annotated, thereby limiting the size of the datasets that can be used and the generalization of the approaches. In this work, we propose to circumvent the lack of annotated data with weak supervision. We propose a deep architecture, trained solely on image level annotations, that can be used for both tool presence detection and localization in surgical videos. Our architecture relies on a fully convolutional neural network, trained end-to-end, enabling us to localize surgical tools without explicit spatial annotations. We demonstrate the benefits of our approach on a large public dataset, Cholec80, which is fully annotated with binary tool presence information and of which 5 videos have been fully annotated with bounding boxes and tool centers for the evaluation. ", "link": "http://arxiv.org/abs/1806.05573", "authors": [{"name": "Armine Vardazaryan", "link": "http://arxiv.org/find/cs/1/au:+Vardazaryan_A/0/1/0/all/0/1"}, {"name": "Didier Mutter", "link": "http://arxiv.org/find/cs/1/au:+Mutter_D/0/1/0/all/0/1"}, {"name": "Jacques Marescaux", "link": "http://arxiv.org/find/cs/1/au:+Marescaux_J/0/1/0/all/0/1"}, {"name": "Nicolas Padoy", "link": "http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Autoregressive Quantile Networks for Generative Modeling. (arXiv:1806.05575v1 [cs.LG])", "description": "We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution. ", "link": "http://arxiv.org/abs/1806.05575", "authors": [{"name": "Georg Ostrovski", "link": "http://arxiv.org/find/cs/1/au:+Ostrovski_G/0/1/0/all/0/1"}, {"name": "Will Dabney", "link": "http://arxiv.org/find/cs/1/au:+Dabney_W/0/1/0/all/0/1"}, {"name": "R&#xe9;mi Munos", "link": "http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Tract orientation mapping for bundle-specific tractography. (arXiv:1806.05580v1 [cs.CV])", "description": "While the major white matter tracts are of great interest to numerous studies in neuroscience and medicine, their manual dissection in larger cohorts from diffusion MRI tractograms is time-consuming, requires expert knowledge and is hard to reproduce. Tract orientation mapping (TOM) is a novel concept that facilitates bundle-specific tractography based on a learned mapping from the original fiber orientation distribution function (fODF) peaks to a list of tract orientation maps (also abbr. TOM). Each TOM represents one of the known tracts with each voxel containing no more than one orientation vector. TOMs can act as a prior or even as direct input for tractography. We use an encoder-decoder fully-convolutional neural network architecture to learn the required mapping. In comparison to previous concepts for the reconstruction of specific bundles, the presented one avoids various cumbersome processing steps like whole brain tractography, atlas registration or clustering. We compare it to four state of the art bundle recognition methods on 20 different bundles in a total of 105 subjects from the Human Connectome Project. Results are anatomically convincing even for difficult tracts, while reaching low angular errors, unprecedented runtimes and top accuracy values (Dice). Our code and our data are openly available. ", "link": "http://arxiv.org/abs/1806.05580", "authors": [{"name": "Jakob Wasserthal", "link": "http://arxiv.org/find/cs/1/au:+Wasserthal_J/0/1/0/all/0/1"}, {"name": "Peter F. Neher", "link": "http://arxiv.org/find/cs/1/au:+Neher_P/0/1/0/all/0/1"}, {"name": "Klaus H. Maier-Hein", "link": "http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Market-Oriented Information Trading in Internet of Things (IoT) for Smart Cities. (arXiv:1806.05583v1 [cs.DC])", "description": "Internet of Things (IoT) technology is a fundamental infrastructure for information transmission and integration in smart city implementations to achieve effective fine-grained city management, efficient operations, and improved life quality of citizens. Smart-city IoT systems usually involve high volume and variety of information circulation. The information lifecycle also involves many parties, stakeholders, and entities such as individuals, businesses, and government agencies with their own objectives which needed to be incentivized properly. As such, recent studies have modeled smart-city IoT systems as a market, where information is treated as a commodity by market participants. In this work, we first present a general information-centric system architecture to analyze smart-city IoT systems. We then discuss features of market-oriented approaches in IoT, including market incentive, IoT service pattern, information freshness, and social impacts. System design chanlenges and related work are also reviewed. Finally, we optimize information trading in smart-city IoT systems, considering direct and indirect network externalities in a social domain. ", "link": "http://arxiv.org/abs/1806.05583", "authors": [{"name": "Yang Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"}, {"name": "Zehui Xiong", "link": "http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1"}, {"name": "Dusit Niyato", "link": "http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1"}, {"name": "Ping Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"}, {"name": "Zhu Han", "link": "http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Inducing information stability and applications thereof to obtaining information theoretic necessary conditions directly from operational requirements. (arXiv:1806.05589v1 [cs.IT])", "description": "This work constructs a discrete random variable that, when conditioned upon, ensures information stability of quasi-images. Using this construction, a new methodology is derived to obtain information theoretic necessary conditions directly from operational requirements. In particular, this methodology is used to derive new necessary conditions for keyed authentication over discrete memoryless channels and to establish the capacity region of the wiretap channel, subject to finite leakage and finite error, under two different secrecy metrics. These examples establish the usefulness of the proposed methodology. ", "link": "http://arxiv.org/abs/1806.05589", "authors": [{"name": "Eric Graves", "link": "http://arxiv.org/find/cs/1/au:+Graves_E/0/1/0/all/0/1"}, {"name": "Tan F. Wong", "link": "http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Improving Consistency-Based Semi-Supervised Learning with Weight Averaging. (arXiv:1806.05594v1 [cs.LG])", "description": "Recent advances in deep unsupervised learning have renewed interest in semi-supervised methods, which can learn from both labeled and unlabeled data. Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. We show that consistency regularization leads to flatter but narrower optima. We also show that the test error surface for these methods is approximately convex in regions of weight space traversed by SGD. Inspired by these observations, we propose to train consistency based semi-supervised models with stochastic weight averaging (SWA), a recent method which averages weights along the trajectory of SGD. We also develop fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With fast-SWA we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100 over many different numbers of observed training labels. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to 6.28% of the previous best result in the literature. We also improve the best known result from 80% accuracy to 83% for domain adaptation from CIFAR-10 to STL. Finally, we show that with fast-SWA the simple $\\Pi$ model becomes state-of-the-art for large labeled settings. ", "link": "http://arxiv.org/abs/1806.05594", "authors": [{"name": "Ben Athiwaratkun", "link": "http://arxiv.org/find/cs/1/au:+Athiwaratkun_B/0/1/0/all/0/1"}, {"name": "Marc Finzi", "link": "http://arxiv.org/find/cs/1/au:+Finzi_M/0/1/0/all/0/1"}, {"name": "Pavel Izmailov", "link": "http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1"}, {"name": "Andrew Gordon Wilson", "link": "http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Static-Loop-Current Attack against the KLJN Secure Key Exchange System. (arXiv:1806.05596v1 [cs.CR])", "description": "A new attack against the Kirchhoff-Law-Johnson-Noise (KLJN) key distribution system is introduced. The attack is based on 1) Utilizing the dc-voltage-source. ", "link": "http://arxiv.org/abs/1806.05596", "authors": [{"name": "Mutaz Y. Melhem", "link": "http://arxiv.org/find/cs/1/au:+Melhem_M/0/1/0/all/0/1"}, {"name": "Laszlo B. Kish", "link": "http://arxiv.org/find/cs/1/au:+Kish_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Survey on Open Information Extraction. (arXiv:1806.05599v1 [cs.CL])", "description": "We provide a detailed overview of the various approaches that were proposed to date to solve the task of Open Information Extraction. We present the major challenges that such systems face, show the evolution of the suggested approaches over time and depict the specific issues they address. In addition, we provide a critique of the commonly applied evaluation procedures for assessing the performance of Open IE systems and highlight some directions for future work. ", "link": "http://arxiv.org/abs/1806.05599", "authors": [{"name": "Christina Niklaus", "link": "http://arxiv.org/find/cs/1/au:+Niklaus_C/0/1/0/all/0/1"}, {"name": "Matthias Cetto", "link": "http://arxiv.org/find/cs/1/au:+Cetto_M/0/1/0/all/0/1"}, {"name": "Andr&#xe9; Freitas", "link": "http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1"}, {"name": "Siegfried Handschuh", "link": "http://arxiv.org/find/cs/1/au:+Handschuh_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Gender Prediction in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System. (arXiv:1806.05600v1 [cs.CL])", "description": "The rapid expansion in the usage of social media networking sites leads to a huge amount of unprocessed user generated data which can be used for text mining. Author profiling is the problem of automatically determining profiling aspects like the author's gender and age group through a text is gaining much popularity in computational linguistics. Most of the past research in author profiling is concentrated on English texts \\cite{1,2}. However many users often change the language while posting on social media which is called code-mixing, and it develops some challenges in the field of text classification and author profiling like variations in spelling, non-grammatical structure and transliteration \\cite{3}. There are very few English-Hindi code-mixed annotated datasets of social media content present online \\cite{4}. In this paper, we analyze the task of author's gender prediction in code-mixed content and present a corpus of English-Hindi texts collected from Twitter which is annotated with author's gender. We also explore language identification of every word in this corpus. We present a supervised classification baseline system which uses various machine learning algorithms to identify the gender of an author using a text, based on character and word level features. ", "link": "http://arxiv.org/abs/1806.05600", "authors": [{"name": "Ankush Khandelwal", "link": "http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1"}, {"name": "Sahil Swami", "link": "http://arxiv.org/find/cs/1/au:+Swami_S/0/1/0/all/0/1"}, {"name": "Syed Sarfaraz Akhtar", "link": "http://arxiv.org/find/cs/1/au:+Akhtar_S/0/1/0/all/0/1"}, {"name": "Manish Shrivastava", "link": "http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Anonymous Information Delivery. (arXiv:1806.05601v1 [cs.IT])", "description": "We introduce the problem of anonymous information delivery (AID), comprised of $K$ messages, a user, and $N$ servers (each holds $M$ messages) that wish to deliver one out of $K$ messages to the user anonymously, i.e., without revealing the delivered message index to the user. This AID problem may be viewed as the dual of the private information retrieval problem. The information theoretic capacity of AID, $C$, is defined as the maximum number of bits of the desired message that can be anonymously delivered per bit of total communication to the user. For the AID problem with $K$ messages, $N$ servers, $M$ messages stored per server, and $N \\geq \\lceil \\frac{K}{M} \\rceil$, we provide an achievable scheme of rate $1/\\lceil \\frac{K}{M} \\rceil$ and an information theoretic converse of rate $M/K$, i.e., the AID capacity satisfies $1/\\lceil \\frac{K}{M} \\rceil \\leq C \\leq M/K$. This settles the capacity of AID when $\\frac{K}{M}$ is an integer. When $\\frac{K}{M}$ is not an integer, we show that the converse rate of $M/K$ is achievable if $N \\geq \\frac{K}{\\gcd(K,M)} - (\\frac{M}{\\gcd(K,M)}-1)(\\lfloor \\frac{K}{M} \\rfloor -1)$, and the achievable rate of $1/\\lceil \\frac{K}{M} \\rceil$ is optimal if $N = \\lceil \\frac{K}{M} \\rceil$. Otherwise if $\\lceil \\frac{K}{M} \\rceil < N < \\frac{K}{\\gcd(K,M)} - (\\frac{M}{\\gcd(K,M)}-1)(\\lfloor \\frac{K}{M} \\rfloor -1)$, we give an improved achievable scheme and prove its optimality for several small settings. ", "link": "http://arxiv.org/abs/1806.05601", "authors": [{"name": "Hua Sun", "link": "http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Detecting Statistically Significant Communities. (arXiv:1806.05602v1 [cs.SI])", "description": "Community detection is a key data analysis problem across different fields. During the past decades, numerous algorithms have been proposed to address this issue. However, most work on community detection does not address the issue of statistical significance. Although some research efforts have been made towards mining statistically significant communities, deriving an analytical solution of p-value for one community under the configuration model is still a challenging mission that remains unsolved. To partially fulfill this void, we present a tight upper bound on the p-value of a single community under the configuration model, which can be used for quantifying the statistical significance of each community analytically. Meanwhile, we present a local search method to detect statistically significant communities in an iterative manner. Experimental results demonstrate that our method is comparable with the competing methods on detecting statistically significant communities. ", "link": "http://arxiv.org/abs/1806.05602", "authors": [{"name": "Zengyou He", "link": "http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"}, {"name": "Hao Liang", "link": "http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"}, {"name": "Zheng Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"}, {"name": "Can Zhao", "link": "http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "From Self-ception to Image Self-ception: A method to represent an image with its own approximations. (arXiv:1806.05610v1 [cs.CV])", "description": "A concept of defining images based on its own approximate ones is proposed here, which is called 'Self-ception'. In this regard, an algorithm is proposed to implement the self-ception for images, which we call it 'Image Self-ception' since we use it for images. We can control the accuracy of this self-ception representation by deciding how many segments or regions we want to use for the representation. Some self-ception images are included in the paper. The video versions of the proposed image self-ception algorithm in action are shown in a YouTube channel (find it by Googling image self-ception). ", "link": "http://arxiv.org/abs/1806.05610", "authors": [{"name": "Hamed Shah-Hosseini", "link": "http://arxiv.org/find/cs/1/au:+Shah_Hosseini_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Stochastic Variance-Reduced Policy Gradient. (arXiv:1806.05618v1 [cs.LG])", "description": "In this paper, we propose a novel reinforcement- learning algorithm consisting in a stochastic variance-reduced version of policy gradient for solving Markov Decision Processes (MDPs). Stochastic variance-reduced gradient (SVRG) methods have proven to be very successful in supervised learning. However, their adaptation to policy gradient is not straightforward and needs to account for I) a non-concave objective func- tion; II) approximations in the full gradient com- putation; and III) a non-stationary sampling pro- cess. The result is SVRPG, a stochastic variance- reduced policy gradient algorithm that leverages on importance weights to preserve the unbiased- ness of the gradient estimate. Under standard as- sumptions on the MDP, we provide convergence guarantees for SVRPG with a convergence rate that is linear under increasing batch sizes. Finally, we suggest practical variants of SVRPG, and we empirically evaluate them on continuous MDPs. ", "link": "http://arxiv.org/abs/1806.05618", "authors": [{"name": "Matteo Papini", "link": "http://arxiv.org/find/cs/1/au:+Papini_M/0/1/0/all/0/1"}, {"name": "Damiano Binaghi", "link": "http://arxiv.org/find/cs/1/au:+Binaghi_D/0/1/0/all/0/1"}, {"name": "Giuseppe Canonaco", "link": "http://arxiv.org/find/cs/1/au:+Canonaco_G/0/1/0/all/0/1"}, {"name": "Matteo Pirotta", "link": "http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"}, {"name": "Marcello Restelli", "link": "http://arxiv.org/find/cs/1/au:+Restelli_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "DynSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes. (arXiv:1806.05620v1 [cs.CV])", "description": "The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this paper we present DynSLAM, a visual SLAM system that, building over ORB-SLAM2 [1], adds the capabilities of dynamic object detection and background inpainting. DynSLAM is robust in dynamic scenarios for monocular, stereo and RGB-D configurations. We are capable of detecting the moving objects either by multi-view geometry, deep learning or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments. ", "link": "http://arxiv.org/abs/1806.05620", "authors": [{"name": "Berta Besc&#xf3;s", "link": "http://arxiv.org/find/cs/1/au:+Bescos_B/0/1/0/all/0/1"}, {"name": "Jos&#xe9; M. F&#xe1;cil", "link": "http://arxiv.org/find/cs/1/au:+Facil_J/0/1/0/all/0/1"}, {"name": "Javier Civera", "link": "http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1"}, {"name": "Jos&#xe9; Neira", "link": "http://arxiv.org/find/cs/1/au:+Neira_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "VoxCeleb2: Deep Speaker Recognition. (arXiv:1806.05622v1 [cs.SD])", "description": "The objective of this paper is speaker recognition under noisy and unconstrained conditions. ", "link": "http://arxiv.org/abs/1806.05622", "authors": [{"name": "Joon Son Chung", "link": "http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1"}, {"name": "Arsha Nagrani", "link": "http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1"}, {"name": "Andrew Zisserman", "link": "http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "NCRF++: An Open-source Neural Sequence Labeling Toolkit. (arXiv:1806.05626v1 [cs.CL])", "description": "This paper describes NCRF++, a toolkit for neural sequence labeling. NCRF++ is designed for quick implementation of different neural sequence labeling models with a CRF inference layer. It provides users with an inference for building the custom model structure through configuration file with flexible neural feature design and utilization. Built on PyTorch, the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTM-CRF, facilitating reproducing and refinement on those methods. ", "link": "http://arxiv.org/abs/1806.05626", "authors": [{"name": "Jie Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"}, {"name": "Yue Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning in POMDPs with Monte Carlo Tree Search. (arXiv:1806.05631v1 [cs.AI])", "description": "The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence. ", "link": "http://arxiv.org/abs/1806.05631", "authors": [{"name": "Sammie Katt", "link": "http://arxiv.org/find/cs/1/au:+Katt_S/0/1/0/all/0/1"}, {"name": "Frans A. Oliehoek", "link": "http://arxiv.org/find/cs/1/au:+Oliehoek_F/0/1/0/all/0/1"}, {"name": "Christopher Amato", "link": "http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Self-Imitation Learning. (arXiv:1806.05635v1 [cs.LG])", "description": "This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks. ", "link": "http://arxiv.org/abs/1806.05635", "authors": [{"name": "Junhyuk Oh", "link": "http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"}, {"name": "Yijie Guo", "link": "http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"}, {"name": "Satinder Singh", "link": "http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"}, {"name": "Honglak Lee", "link": "http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Immunization of networks with non-overlapping community structure. (arXiv:1806.05637v1 [cs.SI])", "description": "Although community structure is ubiquitous in complex networks, few works exploit this topological property to control epidemics. In this work, devoted to networks with non-overlapping community structure (i.e, a node belongs to a single community), we propose and investigate three deterministic immunization strategies. In order to characterize the influence of a node, various pieces of information are used such as the number of communities that the node can reach in one hop, the nature of the links (intra community links, inter community links), the size of the communities, and the interconnection density between communities. Numerical simulations with the Susceptible-Infected-Removed (SIR) epidemiological model are conducted on both real-world and synthetic networks. Experimental results show that the proposed strategies are more effective than classical deterministic alternatives that are agnostic of the community structure. Additionally, they outperform stochastic and deterministic strategies designed for modular networks. ", "link": "http://arxiv.org/abs/1806.05637", "authors": [{"name": "Zakariya Ghalmane", "link": "http://arxiv.org/find/cs/1/au:+Ghalmane_Z/0/1/0/all/0/1"}, {"name": "Mohammed El Hassouni", "link": "http://arxiv.org/find/cs/1/au:+Hassouni_M/0/1/0/all/0/1"}, {"name": "Hocine Cherifi", "link": "http://arxiv.org/find/cs/1/au:+Cherifi_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Grounded Textual Entailment. (arXiv:1806.05645v1 [cs.CL])", "description": "Capturing semantic relations between sentences, such as entailment, is a long-standing challenge for computational semantics. Logic-based models analyse entailment in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant \"world\" or \"situation\"). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare \"blind\" and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing \"grounding\" in an optimal fashion. ", "link": "http://arxiv.org/abs/1806.05645", "authors": [{"name": "Hoa Trong Vu", "link": "http://arxiv.org/find/cs/1/au:+Vu_H/0/1/0/all/0/1"}, {"name": "Claudio Greco", "link": "http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1"}, {"name": "Aliia Erofeeva", "link": "http://arxiv.org/find/cs/1/au:+Erofeeva_A/0/1/0/all/0/1"}, {"name": "Somayeh Jafaritazehjan", "link": "http://arxiv.org/find/cs/1/au:+Jafaritazehjan_S/0/1/0/all/0/1"}, {"name": "Guido Linders", "link": "http://arxiv.org/find/cs/1/au:+Linders_G/0/1/0/all/0/1"}, {"name": "Marc Tanti", "link": "http://arxiv.org/find/cs/1/au:+Tanti_M/0/1/0/all/0/1"}, {"name": "Alberto Testoni", "link": "http://arxiv.org/find/cs/1/au:+Testoni_A/0/1/0/all/0/1"}, {"name": "Raffaella Bernardi", "link": "http://arxiv.org/find/cs/1/au:+Bernardi_R/0/1/0/all/0/1"}, {"name": "Albert Gatt", "link": "http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "HGR-Net: A Two-stage Convolutional Neural Network for Hand Gesture Segmentation and Recognition. (arXiv:1806.05653v1 [cs.CV])", "description": "Robust recognition of hand gesture in real-world applications is still a challenging task due to the many aspects such as cluttered backgrounds and uncontrolled environment factors. In most existing methods hand segmentation is a primary step for hand gesture recognition, because it reduces redundant information from the image background, before passing them to the recognition stages. Therefore, in this paper we propose a two-stage deep convolutional neural network (CNN) architecture called HGR-Net, where the first stage performs accurate pixel-level semantic segmentation into hand region and the second stage identifies hand gesture style. The segmentation stage architecture is based on the combination of fully convolutional deep residual neural network and atrous spatial pyramid pooling. Although the segmentation sub-network is trained without using depth information, it is robust enough against challenging situations such as changes in the lightning and complex backgrounds. In the recognition stage a two-stream CNN is used to obtain the best classification score. We also apply an effective data augmentation technique for maximizing the generalization capability of HGR-Net. Extensive experiments on public hand gesture datasets show that our deep architecture achieves prominent performance in segmentation and recognition for static hand gestures. ", "link": "http://arxiv.org/abs/1806.05653", "authors": [{"name": "Amirhossein Dadashzadeh", "link": "http://arxiv.org/find/cs/1/au:+Dadashzadeh_A/0/1/0/all/0/1"}, {"name": "Alireza Tavakoli Targhi", "link": "http://arxiv.org/find/cs/1/au:+Targhi_A/0/1/0/all/0/1"}, {"name": "Maryam Tahmasbi", "link": "http://arxiv.org/find/cs/1/au:+Tahmasbi_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Efficient and Modular Coalgebraic Partition Refinement. (arXiv:1806.05654v1 [cs.DS])", "description": "We present a generic partition refinement algorithm that quotients coalgebraic systems by behavioural equivalence, a task arising in different contexts. Coalgebraic generality allows us to not only cover classical relational systems and various forms of weighted systems but way can combine existing system types in various ways. Under assumptions on the type functor that allow representing its finite coalgebras in terms of nodes and edges, our algorithm runs in time $\\mathcal{O}(m\\cdot \\log n)$ where $n$ and $m$ are the numbers of nodes and edges, respectively. The generic complexity results and the possibilities of combining system types is a toolbox for efficient partition refinement algorithms. Instances of our generic algorithm thus match the runtime of the best known algorithms for unlabelled transition systems, Markov chains, and deterministic automata (with fixed alphabets), and improve the best known algorithms for Segala systems. ", "link": "http://arxiv.org/abs/1806.05654", "authors": [{"name": "Thorsten Wi&#xdf;mann", "link": "http://arxiv.org/find/cs/1/au:+Wissmann_T/0/1/0/all/0/1"}, {"name": "Ulrich Dorsch", "link": "http://arxiv.org/find/cs/1/au:+Dorsch_U/0/1/0/all/0/1"}, {"name": "Stefan Milius", "link": "http://arxiv.org/find/cs/1/au:+Milius_S/0/1/0/all/0/1"}, {"name": "Lutz Schr&#xf6;der", "link": "http://arxiv.org/find/cs/1/au:+Schroder_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Abstract Meaning Representation for Multi-Document Summarization. (arXiv:1806.05655v1 [cs.CL])", "description": "Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research. ", "link": "http://arxiv.org/abs/1806.05655", "authors": [{"name": "Kexin Liao", "link": "http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1"}, {"name": "Logan Lebanoff", "link": "http://arxiv.org/find/cs/1/au:+Lebanoff_L/0/1/0/all/0/1"}, {"name": "Fei Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Losing at Checkers is Hard. (arXiv:1806.05657v1 [cs.CC])", "description": "We prove computational intractability of variants of checkers: (1) deciding whether there is a move that forces the other player to win in one move is NP-complete; (2) checkers where players must always be able to jump on their turn is PSPACE-complete; and (3) cooperative versions of (1) and (2) are NP-complete. We also give cooperative checkers puzzles whose solutions are the letters of the alphabet. ", "link": "http://arxiv.org/abs/1806.05657", "authors": [{"name": "Jeffrey Bosboom", "link": "http://arxiv.org/find/cs/1/au:+Bosboom_J/0/1/0/all/0/1"}, {"name": "Spencer Congero", "link": "http://arxiv.org/find/cs/1/au:+Congero_S/0/1/0/all/0/1"}, {"name": "Erik D. Demaine", "link": "http://arxiv.org/find/cs/1/au:+Demaine_E/0/1/0/all/0/1"}, {"name": "Martin L. Demaine", "link": "http://arxiv.org/find/cs/1/au:+Demaine_M/0/1/0/all/0/1"}, {"name": "Jayson Lynch", "link": "http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Structure-Infused Copy Mechanisms for Abstractive Summarization. (arXiv:1806.05658v1 [cs.CL])", "description": "Seq2seq learning has produced promising results on summarization. However, in many cases, system summaries still struggle to keep the meaning of the original intact. They may miss out important words or relations that play critical roles in the syntactic structure of source sentences. In this paper, we present structure-infused copy mechanisms to facilitate copying important words and relations from a source sentence to a summary sentence. The approach naturally combines the dependency structure of the source sentence with the copy mechanism of an abstractive sentence summarization system. Experimental results demonstrate the effectiveness of incorporating source-side syntactic information in the system, and our proposed approach compares favorably to state-of-the-art methods. ", "link": "http://arxiv.org/abs/1806.05658", "authors": [{"name": "Kaiqiang Song", "link": "http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1"}, {"name": "Lin Zhao", "link": "http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"}, {"name": "Fei Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Interactive Classification for Deep Learning Interpretation. (arXiv:1806.05660v1 [cs.CV])", "description": "We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms and obtain new classifications in real time, which allows them to ask a variety of \"what if\" questions by experimentally modifying images and seeing how the model reacts. Our system allows users to compare and contrast what image regions humans and machine learning models use for classification, revealing a wide range of surprising results ranging from spectacular failures (e.g., a \"water bottle\" image becomes a \"concert\" when removing a person) to impressive resilience (e.g., a \"baseball player\" image remains correctly classified even without a glove or base). We demonstrate our system at The 2018 Conference on Computer Vision and Pattern Recognition (CVPR) for the audience to try it live. Our system is open-sourced at https://github.com/poloclub/interactive-classification. A video demo is available at https://youtu.be/llub5GcOF6w. ", "link": "http://arxiv.org/abs/1806.05660", "authors": [{"name": "Angel Cabrera", "link": "http://arxiv.org/find/cs/1/au:+Cabrera_A/0/1/0/all/0/1"}, {"name": "Fred Hohman", "link": "http://arxiv.org/find/cs/1/au:+Hohman_F/0/1/0/all/0/1"}, {"name": "Jason Lin", "link": "http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"}, {"name": "Duen Horng Chau", "link": "http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations. (arXiv:1806.05662v1 [cs.LG])", "description": "Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels. ", "link": "http://arxiv.org/abs/1806.05662", "authors": [{"name": "Zhilin Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"}, {"name": "Jake", "link": "http://arxiv.org/find/cs/1/au:+Jake/0/1/0/all/0/1"}, {"name": "Zhao", "link": "http://arxiv.org/find/cs/1/au:+Zhao/0/1/0/all/0/1"}, {"name": "Bhuwan Dhingra", "link": "http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"}, {"name": "Kaiming He", "link": "http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"}, {"name": "William W. Cohen", "link": "http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"}, {"name": "Ruslan Salakhutdinov", "link": "http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"}, {"name": "Yann LeCun", "link": "http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning Human Optical Flow. (arXiv:1806.05666v1 [cs.CV])", "description": "The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research. ", "link": "http://arxiv.org/abs/1806.05666", "authors": [{"name": "Anurag Ranjan", "link": "http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1"}, {"name": "Javier Romero", "link": "http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1"}, {"name": "Michael J. Black", "link": "http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Spoke-Darts for High-Dimensional Blue-Noise Sampling. (arXiv:1408.1118v3 [cs.GR] UPDATED)", "description": "Blue noise sampling has proved useful for many graphics applications, but remains underexplored in high-dimensional spaces due to the difficulty of generating distributions and proving properties about them. We present a blue noise sampling method with good quality and performance across different dimensions. The method, spoke-dart sampling, shoots rays from prior samples and selects samples from these rays. It combines the advantages of two major high-dimensional sampling methods: the locality of advancing front with the dimensionality-reduction of hyperplanes, specifically line sampling. We prove that the output sampling is saturated with high probability, with bounds on distances between pairs of samples and between any domain point and its nearest sample. We demonstrate spoke-dart applications for approximate Delaunay graph construction, global optimization, and robotic motion planning. Both the blue-noise quality of the output distribution and the adaptability of the intermediate processes of our method are useful in these applications. ", "link": "http://arxiv.org/abs/1408.1118", "authors": [{"name": "Scott A. Mitchell", "link": "http://arxiv.org/find/cs/1/au:+Mitchell_S/0/1/0/all/0/1"}, {"name": "Mohamed S. Ebeida", "link": "http://arxiv.org/find/cs/1/au:+Ebeida_M/0/1/0/all/0/1"}, {"name": "Muhammad A. Awad", "link": "http://arxiv.org/find/cs/1/au:+Awad_M/0/1/0/all/0/1"}, {"name": "Chonhyon Park", "link": "http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"}, {"name": "Anjul Patney", "link": "http://arxiv.org/find/cs/1/au:+Patney_A/0/1/0/all/0/1"}, {"name": "Ahmad A. Rushdi", "link": "http://arxiv.org/find/cs/1/au:+Rushdi_A/0/1/0/all/0/1"}, {"name": "Laura P. Swiler", "link": "http://arxiv.org/find/cs/1/au:+Swiler_L/0/1/0/all/0/1"}, {"name": "Dinesh Manocha", "link": "http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"}, {"name": "Li-Yi Wei", "link": "http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Even Delta-Matroids and the Complexity of Planar Boolean CSPs. (arXiv:1602.03124v6 [cs.CC] UPDATED)", "description": "The main result of this paper is a generalization of the classical blossom algorithm for finding perfect matchings. Our algorithm can efficiently solve Boolean CSPs where each variable appears in exactly two constraints (we call it edge CSP) and all constraints are even $\\Delta$-matroid relations (represented by lists of tuples). As a consequence of this, we settle the complexity classification of planar Boolean CSPs started by Dvorak and Kupec. ", "link": "http://arxiv.org/abs/1602.03124", "authors": [{"name": "Alexandr Kazda", "link": "http://arxiv.org/find/cs/1/au:+Kazda_A/0/1/0/all/0/1"}, {"name": "Vladimir Kolmogorov", "link": "http://arxiv.org/find/cs/1/au:+Kolmogorov_V/0/1/0/all/0/1"}, {"name": "Michal Rol&#xed;nek", "link": "http://arxiv.org/find/cs/1/au:+Rolinek_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Edge-Stable Equimatchable Graphs. (arXiv:1602.09127v3 [cs.DM] UPDATED)", "description": "A graph $G$ is \\emph{equimatchable} if every maximal matching of $G$ has the same cardinality. We are interested in equimatchable graphs such that the removal of any edge from the graph preserves the equimatchability. We call an equimatchable graph $G$ \\emph{edge-stable} if $G\\setminus {e}$, that is the graph obtained by the removal of edge $e$ from $G$, is also equimatchable for any $e \\in E(G)$. After noticing that edge-stable equimatchable graphs are either 2-connected factor-critical or bipartite, we characterize edge-stable equimatchable graphs. This characterization yields an $O(\\min(n^{3.376}, n^{1.5}m))$ time recognition algorithm. Lastly, we introduce and shortly discuss the related notions of edge-critical, vertex-stable and vertex-critical equimatchable graphs. In particular, we emphasize the links between our work and the well-studied notion of shedding vertices, and point out some open questions. ", "link": "http://arxiv.org/abs/1602.09127", "authors": [{"name": "Zakir Deniz", "link": "http://arxiv.org/find/cs/1/au:+Deniz_Z/0/1/0/all/0/1"}, {"name": "T&#x131;naz Ekim", "link": "http://arxiv.org/find/cs/1/au:+Ekim_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Algorithmic aspects of branched coverings II/V. Sphere bisets and their decompositions. (arXiv:1603.04059v4 [math.GR] UPDATED)", "description": "We consider \"Thurston maps\": branched self-coverings of the sphere with ultimately periodic critical points, and prove that the Thurston equivalence problem between them (continuous deformation of maps along with their critical orbits) is decidable. ", "link": "http://arxiv.org/abs/1603.04059", "authors": [{"name": "Laurent Bartholdi", "link": "http://arxiv.org/find/math/1/au:+Bartholdi_L/0/1/0/all/0/1"}, {"name": "Dzmitry Dudko", "link": "http://arxiv.org/find/math/1/au:+Dudko_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning a Tree-Structured Ising Model in Order to Make Predictions. (arXiv:1604.06749v3 [math.ST] UPDATED)", "description": "We study the problem of learning a tree Ising model from samples such that subsequent predictions made using the model are accurate. The prediction task considered in this paper is that of predicting the values of a subset of variables given values of some other subset of variables. Virtually all previous work on graphical model learning has focused on recovering the true underlying graph. We define a distance (\"small set TV\" or ssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets $\\mathcal{S}$ of a given size, of the total variation between the marginals of $P$ and $Q$ on $\\mathcal{S}$; this distance captures the accuracy of the prediction task of interest. We derive non-asymptotic bounds on the number of samples needed to get a distribution (from the same class) with small ssTV relative to the one generating the samples. One of the main messages of this paper is that far fewer samples are needed than for recovering the underlying tree, which means that accurate predictions are possible using the wrong tree. ", "link": "http://arxiv.org/abs/1604.06749", "authors": [{"name": "Guy Bresler", "link": "http://arxiv.org/find/math/1/au:+Bresler_G/0/1/0/all/0/1"}, {"name": "Mina Karzand", "link": "http://arxiv.org/find/math/1/au:+Karzand_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Graph parameters, Ramsey theory and the speed of hereditary properties. (arXiv:1608.07727v2 [cs.DM] UPDATED)", "description": "The speed of a hereditary property $P$ is the number $P_n$ of $n$-vertex labelled graphs in $P$. It is known that the rates of growth of $P_n$ constitute discrete layers and the speed jumps, in particular, from constant to polynomial, from polynomial to exponential and from exponential to factorial. One more jump occurs when the entropy $\\lim_{n\\to\\infty}\\frac{\\log_2 P_n}{\\binom{n}{2}}$ changes from 0 to a nonzero value. In the present paper, for each of these jumps we identify a graph parameter responsible for it, i.e. we show that a jump of the speed coincides with a jump of the respective parameter from finitude to infinity. In particular, we show that the speed of a hereditary property $P$ is sub-factorial if and only if the neighbourhood diversity of graphs in $P$ is bounded by a constant, and that the entropy of a hereditary property $P$ is 0 if and only if the VC-dimension of graphs in $P$ is bounded by a constant. All the result are obtained by Ramsey-type arguments. ", "link": "http://arxiv.org/abs/1608.07727", "authors": [{"name": "Vadim Lozin", "link": "http://arxiv.org/find/cs/1/au:+Lozin_V/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Split-door criterion: Identification of causal effects through auxiliary outcomes. (arXiv:1611.09414v2 [stat.ME] UPDATED)", "description": "We present a method for estimating causal effects in time series data when fine-grained information about the outcome of interest is available. Specifically, we examine what we call the split-door setting, where the outcome variable can be split into two parts: one that is potentially affected by the cause being studied and another that is independent of it, with both parts sharing the same (unobserved) confounders. We show that under these conditions, the problem of identification reduces to that of testing for independence among observed variables, and present a method that uses this approach to automatically find subsets of the data that are causally identified. We demonstrate the method by estimating the causal impact of Amazon's recommender system on traffic to product pages, finding thousands of examples within the dataset that satisfy the split-door criterion. Unlike past studies based on natural experiments that were limited to a single product category, our method applies to a large and representative sample of products viewed on the site. In line with previous work, we find that the widely-used click-through rate (CTR) metric overestimates the causal impact of recommender systems; depending on the product category, we estimate that 50-80\\% of the traffic attributed to recommender systems would have happened even without any recommendations. We conclude with guidelines for using the split-door criterion as well as a discussion of other contexts where the method can be applied. ", "link": "http://arxiv.org/abs/1611.09414", "authors": [{"name": "Amit Sharma", "link": "http://arxiv.org/find/stat/1/au:+Sharma_A/0/1/0/all/0/1"}, {"name": "Jake M. Hofman", "link": "http://arxiv.org/find/stat/1/au:+Hofman_J/0/1/0/all/0/1"}, {"name": "Duncan J. Watts", "link": "http://arxiv.org/find/stat/1/au:+Watts_D/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Reducing Guesswork via an Unreliable Oracle. (arXiv:1703.01672v2 [cs.IT] UPDATED)", "description": "Alice holds an random variable $X$, and Bob is trying to guess its value by asking questions of the form \"is $X=x$?\". Alice answers truthfully and the game terminates once Bob guesses correctly. Before the game begins, Bob is allowed to reach out to an oracle, Carole, and ask her any yes/no question, i.e., a question of the form \"is $X\\in A$?\". Carole is known to lie with a given probability $p$. What should Bob ask Carole if he would like to minimize his expected guessing time? When Carole is always truthful ($p=0$), it is not difficult to check that Bob should order the symbol probabilities in descending order, and ask Carole whether the index of $X$ w.r.t this order is even or odd. We show that this strategy is almost optimal for any lying probability $p$, up to a small additive constant upper bounded by a $1/4$. We discuss a connection to the cutoff rate of the BSC with feedback. ", "link": "http://arxiv.org/abs/1703.01672", "authors": [{"name": "Amir Burin", "link": "http://arxiv.org/find/cs/1/au:+Burin_A/0/1/0/all/0/1"}, {"name": "Ofer Shayevitz", "link": "http://arxiv.org/find/cs/1/au:+Shayevitz_O/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "PatternNet: Visual Pattern Mining with Deep Neural Network. (arXiv:1703.06339v2 [cs.CV] UPDATED)", "description": "Visual patterns represent the discernible regularity in the visual world. They capture the essential nature of visual objects or scenes. Understanding and modeling visual patterns is a fundamental problem in visual recognition that has wide ranging applications. In this paper, we study the problem of visual pattern mining and propose a novel deep neural network architecture called PatternNet for discovering these patterns that are both discriminative and representative. The proposed PatternNet leverages the filters in the last convolution layer of a convolutional neural network to find locally consistent visual patches, and by combining these filters we can effectively discover unique visual patterns. In addition, PatternNet can discover visual patterns efficiently without performing expensive image patch sampling, and this advantage provides an order of magnitude speedup compared to most other approaches. We evaluate the proposed PatternNet subjectively by showing randomly selected visual patterns which are discovered by our method and quantitatively by performing image classification with the identified visual patterns and comparing our performance with the current state-of-the-art. We also directly evaluate the quality of the discovered visual patterns by leveraging the identified patterns as proposed objects in an image and compare with other relevant methods. Our proposed network and procedure, PatterNet, is able to outperform competing methods for the tasks described. ", "link": "http://arxiv.org/abs/1703.06339", "authors": [{"name": "Hongzhi Li", "link": "http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"}, {"name": "Joseph G. Ellis", "link": "http://arxiv.org/find/cs/1/au:+Ellis_J/0/1/0/all/0/1"}, {"name": "Lei Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"}, {"name": "Shih-Fu Chang", "link": "http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Power-and Rate-Adaptation Improves the Effective Capacity of C-RAN for Nakagami-$m$ Fading Channels. (arXiv:1704.01924v5 [cs.IT] UPDATED)", "description": "We propose a power-and rate-adaptation scheme for cloud radio access networks (C-RANs), where each radio remote head (RRH) is connected to the baseband unit (BBU) pool through optical links. The RRHs jointly support the users by efficiently exploiting the enhanced spatial degrees of freedom. Our proposed scheme aims for maximizing the effective capacity (EC) of the user subject to both per-RRH average-and peak-power constraints, where the EC is defined as the maximum arrival rate that can be supported by the C-RAN under the statistical delay requirement. We first transform the EC maximization problem into an equivalent convex optimization problem. By using the Lagrange dual decomposition method and solving the Karush-Kuhn-Tucker (KKT) equations, the optimal transmission power of each RRH can be obtained in closed-form. Furthermore, an online tracking method is provided for approximating the average power of each RRH for the sake of updating the Lagrange dual variables. For the special case of two RRHs, the expression of the average power of each RRH can be calculated in explicit form. Hence, the Lagrange dual variables can be computed in advance in this special case. Furthermore, we derive the power allocation for two important extreme cases: 1) no delay constraint; 2) extremely stringent delay-requirements. Our simulation results show that the proposed scheme significantly outperforms the conventional algorithm without considering the delay requirements. Furthermore, when appropriately tuning the value of the delay exponent, our proposed algorithm is capable of guaranteeing a delay outage probability below $10^{-9}$ when the maximum tolerable delay is 1 ms. This is suitable for the future ultra-reliable low latency communications (URLLC). ", "link": "http://arxiv.org/abs/1704.01924", "authors": [{"name": "Hong Ren", "link": "http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"}, {"name": "Nan Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"}, {"name": "Cunhua Pan", "link": "http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"}, {"name": "Maged Elkashlan", "link": "http://arxiv.org/find/cs/1/au:+Elkashlan_M/0/1/0/all/0/1"}, {"name": "Arumugam Nallanathan", "link": "http://arxiv.org/find/cs/1/au:+Nallanathan_A/0/1/0/all/0/1"}, {"name": "Xiaohu You", "link": "http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1"}, {"name": "Lajos Hanzo", "link": "http://arxiv.org/find/cs/1/au:+Hanzo_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Masked Autoregressive Flow for Density Estimation. (arXiv:1705.07057v4 [stat.ML] UPDATED)", "description": "Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks. ", "link": "http://arxiv.org/abs/1705.07057", "authors": [{"name": "George Papamakarios", "link": "http://arxiv.org/find/stat/1/au:+Papamakarios_G/0/1/0/all/0/1"}, {"name": "Theo Pavlakou", "link": "http://arxiv.org/find/stat/1/au:+Pavlakou_T/0/1/0/all/0/1"}, {"name": "Iain Murray", "link": "http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "LAP: a Linearize and Project Method for Solving Inverse Problems with Coupled Variables. (arXiv:1705.09992v3 [math.NA] UPDATED)", "description": "Many inverse problems involve two or more sets of variables that represent different physical quantities but are tightly coupled with each other. For example, image super-resolution requires joint estimation of the image and motion parameters from noisy measurements. Exploiting this structure is key for efficiently solving these large-scale optimization problems, which are often ill-conditioned. ", "link": "http://arxiv.org/abs/1705.09992", "authors": [{"name": "James Herring", "link": "http://arxiv.org/find/math/1/au:+Herring_J/0/1/0/all/0/1"}, {"name": "James Nagy", "link": "http://arxiv.org/find/math/1/au:+Nagy_J/0/1/0/all/0/1"}, {"name": "Lars Ruthotto", "link": "http://arxiv.org/find/math/1/au:+Ruthotto_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Benchmark problems for phase retrieval. (arXiv:1706.00399v3 [cs.IT] UPDATED)", "description": "In recent years, the mathematical and algorithmic aspects of the phase retrieval problem have received considerable attention. Many papers in this area mention crystallography as a principal application. In crystallography, the signal to be recovered is periodic and comprised of atomic distributions arranged homogeneously in the unit cell of the crystal. The crystallographic problem is both the leading application and one of the hardest forms of phase retrieval. We have constructed a graded set of benchmark problems for evaluating algorithms that perform this type of phase retrieval. The data, publicly available online, is provided in an easily interpretable format. We also propose a simple and unambiguous success/failure criterion based on the actual needs in crystallography. Baseline runtimes were obtained with an iterative algorithm that is similar but more transparent than those used in crystallography. Empirically, the runtimes grow exponentially with respect to a new hardness parameter: the sparsity of the signal autocorrelation. We also review the algorithms used by the leading software packages. This set of benchmark problems, we hope, will encourage the development of new algorithms for the phase retrieval problem in general, and crystallography in particular. ", "link": "http://arxiv.org/abs/1706.00399", "authors": [{"name": "Veit Elser", "link": "http://arxiv.org/find/cs/1/au:+Elser_V/0/1/0/all/0/1"}, {"name": "Ti-Yen Lan", "link": "http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1"}, {"name": "Tamir Bendory", "link": "http://arxiv.org/find/cs/1/au:+Bendory_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning Deep ResNet Blocks Sequentially using Boosting Theory. (arXiv:1706.04964v4 [cs.LG] UPDATED)", "description": "Deep neural networks are known to be difficult to train due to the instability of back-propagation. A deep \\emph{residual network} (ResNet) with identity loops remedies this by stabilizing gradient computations. We prove a boosting theory for the ResNet architecture. We construct $T$ weak module classifiers, each contains two of the $T$ layers, such that the combined strong learner is a ResNet. Therefore, we introduce an alternative Deep ResNet training algorithm, \\emph{BoostResNet}, which is particularly suitable in non-differentiable architectures. Our proposed algorithm merely requires a sequential training of $T$ \"shallow ResNets\" which are inexpensive. We prove that the training error decays exponentially with the depth $T$ if the \\emph{weak module classifiers} that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. Our results apply to general multi-class ResNets. A generalization error bound based on margin theory is proved and suggests ResNet's resistant to overfitting under network with $l_1$ norm bounded weights. ", "link": "http://arxiv.org/abs/1706.04964", "authors": [{"name": "Furong Huang", "link": "http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"}, {"name": "Jordan Ash", "link": "http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1"}, {"name": "John Langford", "link": "http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1"}, {"name": "Robert Schapire", "link": "http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Multi-Level Power-Imbalance Allocation Control for Secondary Frequency Control of Power Systems. (arXiv:1708.03832v4 [cs.SY] UPDATED)", "description": "A consensus-control-based multi-level control law named Multi-Level Power-Imbalance Allocation Control (MLPIAC) is presented for a large-scale power system partitioned into two or more areas. Centralized control is implemented in each area while distributed control is implemented at the coordination level of the areas. Besides restoring nominal frequency with a minimal control cost, MLPIAC can improve the transient performance of the system through an accelerated convergence of the control inputs without oscillations. At the coordination level of the control areas, because the number of the areas is smaller than that of nodes, MLPIAC is more effective to obtain the minimized control cost than the purely distributed control law. At the level of the control in each area, because the number of nodes is much smaller than the total number of nodes in the whole network, the overheads in the communications and the computations are reduced compared to the pure centralized control. The asymptotic stability of MLPIAC is proven using the Lyapunov method and the performance is evaluated through simulations. ", "link": "http://arxiv.org/abs/1708.03832", "authors": [{"name": "Kaihua Xi", "link": "http://arxiv.org/find/cs/1/au:+Xi_K/0/1/0/all/0/1"}, {"name": "Hai Xiang Lin", "link": "http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"}, {"name": "Chen Shen", "link": "http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"}, {"name": "Jan H. van Schuppen", "link": "http://arxiv.org/find/cs/1/au:+Schuppen_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Framework for Validating Models of Evasion Attacks on Machine Learning, with Application to PDF Malware Detection. (arXiv:1708.08327v3 [cs.CR] UPDATED)", "description": "Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. However, there is increasing evidence that machine learning models are susceptible to evasion attacks, in which an adversary makes small changes to the input (such as malware) in order to cause erroneous predictions (for example, to avoid being detected). Evasion attacks on ML fall into two broad categories: 1) those which generate actual malicious instances and demonstrate both evasion of ML and efficacy of attack (we call these problem space attacks), and 2) attacks which directly manipulate features used by ML, abstracting efficacy of attack into a mathematical cost function (we call these feature space attacks). Central to our inquiry is the following fundamental question: are feature space models of attacks useful proxies for real attacks? In the process of answering this question, we make two major contributions: 1) a general methodology for evaluating validity of mathematical models of ML evasion attacks, and 2) an application of this methodology as a systematic hypothesis-driven evaluation of feature space evasion attacks on ML-based PDF malware detectors. Specific to our case study, we find that a) feature space evasion models are in general not adequate in representing real attacks, b) such models can be significantly improved by identifying conserved features (features that are invariant in real attacks) whenever these exist, and c) ML hardened using the improved feature space models remains robust to alternative attacks, in contrast to ML hardened using a very powerful class of problem space attacks, which does not. ", "link": "http://arxiv.org/abs/1708.08327", "authors": [{"name": "Liang Tong", "link": "http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1"}, {"name": "Bo Li", "link": "http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"}, {"name": "Chen Hajaj", "link": "http://arxiv.org/find/cs/1/au:+Hajaj_C/0/1/0/all/0/1"}, {"name": "Chaowei Xiao", "link": "http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"}, {"name": "Yevgeniy Vorobeychik", "link": "http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Robust Model Predictive Control Approach for Autonomous Underwater Vehicles Operating in a Constrained workspace. (arXiv:1709.04940v2 [cs.RO] UPDATED)", "description": "This paper presents a novel Nonlinear Model Predictive Control (NMPC) scheme for underwater robotic vehicles operating in a constrained workspace including static obstacles. The purpose of the controller is to guide the vehicle towards specific way points. Various limitations such as: obstacles, workspace boundary, thruster saturation and predefined desired upper bound of the vehicle velocity are captured as state and input constraints and are guaranteed during the control design. The proposed scheme incorporates the full dynamics of the vehicle in which the ocean currents are also involved. Hence, the control inputs calculated by the proposed scheme are formulated in a way that the vehicle will exploit the ocean currents, when these are in favor of the way-point tracking mission which results in reduced energy consumption by the thrusters. The performance of the proposed control strategy is experimentally verified using a $4$ Degrees of Freedom (DoF) underwater robotic vehicle inside a constrained test tank with obstacles. ", "link": "http://arxiv.org/abs/1709.04940", "authors": [{"name": "Shahab Heshmati-alamdari", "link": "http://arxiv.org/find/cs/1/au:+Heshmati_alamdari_S/0/1/0/all/0/1"}, {"name": "George C. Karras", "link": "http://arxiv.org/find/cs/1/au:+Karras_G/0/1/0/all/0/1"}, {"name": "Panos Marantos", "link": "http://arxiv.org/find/cs/1/au:+Marantos_P/0/1/0/all/0/1"}, {"name": "Kostas J. Kyriakopoulos", "link": "http://arxiv.org/find/cs/1/au:+Kyriakopoulos_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Investigation of Frame Alignments for GMM-based Text-prompted Speaker Verification. (arXiv:1710.10436v2 [cs.SD] UPDATED)", "description": "Frame alignments can be computed by different methods in GMM-based speaker verification. By incorporating a phonetic Gaussian mixture model (PGMM), we are able to compare the performance using alignments extracted from the deep neural networks (DNN) and the conventional hidden Markov model (HMM) in digit-prompted speaker verification. The different characteristics of these two alignments can be utilized to verify the speech content thus further improve the security of the system. Our experiments on the RSR2015 Part-3 digit-prompted task show that, the DNN-based alignment performs on par with the HMM alignment. Results also demonstrate that it is effective to use the proposed Kullback-Leibler (KL) divergence based algorithm to reject speech with incorrect content. ", "link": "http://arxiv.org/abs/1710.10436", "authors": [{"name": "Yi Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"}, {"name": "Liang He", "link": "http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"}, {"name": "Jia Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"}, {"name": "Michael T. Johnson", "link": "http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Sparse Randomized Kaczmarz for Support Recovery of Jointly Sparse Corrupted Multiple Measurement Vectors. (arXiv:1711.02743v3 [eess.SP] UPDATED)", "description": "While single measurement vector (SMV) models have been widely studied in signal processing, there is a surging interest in addressing the multiple measurement vectors (MMV) problem. In the MMV setting, more than one measurement vector is available and the multiple signals to be recovered share some commonalities such as a common support. Applications in which MMV is a naturally occurring phenomenon include online streaming, medical imaging, and video recovery. This work presents a stochastic iterative algorithm for the support recovery of jointly sparse corrupted MMV. We present a variant of the Sparse Randomized Kaczmarz algorithm for corrupted MMV and compare our proposed method with an existing Kaczmarz type algorithm for MMV problems. We also showcase the usefulness of our approach in the online (streaming) setting and provide empirical evidence that suggests the robustness of the proposed method to the distribution of the corruption and the number of corruptions occurring. ", "link": "http://arxiv.org/abs/1711.02743", "authors": [{"name": "Natalie Durgin", "link": "http://arxiv.org/find/eess/1/au:+Durgin_N/0/1/0/all/0/1"}, {"name": "Rachel Grotheer", "link": "http://arxiv.org/find/eess/1/au:+Grotheer_R/0/1/0/all/0/1"}, {"name": "Chenxi Huang", "link": "http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1"}, {"name": "Shuang Li", "link": "http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"}, {"name": "Anna Ma", "link": "http://arxiv.org/find/eess/1/au:+Ma_A/0/1/0/all/0/1"}, {"name": "Deanna Needell", "link": "http://arxiv.org/find/eess/1/au:+Needell_D/0/1/0/all/0/1"}, {"name": "Jing Qin", "link": "http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Constraint Coupled Distributed Optimization: a Relaxation and Duality Approach. (arXiv:1711.09221v2 [cs.SY] UPDATED)", "description": "In this paper we consider a distributed optimization scenario in which agents of a network want to minimize the sum of local convex cost functions, each one depending on a local variable, subject to convex local and coupling constraints, with the latter involving all the decision variables. We propose a novel distributed algorithm based on a relaxation of the primal problem and an elegant exploration of duality theory. Despite its complex derivation, based on several duality steps, the distributed algorithm has a very simple and intuitive structure. That is, each node finds a primal-dual optimal solution pair of a local, relaxed version of the original problem, and then linearly updates other dual variables. We prove that agents' estimates asymptotically converge to an optimal solution of the given problem, namely to a point satisfying both local and coupling constraints and having optimal cost. This primal recovery property is obtained without any averaging mechanism typically used in dual methods. To corroborate the theoretical results, we show how the methodology applies to an instance of a Distributed Model Predictive Control scheme in a microgrid control scenario. ", "link": "http://arxiv.org/abs/1711.09221", "authors": [{"name": "Ivano Notarnicola", "link": "http://arxiv.org/find/cs/1/au:+Notarnicola_I/0/1/0/all/0/1"}, {"name": "Giuseppe Notarstefano", "link": "http://arxiv.org/find/cs/1/au:+Notarstefano_G/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Joint Blind Motion Deblurring and Depth Estimation of Light Field. (arXiv:1711.10918v2 [cs.CV] UPDATED)", "description": "Removing camera motion blur from a single light field is a challenging task since it is highly ill-posed inverse problem. The problem becomes even worse when blur kernel varies spatially due to scene depth variation and high-order camera motion. In this paper, we propose a novel algorithm to estimate all blur model variables jointly, including latent sub-aperture image, camera motion, and scene depth from the blurred 4D light field. Exploiting multi-view nature of a light field relieves the inverse property of the optimization by utilizing strong depth cues and multi-view blur observation. The proposed joint estimation achieves high quality light field deblurring and depth estimation simultaneously under arbitrary 6-DOF camera motion and unconstrained scene depth. Intensive experiment on real and synthetic blurred light field confirms that the proposed algorithm outperforms the state-of-the-art light field deblurring and depth estimation methods. ", "link": "http://arxiv.org/abs/1711.10918", "authors": [{"name": "Dongwoo Lee", "link": "http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"}, {"name": "Haesol Park", "link": "http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"}, {"name": "In Kyu Park", "link": "http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1"}, {"name": "Kyoung Mu Lee", "link": "http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches. (arXiv:1711.11221v3 [cs.CL] UPDATED)", "description": "Sentences in a well-formed text are connected to each other via various links to form the cohesive structure of the text. Current neural machine translation (NMT) systems translate a text in a conventional sentence-by-sentence fashion, ignoring such cross-sentence links and dependencies. This may lead to generate an incoherent target text for a coherent source text. In order to handle this issue, we propose a cache-based approach to modeling coherence for neural machine translation by capturing contextual information either from recently translated sentences or the entire document. Particularly, we explore two types of caches: a dynamic cache, which stores words from the best translation hypotheses of preceding sentences, and a topic cache, which maintains a set of target-side topical words that are semantically related to the document to be translated. On this basis, we build a new layer to score target words in these two caches with a cache-based neural model. Here the estimated probabilities from the cache-based neural model are combined with NMT probabilities into the final word prediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines. ", "link": "http://arxiv.org/abs/1711.11221", "authors": [{"name": "Shaohui Kuang", "link": "http://arxiv.org/find/cs/1/au:+Kuang_S/0/1/0/all/0/1"}, {"name": "Deyi Xiong", "link": "http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1"}, {"name": "Weihua Luo", "link": "http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"}, {"name": "Guodong Zhou", "link": "http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Element Distinctness Revisited. (arXiv:1711.11336v3 [quant-ph] UPDATED)", "description": "The element distinctness problem is the problem of determining whether the elements of a list are distinct, that is, if $x=(x_1,...,x_N)$ is a list with $N$ elements, we ask whether the elements of $x$ are distinct or not. The solution in a classical computer requires $N$ queries because it uses sorting to check whether there are equal elements. In the quantum case, it is possible to solve the problem in $O(N^{2/3})$ queries. There is an extension which asks whether there are $k$ colliding elements, known as element $k$-distinctness problem. ", "link": "http://arxiv.org/abs/1711.11336", "authors": [{"name": "Renato Portugal", "link": "http://arxiv.org/find/quant-ph/1/au:+Portugal_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Relation Networks for Object Detection. (arXiv:1711.11575v2 [cs.CV] UPDATED)", "description": "Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances individually, without exploiting their relations during learning. ", "link": "http://arxiv.org/abs/1711.11575", "authors": [{"name": "Han Hu", "link": "http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"}, {"name": "Jiayuan Gu", "link": "http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"}, {"name": "Zheng Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"}, {"name": "Jifeng Dai", "link": "http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"}, {"name": "Yichen Wei", "link": "http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Fruit recognition from images using deep learning. (arXiv:1712.00580v3 [cs.CV] UPDATED)", "description": "In this paper we introduce a new, high-quality, dataset of images containing fruits. We also present the results of some numerical experiment for training a neural network to detect fruits. We discuss the reason why we chose to use fruits in this project by proposing a few applications that could use this kind of neural network. ", "link": "http://arxiv.org/abs/1712.00580", "authors": [{"name": "Horea Mure&#x15f;an", "link": "http://arxiv.org/find/cs/1/au:+Muresan_H/0/1/0/all/0/1"}, {"name": "Mihai Oltean", "link": "http://arxiv.org/find/cs/1/au:+Oltean_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "4DFAB: A Large Scale 4D Facial Expression Database for Biometric Applications. (arXiv:1712.01443v2 [cs.CV] UPDATED)", "description": "The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes. ", "link": "http://arxiv.org/abs/1712.01443", "authors": [{"name": "Shiyang Cheng", "link": "http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"}, {"name": "Irene Kotsia", "link": "http://arxiv.org/find/cs/1/au:+Kotsia_I/0/1/0/all/0/1"}, {"name": "Maja Pantic", "link": "http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1"}, {"name": "Stefanos Zafeiriou", "link": "http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning Spontaneity to Improve Emotion Recognition In Speech. (arXiv:1712.04753v3 [eess.AS] UPDATED)", "description": "We investigate the effect and usefulness of spontaneity (i.e. whether a given speech is spontaneous or not) in speech in the context of emotion recognition. We hypothesize that emotional content in speech is interrelated with its spontaneity, and use spontaneity classification as an auxiliary task to the problem of emotion recognition. We propose two supervised learning settings that utilize spontaneity to improve speech emotion recognition: a hierarchical model that performs spontaneity detection before performing emotion recognition, and a multitask learning model that jointly learns to recognize both spontaneity and emotion. Through various experiments on the well known IEMOCAP database, we show that by using spontaneity detection as an additional task, significant improvement can be achieved over emotion recognition systems that are unaware of spontaneity. We achieve state-of-the-art emotion recognition accuracy (4-class, 69.1%) on the IEMOCAP database outperforming several relevant and competitive baselines. ", "link": "http://arxiv.org/abs/1712.04753", "authors": [{"name": "Karttikeya Mangalam", "link": "http://arxiv.org/find/eess/1/au:+Mangalam_K/0/1/0/all/0/1"}, {"name": "Tanaya Guha", "link": "http://arxiv.org/find/eess/1/au:+Guha_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Transaction Propagation on Permissionless Blockchains: Incentive and Routing Mechanisms. (arXiv:1712.07564v2 [cs.CR] UPDATED)", "description": "Existing permissionless blockchain solutions rely on peer-to-peer propagation mechanisms, where nodes in a network transfer transaction they received to their neighbors. Unfortunately, there is no explicit incentive for such transaction propagation. Therefore, existing propagation mechanisms will not be sustainable in a fully decentralized blockchain with rational nodes. In this work, we formally define the problem of incentivizing nodes for transaction propagation. We propose an incentive mechanism where each node involved in the propagation of a transaction receives a share of the transaction fee. We also show that our proposal is Sybil-proof. Furthermore, we combine the incentive mechanism with smart routing to reduce the communication and storage costs at the same time. The proposed routing mechanism reduces the redundant transaction propagation from the size of the network to a factor of average shortest path length. The routing mechanism is built upon a specific type of consensus protocol where the round leader who creates the transaction block is known in advance. Note that our routing mechanism is a generic one and can be adopted independently from the incentive mechanism. ", "link": "http://arxiv.org/abs/1712.07564", "authors": [{"name": "Oguzhan Ersoy", "link": "http://arxiv.org/find/cs/1/au:+Ersoy_O/0/1/0/all/0/1"}, {"name": "Zhijie Ren", "link": "http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"}, {"name": "Zekeriya Erkin", "link": "http://arxiv.org/find/cs/1/au:+Erkin_Z/0/1/0/all/0/1"}, {"name": "Reginald L. Lagendijk", "link": "http://arxiv.org/find/cs/1/au:+Lagendijk_R/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Primal-Dual Method for Optimal Control and Trajectory Generation in High-Dimensional Systems. (arXiv:1712.08226v3 [cs.SY] UPDATED)", "description": "Presented is a method for efficient computation of the Hamilton-Jacobi (HJ) equation for time-optimal control problems using the generalized Hopf formula. Typically, numerical methods to solve the HJ equation rely on a discrete grid of the solution space and exhibit exponential scaling with dimension. The generalized Hopf formula avoids the use of grids and numerical gradients by formulating an unconstrained convex optimization problem. The solution at each point is completely independent, and allows a massively parallel implementation if solutions at multiple points are desired. This work presents a primal-dual method for efficient numeric solution and presents how the resulting optimal trajectory can be generated directly from the solution of the Hopf formula, without further optimization. Examples presented have execution times on the order of milliseconds and experiments show computation scales approximately polynomial in dimension with very small high-order coefficients. ", "link": "http://arxiv.org/abs/1712.08226", "authors": [{"name": "Matthew R. Kirchner", "link": "http://arxiv.org/find/cs/1/au:+Kirchner_M/0/1/0/all/0/1"}, {"name": "Gary Hewer", "link": "http://arxiv.org/find/cs/1/au:+Hewer_G/0/1/0/all/0/1"}, {"name": "Jerome Darbon", "link": "http://arxiv.org/find/cs/1/au:+Darbon_J/0/1/0/all/0/1"}, {"name": "Stanley Osher", "link": "http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "An Innovative Approach to Achieve Compositionality Efficiently using Multi-Version Object Based Transactional Systems. (arXiv:1712.09803v5 [cs.DC] UPDATED)", "description": "In the modern era of multicore processors, utilizing cores is a tedious job. Synchronization and communication among processors involve high cost. Software transaction memory systems (STMs) addresses this issues and provide better concurrency in which programmer need not have to worry about consistency issues. Another advantage of STMs is that they facilitate compositionality of concurrent programs with great ease. Different concurrent operations that need to be composed to form a single atomic unit is achieved by encapsulating them in a single transaction. In this paper, we introduce a new STM system as multi-version object based STM (MVOSTM) which is the combination of both of these ideas for harnessing greater concurrency in STMs. As the name suggests MVOSTM, works on a higher level and maintains multiple versions corresponding to each key. We have developed MVOSTM with the unlimited number of versions corresponding to each key. In addition to that, we have developed garbage collection for MVOSTM (MVOSTM-GC) to delete unwanted versions corresponding to the keys to reduce traversal overhead. MVOSTM provides greater concurrency while reducing the number of aborts and it ensures compositionality by making the transactions atomic. Here, we have used MVOSTM for the list and hash-table data structure as list-MVOSTM and HT- MVOSTM. Experimental results of list-MVOSTM outperform almost two to twenty fold speedup than existing state-of-the-art list based STMs (Trans-list, Boosting-list, NOrec-list, list-MVTO, and list-OSTM). HT-MVOSTM shows a significant performance gain of almost two to nineteen times better than existing state-of-the-art hash-table based STMs (ESTM, RWSTMs, HT-MVTO, and HT-OSTM). MVOSTM with list and hash-table shows the least number of aborts among all the existing STM algorithms. MVOSTM satisfies correctness-criteria as opacity. ", "link": "http://arxiv.org/abs/1712.09803", "authors": [{"name": "Chirag Juyal", "link": "http://arxiv.org/find/cs/1/au:+Juyal_C/0/1/0/all/0/1"}, {"name": "Sandeep Kulkarni", "link": "http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1"}, {"name": "Sweta Kumari", "link": "http://arxiv.org/find/cs/1/au:+Kumari_S/0/1/0/all/0/1"}, {"name": "Sathya Peri", "link": "http://arxiv.org/find/cs/1/au:+Peri_S/0/1/0/all/0/1"}, {"name": "Archit Somani", "link": "http://arxiv.org/find/cs/1/au:+Somani_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Instruction-Level Abstraction (ILA): A Uniform Specification for System-on-Chip (SoC) Verification. (arXiv:1801.01114v2 [cs.AR] UPDATED)", "description": "Modern Systems-on-Chip (SoC) designs are increasingly heterogeneous and contain specialized semi-programmable accelerators in addition to programmable processors. In contrast to the pre-accelerator era, when the ISA played an important role in verification by enabling a clean separation of concerns between software and hardware, verification of these \"accelerator-rich\" SoCs presents new challenges. From the perspective of hardware designers, there is a lack of a common framework for the formal functional specification of accelerator behavior. From the perspective of software developers, there exists no unified framework for reasoning about software/hardware interactions of programs that interact with accelerators. This paper addresses these challenges by providing a formal specification and high-level abstraction for accelerator functional behavior. It formalizes the concept of an Instruction Level Abstraction (ILA), developed informally in our previous work, and shows its application in modeling and verification of accelerators. This formal ILA extends the familiar notion of instructions to accelerators and provides a uniform, modular, and hierarchical abstraction for modeling software-visible behavior of both accelerators and programmable processors. We demonstrate the applicability of the ILA through several case studies of accelerators (for image processing, machine learning, and cryptography), and a general-purpose processor (RISC-V). We show how the ILA model facilitates equivalence checking between two ILAs, and between an ILA and its hardware finite-state machine (FSM) implementation. Further, this equivalence checking supports accelerator upgrades using the notion of ILA compatibility, similar to processor upgrades using ISA compatibility. ", "link": "http://arxiv.org/abs/1801.01114", "authors": [{"name": "Bo-Yuan Huang", "link": "http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"}, {"name": "Hongce Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"}, {"name": "Pramod Subramanyan", "link": "http://arxiv.org/find/cs/1/au:+Subramanyan_P/0/1/0/all/0/1"}, {"name": "Yakir Vizel", "link": "http://arxiv.org/find/cs/1/au:+Vizel_Y/0/1/0/all/0/1"}, {"name": "Aarti Gupta", "link": "http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"}, {"name": "Sharad Malik", "link": "http://arxiv.org/find/cs/1/au:+Malik_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models. (arXiv:1801.02227v2 [stat.ML] UPDATED)", "description": "We propose a new technique that boosts the convergence of training generative adversarial networks. Generally, the rate of training deep models reduces severely after multiple iterations. A key reason for this phenomenon is that a deep network is expressed using a highly non-convex finite-dimensional model, and thus the parameter gets stuck in a local optimum. Because of this, methods often suffer not only from degeneration of the convergence speed but also from limitations in the representational power of the trained network. To overcome this issue, we propose an additional layer called the gradient layer to seek a descent direction in an infinite-dimensional space. Because the layer is constructed in the infinite-dimensional space, we are not restricted by the specific model structure of finite-dimensional models. As a result, we can get out of the local optima in finite-dimensional models and move towards the global optimal function more directly. In this paper, this phenomenon is explained from the functional gradient method perspective of the gradient layer. Interestingly, the optimization procedure using the gradient layer naturally constructs the deep structure of the network. Moreover, we demonstrate that this procedure can be regarded as a discretization method of the gradient flow that naturally reduces the objective function. Finally, the method is tested using several numerical experiments, which show its fast convergence. ", "link": "http://arxiv.org/abs/1801.02227", "authors": [{"name": "Atsushi Nitanda", "link": "http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1"}, {"name": "Taiji Suzuki", "link": "http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Grassmannian Codes with New Distance Measures for Network Coding. (arXiv:1801.02329v4 [cs.IT] UPDATED)", "description": "Subspace codes are known to be useful in error-correction for random network coding. Recently, they were used to prove that vector network codes outperform scalar linear network codes, on multicast networks, with respect to the alphabet size. The multicast networks which were used are generalized combination networks. In both the scalar and the vector network coding solutions, the subspace distance is used as the distance measure for the codes which solve the coding problem in the generalized combination networks. In this work we show that for a general network coding solution for the generalized combination networks, we can replace the subspace distance with two other possible distance measures which generalize the subspace distance. The two distance measures are shown to be equivalent under an orthogonal transformation. We show that the Grassmannian codes with the new distance measures generalize the subspace codes and the subspace designs with the subspace distance and the strength of the design, respectively. We prove that each code with the largest number of codewords and one of the generalized distances, given the other parameters, has the minimum requirements needed to solve a given generalized combination network with a scalar linear network code. We discuss the coding problems related to these two distance measures and the lower and upper bounds on the sizes of the related codes. ", "link": "http://arxiv.org/abs/1801.02329", "authors": [{"name": "Tuvi Etzion", "link": "http://arxiv.org/find/cs/1/au:+Etzion_T/0/1/0/all/0/1"}, {"name": "Hui Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace. (arXiv:1801.05558v3 [stat.ML] UPDATED)", "description": "Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. Our primary contribution is the {\\em MT-net}, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on. Additionally, a task-specific learner of an {\\em MT-net} performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks. ", "link": "http://arxiv.org/abs/1801.05558", "authors": [{"name": "Yoonho Lee", "link": "http://arxiv.org/find/stat/1/au:+Lee_Y/0/1/0/all/0/1"}, {"name": "Seungjin Choi", "link": "http://arxiv.org/find/stat/1/au:+Choi_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Evaluating Predictive Models of Student Success: Closing the Methodological Gap. (arXiv:1801.08494v2 [stat.AP] UPDATED)", "description": "Model evaluation -- the process of making inferences about the performance of predictive models -- is a critical component of predictive modeling research in learning analytics. We survey the state of the practice with respect to model evaluation in learning analytics, which overwhelmingly uses only naive methods for model evaluation or statistical tests which are not appropriate for predictive model evaluation. We conduct a critical comparison of both null hypothesis significance testing (NHST) and a preferred Bayesian method for model evaluation. Finally, we apply three methods -- the na{\\\"i}ve average commonly used in learning analytics, NHST, and Bayesian -- to a predictive modeling experiment on a large set of MOOC data. We compare 96 different predictive models, including different feature sets, statistical modeling algorithms, and tuning hyperparameters for each, using this case study to demonstrate the different experimental conclusions these evaluation techniques provide. ", "link": "http://arxiv.org/abs/1801.08494", "authors": [{"name": "Josh Gardner", "link": "http://arxiv.org/find/stat/1/au:+Gardner_J/0/1/0/all/0/1"}, {"name": "Christopher Brooks", "link": "http://arxiv.org/find/stat/1/au:+Brooks_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Features, Projections, and Representation Change for Generalized Planning. (arXiv:1801.10055v4 [cs.AI] UPDATED)", "description": "Generalized planning is concerned with the characterization and computation of plans that solve many instances at once. In the standard formulation, a generalized plan is a mapping from feature or observation histories into actions, assuming that the instances share a common pool of features and actions. This assumption, however, excludes the standard relational planning domains where actions and objects change across instances. In this work, we extend the standard formulation of generalized planning to such domains. This is achieved by projecting the actions over the features, resulting in a common set of abstract actions which can be tested for soundness and completeness, and which can be used for generating general policies such as \"if the gripper is empty, pick the clear block above x and place it on the table\" that achieve the goal clear(x) in any Blocksworld instance. In this policy, \"pick the clear block above x\" is an abstract action that may represent the action Unstack(a, b) in one situation and the action Unstack(b, c) in another. Transformations are also introduced for computing such policies by means of fully observable non-deterministic (FOND) planners. The value of generalized representations for learning general policies is also discussed. ", "link": "http://arxiv.org/abs/1801.10055", "authors": [{"name": "Blai Bonet", "link": "http://arxiv.org/find/cs/1/au:+Bonet_B/0/1/0/all/0/1"}, {"name": "Hector Geffner", "link": "http://arxiv.org/find/cs/1/au:+Geffner_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "APPLE Picker: Automatic Particle Picking, a Low-Effort Cryo-EM Framework. (arXiv:1802.00469v2 [cs.CV] UPDATED)", "description": "Particle picking is a crucial first step in the computational pipeline of single-particle cryo-electron microscopy (cryo-EM). Selecting particles from the micrographs is difficult especially for small particles with low contrast. As high-resolution reconstruction typically requires hundreds of thousands of particles, manually picking that many particles is often too time-consuming. While semi-automated particle picking is currently a popular approach, it may suffer from introducing manual bias into the selection process. In addition, semi-automated particle picking is still somewhat time-consuming. This paper presents the APPLE (Automatic Particle Picking with Low user Effort) picker, a simple and novel approach for fast, accurate, and fully automatic particle picking. While our approach was inspired by template matching, it is completely template-free. This approach is evaluated on publicly available datasets containing micrographs of $\\beta$-galactosidase and keyhole limpet hemocyanin projections. ", "link": "http://arxiv.org/abs/1802.00469", "authors": [{"name": "Ayelet Heimowitz", "link": "http://arxiv.org/find/cs/1/au:+Heimowitz_A/0/1/0/all/0/1"}, {"name": "Joakim and&#xe9;n", "link": "http://arxiv.org/find/cs/1/au:+anden_J/0/1/0/all/0/1"}, {"name": "Amit Singer", "link": "http://arxiv.org/find/cs/1/au:+Singer_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Fast Proximal Point Method for Computing Wasserstein Distance. (arXiv:1802.04307v2 [stat.ML] UPDATED)", "description": "Wasserstein distance plays increasingly important roles in machine learning, stochastic programming and image processing. Major efforts have been under way to address its high computational complexity, some leading to approximate or regularized variations such as Sinkhorn distance. However, as we will demonstrate, regularized variations with large regularization parameter will degradate the performance in several important machine learning applications, and small regularization parameter will fail due to numerical stability issues with existing algorithms. We address this challenge by developing an Inexact Proximal point method for Optimal Transport (IPOT) with the proximal operator approximately evaluated at each iteration using projections to the probability simplex. We prove the algorithm has linear convergence rate. We also apply IPOT to learning generative models, and generalize the idea of IPOT to a new method for computing Wasserstein barycenter. ", "link": "http://arxiv.org/abs/1802.04307", "authors": [{"name": "Yujia Xie", "link": "http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1"}, {"name": "Xiangfeng Wang", "link": "http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1"}, {"name": "Ruijia Wang", "link": "http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1"}, {"name": "Hongyuan Zha", "link": "http://arxiv.org/find/stat/1/au:+Zha_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "DiCE: The Infinitely Differentiable Monte-Carlo Estimator. (arXiv:1802.05098v2 [cs.LG] UPDATED)", "description": "The score function estimator is widely used for estimating gradients of stochastic objectives in stochastic computation graphs (SCG), eg, in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order derivatives is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order derivative involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for estimators of higher-order derivatives. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct estimators of derivatives of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and numerical evaluation of the DiCE derivative estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://goo.gl/xkkGxN. ", "link": "http://arxiv.org/abs/1802.05098", "authors": [{"name": "Jakob Foerster", "link": "http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1"}, {"name": "Gregory Farquhar", "link": "http://arxiv.org/find/cs/1/au:+Farquhar_G/0/1/0/all/0/1"}, {"name": "Maruan Al-Shedivat", "link": "http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1"}, {"name": "Tim Rockt&#xe4;schel", "link": "http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1"}, {"name": "Eric P. Xing", "link": "http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1"}, {"name": "Shimon Whiteson", "link": "http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning Privacy Preserving Encodings through Adversarial Training. (arXiv:1802.05214v2 [cs.LG] UPDATED)", "description": "We present a framework to learn privacy-preserving encodings of images (or other high-dimensional data) to inhibit inference of a chosen private attribute. Rather than encoding a fixed dataset or inhibiting a fixed estimator, we aim to to learn an encoding function such that even after this function is fixed, an estimator with knowledge of the encoding is unable to learn to accurately predict the private attribute, when generalizing beyond a training set. We formulate this as adversarial optimization of an encoding function against a classifier for the private attribute, with both modeled as deep neural networks. We describe an optimization approach which successfully yields an encoder that permanently limits inference of the private attribute, while preserving either a generic notion of information, or the estimation of a different, desired, attribute. We experimentally validate the efficacy of our approach on private tasks of real-world complexity, by learning to prevent detection of scene classes from the Places-365 dataset. ", "link": "http://arxiv.org/abs/1802.05214", "authors": [{"name": "Francesco Pittaluga", "link": "http://arxiv.org/find/cs/1/au:+Pittaluga_F/0/1/0/all/0/1"}, {"name": "Sanjeev J. Koppal", "link": "http://arxiv.org/find/cs/1/au:+Koppal_S/0/1/0/all/0/1"}, {"name": "Ayan Chakrabarti", "link": "http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. (arXiv:1802.06093v3 [cs.LG] UPDATED)", "description": "We analyze algorithms for approximating a function $f(x) = \\Phi x$ mapping $\\Re^d$ to $\\Re^d$ using deep linear neural networks, i.e. that learn a function $h$ parameterized by matrices $\\Theta_1,...,\\Theta_L$ and defined by $h(x) = \\Theta_L \\Theta_{L-1} ... \\Theta_1 x$. We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic. ", "link": "http://arxiv.org/abs/1802.06093", "authors": [{"name": "Peter L. Bartlett", "link": "http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"}, {"name": "David P. Helmbold", "link": "http://arxiv.org/find/cs/1/au:+Helmbold_D/0/1/0/all/0/1"}, {"name": "Philip M. Long", "link": "http://arxiv.org/find/cs/1/au:+Long_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A Generative Modeling Approach to Limited Channel ECG Classification. (arXiv:1802.06458v3 [stat.ML] UPDATED)", "description": "Processing temporal sequences is central to a variety of applications in health care, and in particular multi-channel Electrocardiogram (ECG) is a highly prevalent diagnostic modality that relies on robust sequence modeling. While Recurrent Neural Networks (RNNs) have led to significant advances in automated diagnosis with time-series data, they perform poorly when models are trained using a limited set of channels. A crucial limitation of existing solutions is that they rely solely on discriminative models, which tend to generalize poorly in such scenarios. In order to combat this limitation, we develop a generative modeling approach to limited channel ECG classification. This approach first uses a Seq2Seq model to implicitly generate the missing channel information, and then uses the latent representation to perform the actual supervisory task. This decoupling enables the use of unsupervised data and also provides highly robust metric spaces for subsequent discriminative learning. Our experiments with the Physionet dataset clearly evidence the effectiveness of our approach over standard RNNs in disease prediction. ", "link": "http://arxiv.org/abs/1802.06458", "authors": [{"name": "Deepta Rajan", "link": "http://arxiv.org/find/stat/1/au:+Rajan_D/0/1/0/all/0/1"}, {"name": "Jayaraman J. Thiagarajan", "link": "http://arxiv.org/find/stat/1/au:+Thiagarajan_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "On the Connection Between Learning Two-Layers Neural Networks and Tensor Decomposition. (arXiv:1802.07301v2 [cs.LG] UPDATED)", "description": "We establish connections between the problem of learning a two-layers neural network with good generalization error and tensor decomposition. We consider a model with input $\\boldsymbol x \\in \\mathbb R^d$, $r$ hidden units with weights $\\{\\boldsymbol w_i\\}_{1\\le i \\le r}$ and output $y\\in \\mathbb R$, i.e., $y=\\sum_{i=1}^r\\sigma(\\langle\\boldsymbol x, \\boldsymbol w_i\\rangle)$, where $\\langle\\cdot, \\cdot\\rangle$ denotes the scalar product and $\\sigma$ the activation function. First, we show that, if we cannot learn the weights $\\{\\boldsymbol w_i\\}_{1\\le i\\le r}$ accurately, then the neural network does not generalize well. More specifically, the generalization error is close to that of a trivial predictor with access only to the norm of the input. We prove this result in a model with separated isotropic weights and in a model with random weights. In both settings, we assume that the input distribution is Gaussian, which is common in the theoretical literature. Then, we show that the problem of learning the weights $\\{\\boldsymbol w_i\\}_{1\\le i \\le r}$ is at least as hard as the problem of tensor decomposition. We prove this result for any input distribution, and we assume that the activation function is a polynomial whose degree is related to the order of the tensor to be decomposed. Hence, we obtain that learning a two-layers neural network that generalizes well is at least as hard as tensor decomposition. It has been observed that neural network models with more parameters than training samples often generalize well, even if the problem is highly underdetermined. This means that the learning algorithm does not estimate the weights accurately and yet is able to yield a good generalization error. This paper shows that such a phenomenon cannot occur with a two-layers neural network when the input distribution is Gaussian. We also provide numerical evidence supporting our theoretical findings. ", "link": "http://arxiv.org/abs/1802.07301", "authors": [{"name": "Marco Mondelli", "link": "http://arxiv.org/find/cs/1/au:+Mondelli_M/0/1/0/all/0/1"}, {"name": "Andrea Montanari", "link": "http://arxiv.org/find/cs/1/au:+Montanari_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning to Explain: An Information-Theoretic Perspective on Model Interpretation. (arXiv:1802.07814v2 [cs.LG] UPDATED)", "description": "We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation. ", "link": "http://arxiv.org/abs/1802.07814", "authors": [{"name": "Jianbo Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"}, {"name": "Le Song", "link": "http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"}, {"name": "Martin J. Wainwright", "link": "http://arxiv.org/find/cs/1/au:+Wainwright_M/0/1/0/all/0/1"}, {"name": "Michael I. Jordan", "link": "http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity. (arXiv:1802.08183v4 [stat.ML] UPDATED)", "description": "Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of non-convex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized. Although online methods have been introduced, they suffer from similar problems. In this work, we propose Meta-Frank-Wolfe, the first online projection-free algorithm that uses stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves the optimal $O( \\sqrt{T})$ adversarial regret bounds for convex and continuous submodular optimization. We also propose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single stochastic gradient estimate in each round and achieves an $O(T^{2/3})$ stochastic regret bound for convex and continuous submodular optimization. We apply our methods to develop a novel \"lifting\" framework for the online discrete submodular maximization and also see that they outperform current state-of-the-art techniques on various experiments. ", "link": "http://arxiv.org/abs/1802.08183", "authors": [{"name": "Lin Chen", "link": "http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1"}, {"name": "Christopher Harshaw", "link": "http://arxiv.org/find/stat/1/au:+Harshaw_C/0/1/0/all/0/1"}, {"name": "Hamed Hassani", "link": "http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1"}, {"name": "Amin Karbasi", "link": "http://arxiv.org/find/stat/1/au:+Karbasi_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "An Approach to Vehicle Trajectory Prediction Using Automatically Generated Traffic Maps. (arXiv:1802.08632v2 [cs.CV] UPDATED)", "description": "Trajectory and intention prediction of traffic participants is an important task in automated driving and crucial for safe interaction with the environment. In this paper, we present a new approach to vehicle trajectory prediction based on automatically generated maps containing statistical information about the behavior of traffic participants in a given area. These maps are generated based on trajectory observations using image processing and map matching techniques and contain all typical vehicle movements and probabilities in the considered area. Our prediction approach matches an observed trajectory to a behavior contained in the map and uses this information to generate a prediction. We evaluated our approach on a dataset containing over 14000 trajectories and found that it produces significantly more precise mid-term predictions compared to motion model-based prediction approaches. ", "link": "http://arxiv.org/abs/1802.08632", "authors": [{"name": "Jannik Quehl", "link": "http://arxiv.org/find/cs/1/au:+Quehl_J/0/1/0/all/0/1"}, {"name": "Haohao Hu", "link": "http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"}, {"name": "Sascha Wirges", "link": "http://arxiv.org/find/cs/1/au:+Wirges_S/0/1/0/all/0/1"}, {"name": "Martin Lauer", "link": "http://arxiv.org/find/cs/1/au:+Lauer_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "The Price of Stability of Weighted Congestion Games. (arXiv:1802.09952v2 [cs.GT] UPDATED)", "description": "We give exponential lower bounds on the Price of Stability (PoS) of weighted congestion games with polynomial cost functions. In particular, for any positive integer $d$ we construct rather simple games with cost functions of degree at most $d$ which have a PoS of at least $\\varOmega(\\Phi_d)^{d+1}$, where $\\Phi_d\\sim d/\\ln d$ is the unique positive root of equation $x^{d+1}=(x+1)^d$. This essentially closes the huge gap between $\\varTheta(d)$ and $\\Phi_d^{d+1}$ and asymptotically matches the Price of Anarchy upper bound. We further show that the PoS remains exponential even for singleton games. More generally, we also provide a lower bound of $\\varOmega((1+1/\\alpha)^d/d)$ on the PoS of $\\alpha$-approximate Nash equilibria, even for singleton games. All our lower bounds extend to network congestion games, and hold for mixed and correlated equilibria as well. ", "link": "http://arxiv.org/abs/1802.09952", "authors": [{"name": "George Christodoulou", "link": "http://arxiv.org/find/cs/1/au:+Christodoulou_G/0/1/0/all/0/1"}, {"name": "Martin Gairing", "link": "http://arxiv.org/find/cs/1/au:+Gairing_M/0/1/0/all/0/1"}, {"name": "Yiannis Giannakopoulos", "link": "http://arxiv.org/find/cs/1/au:+Giannakopoulos_Y/0/1/0/all/0/1"}, {"name": "Paul G. Spirakis", "link": "http://arxiv.org/find/cs/1/au:+Spirakis_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Online learning over a finite action set with limited switching. (arXiv:1803.01548v2 [cs.LG] UPDATED)", "description": "This paper studies the value of switching actions in the Prediction From Experts (PFE) problem and Adversarial Multi-Armed Bandits (MAB) problem. First, we revisit the well-studied and practically motivated setting of PFE with switching costs. Many algorithms are known to achieve the minimax optimal order of $O(\\sqrt{T \\log n})$ in expectation for both regret and number of switches, where $T$ is the number of iterations and $n$ the number of actions. However, no high probability (h.p.) guarantees are known. Our main technical contribution is the first algorithms which with h.p. achieve this optimal order for both regret and switches. This settles an open problem of [Devroye et al., 2015], and directly implies the first h.p. guarantees for several problems of interest. ", "link": "http://arxiv.org/abs/1803.01548", "authors": [{"name": "Jason Altschuler", "link": "http://arxiv.org/find/cs/1/au:+Altschuler_J/0/1/0/all/0/1"}, {"name": "Kunal Talwar", "link": "http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Sampled-data reachability analysis using sensitivity and mixed-monotonicity. (arXiv:1803.02214v2 [cs.SY] UPDATED)", "description": "This paper over-approximates the reachable sets of a continuous-time uncertain system using the sensitivity of its trajectories with respect to initial conditions and uncertain parameters. We first prove the equivalence between an existing over-approximation result based on the sign-stability of the sensitivity matrices and a discrete-time approach relying on a mixed-monotonicity property. We then present a new over-approximation result which scales at worst linearly with the state dimension and is applicable to any continuous-time system with bounded sensitivity. Finally, we provide a simulation-based approach to estimate these bounds through sampling and falsification. The results are illustrated with numerical examples on traffic networks and satellite orbits. ", "link": "http://arxiv.org/abs/1803.02214", "authors": [{"name": "Pierre-Jean Meyer", "link": "http://arxiv.org/find/cs/1/au:+Meyer_P/0/1/0/all/0/1"}, {"name": "Samuel Coogan", "link": "http://arxiv.org/find/cs/1/au:+Coogan_S/0/1/0/all/0/1"}, {"name": "Murat Arcak", "link": "http://arxiv.org/find/cs/1/au:+Arcak_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Natural Language to Structured Query Generation via Meta-Learning. (arXiv:1803.02400v3 [cs.CL] UPDATED)", "description": "In conventional supervised training, a model is trained to fit all the training examples. However, having a monolithic model may not always be the best strategy, as examples could vary widely. In this work, we explore a different learning protocol that treats each example as a unique pseudo-task, by reducing the original learning problem to a few-shot meta-learning scenario with the help of a domain-dependent relevance function. When evaluated on the WikiSQL dataset, our approach leads to faster convergence and achieves 1.1%--5.4% absolute accuracy gains over the non-meta-learning counterparts. ", "link": "http://arxiv.org/abs/1803.02400", "authors": [{"name": "Po-Sen Huang", "link": "http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"}, {"name": "Chenglong Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"}, {"name": "Rishabh Singh", "link": "http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"}, {"name": "Wen-tau Yih", "link": "http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1"}, {"name": "Xiaodong He", "link": "http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Real-time Cardiovascular MR with Spatio-temporal Artifact Suppression using Deep Learning - Proof of Concept in Congenital Heart Disease. (arXiv:1803.05192v3 [cs.CV] UPDATED)", "description": "PURPOSE: Real-time assessment of ventricular volumes requires high acceleration factors. Residual convolutional neural networks (CNN) have shown potential for removing artifacts caused by data undersampling. In this study we investigated the effect of different radial sampling patterns on the accuracy of a CNN. We also acquired actual real-time undersampled radial data in patients with congenital heart disease (CHD), and compare CNN reconstruction to Compressed Sensing (CS). ", "link": "http://arxiv.org/abs/1803.05192", "authors": [{"name": "Andreas Hauptmann", "link": "http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1"}, {"name": "Simon Arridge", "link": "http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1"}, {"name": "Felix Lucka", "link": "http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1"}, {"name": "Vivek Muthurangu", "link": "http://arxiv.org/find/cs/1/au:+Muthurangu_V/0/1/0/all/0/1"}, {"name": "Jennifer A. Steeden", "link": "http://arxiv.org/find/cs/1/au:+Steeden_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Adaptive Greedy Algorithms for Stochastic Set Cover Problems. (arXiv:1803.07639v6 [cs.DS] UPDATED)", "description": "We study adaptive greedy algorithms for the problems of stochastic set cover with perfect and imperfect coverages. In stochastic set cover with perfect coverage, we are given a set of items and a ground set B. Evaluating an item reveals its state which is a random subset of B drawn from the state distribution of the item. Every element in B is assumed to be present in the state of some item with probability 1. For this problem, we show that the adaptive greedy algorithm has an approximation ratio of H(|B|), the |B|th Harmonic number. In stochastic set cover with imperfect coverage, an element in the ground set need not be present in the state of any item. We show a reduction from this problem to the former problem; the adaptive greedy algorithm for the reduced instance has an approxiation ratio of H(|E|), where E is the set of pairs (F, e) such that the state of item F contains e with positive probability. ", "link": "http://arxiv.org/abs/1803.07639", "authors": [{"name": "Srinivasan Parthasarathy", "link": "http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "The Price of Uncertainty: Chance-constrained OPF vs. In-hindsight OPF. (arXiv:1803.08711v2 [math.OC] UPDATED)", "description": "The operation of power systems has become more challenging due to feed-in of volatile renewable energy sources. Chance-constrained optimal power flow (ccOPF) is one possibility to explicitly consider volatility via probabilistic uncertainties resulting in mean-optimal feedback policies. These policies are computed before knowledge of the realization of the uncertainty is available. On the other hand, the hypothetical case of computing the power injections knowing every realization beforehand---called in-hindsight OPF(hOPF)---cannot be outperformed w.r.t. costs and constraint satisfaction. In this paper, we investigate how ccOPF feedback relates to the full-information hOPF. To this end, we introduce different dimensions of the price of uncertainty. Using mild assumptions on the uncertainty we present sufficient conditions when ccOPF is identical to hOPF. We suggest using the total variational distance of probability densities to quantify the performance gap of hOPF and ccOPF. Finally, we draw upon a tutorial example to illustrate our results. ", "link": "http://arxiv.org/abs/1803.08711", "authors": [{"name": "Tillmann M&#xfc;hlpfordt", "link": "http://arxiv.org/find/math/1/au:+Muhlpfordt_T/0/1/0/all/0/1"}, {"name": "Veit Hagenmeyer", "link": "http://arxiv.org/find/math/1/au:+Hagenmeyer_V/0/1/0/all/0/1"}, {"name": "Timm Faulwasser", "link": "http://arxiv.org/find/math/1/au:+Faulwasser_T/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Dynamic Video Segmentation Network. (arXiv:1804.00931v2 [cs.CV] UPDATED)", "description": "In this paper, we present a detailed design of dynamic video segmentation network (DVSNet) for fast and efficient semantic video segmentation. DVSNet consists of two convolutional neural networks: a segmentation network and a flow network. The former generates highly accurate semantic segmentations, but is deeper and slower. The latter is much faster than the former, but its output requires further processing to generate less accurate semantic segmentations. We explore the use of a decision network to adaptively assign different frame regions to different networks based on a metric called expected confidence score. Frame regions with a higher expected confidence score traverse the flow network. Frame regions with a lower expected confidence score have to pass through the segmentation network. We have extensively performed experiments on various configurations of DVSNet, and investigated a number of variants for the proposed decision network. The experimental results show that our DVSNet is able to achieve up to 70.4% mIoU at 19.8 fps on the Cityscape dataset. A high speed version of DVSNet is able to deliver an fps of 30.4 with 63.2% mIoU on the same dataset. DVSNet is also able to reduce up to 95% of the computational workloads. ", "link": "http://arxiv.org/abs/1804.00931", "authors": [{"name": "Yu-Syuan Xu", "link": "http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"}, {"name": "Tsu-Jui Fu", "link": "http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1"}, {"name": "Hsuan-Kung Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"}, {"name": "Chun-Yi Lee", "link": "http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Implicit Coordination of Caches in Small Cell Networks under Unknown Popularity Profiles. (arXiv:1804.01895v2 [cs.NI] UPDATED)", "description": "We focus on a dense cellular network, in which a limited-size cache is available at every Base Station (BS). In order to optimize the overall performance of the system in such scenario, where a significant fraction of the users is covered by several BSs, a tight coordination among nearby caches is needed. To this end, this pape introduces a class of simple and fully distributed caching policies, which require neither direct communication among BSs, nor a priori knowledge of content popularity. Furthermore, we propose a novel approximate analytical methodology to assess the performance of interacting caches under such policies. Our approach builds upon the well known characteristic time approximation and provides predictions that are surprisingly accurate (hardly distinguishable from the simulations) in most of the scenarios. Both synthetic and trace-driven results show that the our caching policies achieve excellent performance (in some cases provably optimal). They outperform state-of-the-art dynamic policies for interacting caches, and, in some cases, also the greedy content placement, which is known to be the best performing polynomial algorithm under static and perfectly-known content popularity profiles. ", "link": "http://arxiv.org/abs/1804.01895", "authors": [{"name": "Emilio Leonardi", "link": "http://arxiv.org/find/cs/1/au:+Leonardi_E/0/1/0/all/0/1"}, {"name": "Giovanni Neglia", "link": "http://arxiv.org/find/cs/1/au:+Neglia_G/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Speaker Embedding Extraction with Phonetic Information. (arXiv:1804.04862v2 [cs.SD] UPDATED)", "description": "Speaker embeddings achieve promising results on many speaker verification tasks. Phonetic information, as an important component of speech, is rarely considered in the extraction of speaker embeddings. In this paper, we introduce phonetic information to the speaker embedding extraction based on the x-vector architecture. Two methods using phonetic vectors and multi-task learning are proposed. On the Fisher dataset, our best system outperforms the original x-vector approach by 20% in EER, and by 15%, 15% in minDCF08 and minDCF10, respectively. Experiments conducted on NIST SRE10 further demonstrate the effectiveness of the proposed methods. ", "link": "http://arxiv.org/abs/1804.04862", "authors": [{"name": "Yi Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"}, {"name": "Liang He", "link": "http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"}, {"name": "Jia Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"}, {"name": "Michael T. Johnson", "link": "http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "A comparison of methods for model selection when estimating individual treatment effects. (arXiv:1804.05146v2 [stat.ML] UPDATED)", "description": "Practitioners in medicine, business, political science, and other fields are increasingly aware that decisions should be personalized to each patient, customer, or voter. A given treatment (e.g. a drug or advertisement) should be administered only to those who will respond most positively, and certainly not to those who will be harmed by it. Individual-level treatment effects can be estimated with tools adapted from machine learning, but different models can yield contradictory estimates. Unlike risk prediction models, however, treatment effect models cannot be easily evaluated against each other using a held-out test set because the true treatment effect itself is never directly observed. Besides outcome prediction accuracy, several metrics that can leverage held-out data to evaluate treatment effects models have been proposed, but they are not widely used. We provide a didactic framework that elucidates the relationships between the different approaches and compare them all using a variety of simulations of both randomized and observational data. Our results show that researchers estimating heterogenous treatment effects need not limit themselves to a single model-fitting algorithm. Instead of relying on a single method, multiple models fit by a diverse set of algorithms should be evaluated against each other using an objective function learned from the validation set. The model minimizing that objective should be used for estimating the individual treatment effect for future individuals. ", "link": "http://arxiv.org/abs/1804.05146", "authors": [{"name": "Alejandro Schuler", "link": "http://arxiv.org/find/stat/1/au:+Schuler_A/0/1/0/all/0/1"}, {"name": "Michael Baiocchi", "link": "http://arxiv.org/find/stat/1/au:+Baiocchi_M/0/1/0/all/0/1"}, {"name": "Robert Tibshirani", "link": "http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1"}, {"name": "Nigam Shah", "link": "http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Who witnesses The Witness? Finding witnesses in The Witness is hard and sometimes impossible. (arXiv:1804.10193v2 [cs.CC] UPDATED)", "description": "We analyze the computational complexity of the many types of pencil-and-paper-style puzzles featured in the 2016 puzzle video game The Witness. In all puzzles, the goal is to draw a path in a rectangular grid graph from a start vertex to a destination vertex. The different puzzle types place different constraints on the path: preventing some edges from being visited (broken edges); forcing some edges or vertices to be visited (hexagons); forcing some cells to have certain numbers of incident path edges (triangles); or forcing the regions formed by the path to be partially monochromatic (squares), have exactly two special cells (stars), or be singly covered by given shapes (polyominoes) and/or negatively counting shapes (antipolyominoes). We show that any one of these clue types (except the first) is enough to make path finding NP-complete (\"witnesses exist but are hard to find\"), even for rectangular boards. Furthermore, we show that a final clue type (antibody), which necessarily \"cancels\" the effect of another clue in the same region, makes path finding $\\Sigma_2$-complete (\"witnesses do not exist\"), even with a single antibody (combined with many anti/polyominoes), and the problem gets no harder with many antibodies. ", "link": "http://arxiv.org/abs/1804.10193", "authors": [{"name": "Zachary Abel", "link": "http://arxiv.org/find/cs/1/au:+Abel_Z/0/1/0/all/0/1"}, {"name": "Jeffrey Bosboom", "link": "http://arxiv.org/find/cs/1/au:+Bosboom_J/0/1/0/all/0/1"}, {"name": "Erik D. Demaine", "link": "http://arxiv.org/find/cs/1/au:+Demaine_E/0/1/0/all/0/1"}, {"name": "Linus Hamilton", "link": "http://arxiv.org/find/cs/1/au:+Hamilton_L/0/1/0/all/0/1"}, {"name": "Adam Hesterberg", "link": "http://arxiv.org/find/cs/1/au:+Hesterberg_A/0/1/0/all/0/1"}, {"name": "Justin Kopinsky", "link": "http://arxiv.org/find/cs/1/au:+Kopinsky_J/0/1/0/all/0/1"}, {"name": "Jayson Lynch", "link": "http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1"}, {"name": "Mikhail Rudoy", "link": "http://arxiv.org/find/cs/1/au:+Rudoy_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Offline Evaluation of Ranking Policies with Click Models. (arXiv:1804.10488v2 [cs.LG] UPDATED)", "description": "Many web systems rank and present a list of items to users, from recommender systems to search and advertising. An important problem in practice is to evaluate new ranking policies offline and optimize them before they are deployed. We address this problem by proposing evaluation algorithms for estimating the expected number of clicks on ranked lists from historical logged data. The existing algorithms are not guaranteed to be statistically efficient in our problem because the number of recommended lists can grow exponentially with their length. To overcome this challenge, we use models of user interaction with the list of items, the so-called click models, to construct estimators that learn statistically efficiently. We analyze our estimators and prove that they are more efficient than the estimators that do not use the structure of the click model, under the assumption that the click model holds. We evaluate our estimators in a series of experiments on a real-world dataset and show that they consistently outperform prior estimators. ", "link": "http://arxiv.org/abs/1804.10488", "authors": [{"name": "Shuai Li", "link": "http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"}, {"name": "Yasin Abbasi-Yadkori", "link": "http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1"}, {"name": "Branislav Kveton", "link": "http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"}, {"name": "S. Muthukrishnan", "link": "http://arxiv.org/find/cs/1/au:+Muthukrishnan_S/0/1/0/all/0/1"}, {"name": "Vishwa Vinay", "link": "http://arxiv.org/find/cs/1/au:+Vinay_V/0/1/0/all/0/1"}, {"name": "Zheng Wen", "link": "http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Stingray Detection of Aerial Images Using Augmented Training Images Generated by A Conditional Generative Model. (arXiv:1805.04262v2 [cs.CV] UPDATED)", "description": "In this paper, we present an object detection method that tackles the stingray detection problem based on aerial images. In this problem, the images are aerially captured on a sea-surface area by using an Unmanned Aerial Vehicle (UAV), and the stingrays swimming under (but close to) the sea surface are the target we want to detect and locate. To this end, we use a deep object detection method, faster RCNN, to train a stingray detector based on a limited training set of images. To boost the performance, we develop a new generative approach, conditional GLO, to increase the training samples of stingray, which is an extension of the Generative Latent Optimization (GLO) approach. Unlike traditional data augmentation methods that generate new data only for image classification, our proposed method that mixes foreground and background together can generate new data for an object detection task, and thus improve the training efficacy of a CNN detector. Experimental results show that satisfiable performance can be obtained by using our approach on stingray detection in aerial images. ", "link": "http://arxiv.org/abs/1805.04262", "authors": [{"name": "Yi-Min Chou", "link": "http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"}, {"name": "Chien-Hung Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"}, {"name": "Keng-Hao Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"}, {"name": "Chu-Song Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Multi-version Coding with Side Information. (arXiv:1805.04337v2 [cs.IT] UPDATED)", "description": "In applications of storage systems to modern key-value stores, the stored data is highly dynamic due to frequent updates from the system write clients. The multi-version coding problem has been formulated to study the cost of storing dynamic data in asynchronous distributed storage systems. In this problem, previous work considered a completely decentralized system where a server is not aware of which versions of the data are received by the other servers. In this paper, we relax this assumption and study a system where a server may acquire side information of the versions propagated to some other servers. In particular, we study a storage system with $n$ servers that store $\\nu$ totally ordered independent versions of a message. Each server receives a subset of these $\\nu$ versions that defines the state of that server. ", "link": "http://arxiv.org/abs/1805.04337", "authors": [{"name": "Ramy E. Ali", "link": "http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1"}, {"name": "Viveck Cadambe", "link": "http://arxiv.org/find/cs/1/au:+Cadambe_V/0/1/0/all/0/1"}, {"name": "Jaime Llorca", "link": "http://arxiv.org/find/cs/1/au:+Llorca_J/0/1/0/all/0/1"}, {"name": "Antonia Tulino", "link": "http://arxiv.org/find/cs/1/au:+Tulino_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "The Parallel Persistent Memory Model. (arXiv:1805.05580v2 [cs.DC] UPDATED)", "description": "We consider a parallel computational model that consists of $P$ processors, each with a fast local ephemeral memory of limited size, and sharing a large persistent memory. The model allows for each processor to fault with bounded probability, and possibly restart. On faulting all processor state and local ephemeral memory are lost, but the persistent memory remains. This model is motivated by upcoming non-volatile memories that are as fast as existing random access memory, are accessible at the granularity of cache lines, and have the capability of surviving power outages. It is further motivated by the observation that in large parallel systems, failure of processors and their caches is not unusual. ", "link": "http://arxiv.org/abs/1805.05580", "authors": [{"name": "Guy E. Blelloch", "link": "http://arxiv.org/find/cs/1/au:+Blelloch_G/0/1/0/all/0/1"}, {"name": "Phillip B. Gibbons", "link": "http://arxiv.org/find/cs/1/au:+Gibbons_P/0/1/0/all/0/1"}, {"name": "Yan Gu", "link": "http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"}, {"name": "Charles McGuffey", "link": "http://arxiv.org/find/cs/1/au:+McGuffey_C/0/1/0/all/0/1"}, {"name": "Julian Shun", "link": "http://arxiv.org/find/cs/1/au:+Shun_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation. (arXiv:1805.07509v3 [cs.CV] UPDATED)", "description": "Recently, Image-to-Image Translation (IIT) has made great progress in enabling image style transfer and manipulation of semantic context in an image. However, existing approaches require exhaustive labelling of training data, which is labor demanding, difficult to scale up, and hard to adapt to a new domain. To overcome such a key limitation, we propose sparsely grouped generative adversarial networks(SG-GAN), a novel approach that can perform image translation in the sparsely grouped datasets, which most training data are mixed and just a few are labelled. SG-GAN with one-input multiple output architecture can be used for the translations among multiple groups using only a single trained model. As a case study for experimentally validating the advantages of our model, we apply the algorithm to tackle a series of tasks of attribute manipulation for facial images. Experiment results show that SG-GAN can achieve competitive results compared with previous state-of-the-art methods on adequately labelled datasets while attaining a superior quality of image translation results on sparsely grouped datasets where most data is mixed and only small parts are labelled. ", "link": "http://arxiv.org/abs/1805.07509", "authors": [{"name": "Jichao Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"}, {"name": "Yezhi Shu", "link": "http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1"}, {"name": "Songhua Xu", "link": "http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"}, {"name": "Gongze Cao", "link": "http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1"}, {"name": "Fan Zhong", "link": "http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1"}, {"name": "Xueying Qin", "link": "http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Teaching Multiple Concepts to Forgetful Learners. (arXiv:1805.08322v2 [cs.AI] UPDATED)", "description": "How can we help a forgetful learner learn multiple concepts within a limited time frame? For long-term learning, it is crucial to devise teaching strategies that leverage the underlying forgetting mechanisms of the learners. In this paper, we cast the problem of adaptively teaching a forgetful learner as a novel discrete optimization problem, where we seek to optimize a natural objective function that characterizes the learner's expected performance throughout the teaching session. We then propose a simple greedy teaching strategy and derive strong performance guarantees based on two intuitive data-dependent parameters, which characterize the degree of diminishing returns of teaching each concept. We show that, given some assumptions of the learner's memory model, one can efficiently compute the performance bounds. Furthermore, we identify parameter settings of our memory models where greedy is guaranteed to achieve high performance. We have deployed our approach in two concrete applications, namely (1) an educational app for online vocabulary teaching and (2) an app for teaching novices how to recognize bird species. We demonstrate the effectiveness of our algorithm using simulations along with user studies. ", "link": "http://arxiv.org/abs/1805.08322", "authors": [{"name": "Anette Hunziker", "link": "http://arxiv.org/find/cs/1/au:+Hunziker_A/0/1/0/all/0/1"}, {"name": "Yuxin Chen", "link": "http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"}, {"name": "Oisin Mac Aodha", "link": "http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1"}, {"name": "Manuel Gomez Rodriguez", "link": "http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1"}, {"name": "Andreas Krause", "link": "http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"}, {"name": "Pietro Perona", "link": "http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1"}, {"name": "Yisong Yue", "link": "http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"}, {"name": "Adish Singla", "link": "http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Learning towards Minimum Hyperspherical Energy. (arXiv:1805.09298v3 [cs.LG] UPDATED)", "description": "Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as even as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our method, by showing the superior performance with MHE regularization. ", "link": "http://arxiv.org/abs/1805.09298", "authors": [{"name": "Weiyang Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"}, {"name": "Rongmei Lin", "link": "http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"}, {"name": "Zhen Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"}, {"name": "Lixin Liu", "link": "http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"}, {"name": "Zhiding Yu", "link": "http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"}, {"name": "Bo Dai", "link": "http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"}, {"name": "Le Song", "link": "http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Environmental Sound Classification Based on Multi-temporal Resolution Convolutional Neural Network Combining with Multi-level Features. (arXiv:1805.09752v2 [cs.SD] UPDATED)", "description": "Motivated by the fact that characteristics of different sound classes are highly diverse in different temporal scales and hierarchical levels, a novel deep convolutional neural network (CNN) architecture is proposed for the environmental sound classification task. This network architecture takes raw waveforms as input, and a set of separated parallel CNNs are utilized with different convolutional filter sizes and strides, in order to learn feature representations with multi-temporal resolutions. On the other hand, the proposed architecture also aggregates hierarchical features from multi-level CNN layers for classification using direct connections between convolutional layers, which is beyond the typical single-level CNN features employed by the majority of previous studies. This network architecture also improves the flow of information and avoids vanishing gradient problem. The combination of multi-level features boosts the classification performance significantly. Comparative experiments are conducted on two datasets: the environmental sound classification dataset (ESC-50), and DCASE 2017 audio scene classification dataset. Results demonstrate that the proposed method is highly effective in the classification tasks by employing multi-temporal resolution and multi-level features, and it outperforms the previous methods which only account for single-level features. ", "link": "http://arxiv.org/abs/1805.09752", "authors": [{"name": "Boqing Zhu", "link": "http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1"}, {"name": "Kele Xu", "link": "http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"}, {"name": "Dezhi Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"}, {"name": "Lilun Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"}, {"name": "Bo Li", "link": "http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"}, {"name": "Yuxing Peng", "link": "http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Adding New Tasks to a Single Network with Weight Transformations using Binary Masks. (arXiv:1805.11119v2 [cs.CV] UPDATED)", "description": "Visual recognition algorithms are required today to exhibit adaptive abilities. Given a deep model trained on a specific, given task, it would be highly desirable to be able to adapt incrementally to new tasks, preserving scalability as the number of new tasks increases, while at the same time avoiding catastrophic forgetting issues. Recent work has shown that masking the internal weights of a given original conv-net through learned binary variables is a promising strategy. We build upon this intuition and take into account more elaborated affine transformations of the convolutional weights that include learned binary masks. We show that with our generalization it is possible to achieve significantly higher levels of adaptation to new tasks, enabling the approach to compete with fine tuning strategies by requiring slightly more than 1 bit per network parameter per additional task. Experiments on two popular benchmarks showcase the power of our approach, that achieves the new state of the art on the Visual Decathlon Challenge. ", "link": "http://arxiv.org/abs/1805.11119", "authors": [{"name": "Massimiliano Mancini", "link": "http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1"}, {"name": "Elisa Ricci", "link": "http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"}, {"name": "Barbara Caputo", "link": "http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1"}, {"name": "Samuel Rota Bul&#xf2;", "link": "http://arxiv.org/find/cs/1/au:+Bulo_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Incremental Natural Language Processing: Challenges, Strategies, and Evaluation. (arXiv:1805.12518v2 [cs.CL] UPDATED)", "description": "Incrementality is ubiquitous in human-human interaction and beneficial for human-computer interaction. It has been a topic of research in different parts of the NLP community, mostly with focus on the specific topic at hand even though incremental systems have to deal with similar challenges regardless of domain. In this survey, I consolidate and categorize the approaches, identifying similarities and differences in the computation and data, and show trade-offs that have to be considered. A focus lies on evaluating incremental systems because the standard metrics often fail to capture the incremental properties of a system and coming up with a suitable evaluation scheme is non-trivial. ", "link": "http://arxiv.org/abs/1805.12518", "authors": [{"name": "Arne K&#xf6;hn", "link": "http://arxiv.org/find/cs/1/au:+Kohn_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators. (arXiv:1806.00149v2 [cs.NE] UPDATED)", "description": "We propose a new generic type of stochastic neurons, called $q$-neurons, that considers activation functions based on Jackson's $q$-derivatives with stochastic parameters $q$. Our generalization of neural network architectures with $q$-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions. ", "link": "http://arxiv.org/abs/1806.00149", "authors": [{"name": "Frank Nielsen", "link": "http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1"}, {"name": "Ke Sun", "link": "http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Capacity of Single-Server Single-Message Private Information Retrieval with Coded Side Information. (arXiv:1806.00661v2 [cs.IT] UPDATED)", "description": "This paper considers the problem of single-server single-message private information retrieval with coded side information (PIR-CSI). In this problem, there is a server storing a database, and a user which knows a linear combination of a subset of messages in the database as a side information. The number of messages contributing to the side information is known to the server, but the indices and the coefficients of these messages are unknown to the server. The user wishes to download a message from the server privately, i.e., without revealing which message it is requesting, while minimizing the download cost. In this work, we consider two different settings for the PIR-CSI problem depending on the demanded message being or not being one of the messages contributing to the side information. For each setting, we prove an upper bound on the maximum download rate as a function of the size of the database and the size of the side information, and propose a protocol that achieves the rate upper-bound. ", "link": "http://arxiv.org/abs/1806.00661", "authors": [{"name": "Anoosheh Heidarzadeh", "link": "http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1"}, {"name": "Fatemeh Kazemi", "link": "http://arxiv.org/find/cs/1/au:+Kazemi_F/0/1/0/all/0/1"}, {"name": "Alex Sprintson", "link": "http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Stress Test Evaluation for Natural Language Inference. (arXiv:1806.00692v3 [cs.CL] UPDATED)", "description": "Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed \"stress tests\" that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area. ", "link": "http://arxiv.org/abs/1806.00692", "authors": [{"name": "Aakanksha Naik", "link": "http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1"}, {"name": "Abhilasha Ravichander", "link": "http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1"}, {"name": "Norman Sadeh", "link": "http://arxiv.org/find/cs/1/au:+Sadeh_N/0/1/0/all/0/1"}, {"name": "Carolyn Rose", "link": "http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1"}, {"name": "Graham Neubig", "link": "http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Passive Static Equilibrium with Frictional Contacts and Application to Grasp Stability Analysis. (arXiv:1806.01384v2 [cs.RO] UPDATED)", "description": "This paper studies the problem of passive grasp stability under an external disturbance, that is, the ability of a grasp to resist a disturbance through passive responses at the contacts. To obtain physically consistent results, such a model must account for friction phenomena at each contact; the difficulty is that friction forces depend in non-linear fashion on contact behavior (stick or slip). We develop the first polynomial-time algorithm which either solves such complex equilibrium constraints for two-dimensional grasps, or otherwise concludes that no solution exists. To achieve this, we show that the number of possible `slip states' (where each contact is labeled as either sticking or slipping) that must be considered is polynomial (in fact quadratic) in the number of contacts, and not exponential as previously thought. Our algorithm captures passive response behaviors at each contact, while accounting for constraints on friction forces such as the maximum dissipation principle. ", "link": "http://arxiv.org/abs/1806.01384", "authors": [{"name": "Maximilian Haas-Heger", "link": "http://arxiv.org/find/cs/1/au:+Haas_Heger_M/0/1/0/all/0/1"}, {"name": "Christos Papadimitriou", "link": "http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1"}, {"name": "Mihalis Yannakakis", "link": "http://arxiv.org/find/cs/1/au:+Yannakakis_M/0/1/0/all/0/1"}, {"name": "Garud Iyengar", "link": "http://arxiv.org/find/cs/1/au:+Iyengar_G/0/1/0/all/0/1"}, {"name": "Matei Ciocarlie", "link": "http://arxiv.org/find/cs/1/au:+Ciocarlie_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "How Do Source-side Monolingual Word Embeddings Impact Neural Machine Translation?. (arXiv:1806.01515v2 [cs.CL] UPDATED)", "description": "Using pre-trained word embeddings as input layer is a common practice in many natural language processing (NLP) tasks, but it is largely neglected for neural machine translation (NMT). In this paper, we conducted a systematic analysis on the effect of using pre-trained source-side monolingual word embedding in NMT. We compared several strategies, such as fixing or updating the embeddings during NMT training on varying amounts of data, and we also proposed a novel strategy called dual-embedding that blends the fixing and updating strategies. Our results suggest that pre-trained embeddings can be helpful if properly incorporated into NMT, especially when parallel data is limited or additional in-domain monolingual data is readily available. ", "link": "http://arxiv.org/abs/1806.01515", "authors": [{"name": "Shuoyang Ding", "link": "http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"}, {"name": "Kevin Duh", "link": "http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. (arXiv:1806.01811v4 [stat.ML] UPDATED)", "description": "Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine tune parameters such as the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization, which is quite different from the offline and nonconvex setting where adaptive gradient methods shine in practice. We bridge this gap by providing strong theoretical guarantees in batch and stochastic setting, for the convergence of AdaGrad over smooth, nonconvex landscapes, from any initialization of the stepsize, without knowledge of Lipschitz constant of the gradient. We show in the stochastic setting that AdaGrad converges to a stationary point at the optimal $O(1/\\sqrt{N})$ rate (up to a $\\log(N)$ factor), and in the batch setting, at the optimal $O(1/N)$ rate. Moreover, in both settings, the constant in the rate matches the constant obtained as if the variance of the gradient noise and Lipschitz constant of the gradient were known in advance and used to tune the stepsize, up to a logarithmic factor of the mismatch between the optimal stepsize and the stepsize used to initialize AdaGrad. In particular, our results imply that AdaGrad is robust to both the unknown Lipschitz constant and level of stochastic noise on the gradient, in a near-optimal sense. When there is noise, AdaGrad converges at the rate of $O(1/\\sqrt{N})$ with well-tuned stepsize, and when there is not noise, the same algorithm converges at the rate of $O(1/N)$ like well-tuned batch gradient descent. ", "link": "http://arxiv.org/abs/1806.01811", "authors": [{"name": "Rachel Ward", "link": "http://arxiv.org/find/stat/1/au:+Ward_R/0/1/0/all/0/1"}, {"name": "Xiaoxia Wu", "link": "http://arxiv.org/find/stat/1/au:+Wu_X/0/1/0/all/0/1"}, {"name": "Leon Bottou", "link": "http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Data Synthesis based on Generative Adversarial Networks. (arXiv:1806.03384v2 [cs.DB] UPDATED)", "description": "Privacy is an important concern for our society where sharing data with partners or releasing data to the public is a frequent occurrence. Some of the techniques that are being used to achieve privacy are to remove identifiers, alter quasi-identifiers, and perturb values. Unfortunately, these approaches suffer from two limitations. First, it has been shown that private information can still be leaked if attackers possess some background knowledge or other information sources. Second, they do not take into account the adverse impact these methods will have on the utility of the released data. In this paper, we propose a method that meets both requirements. Our method, called table-GAN, uses generative adversarial networks (GANs) to synthesize fake tables that are statistically similar to the original table yet do not incur information leakage. We show that the machine learning models trained using our synthetic tables exhibit performance that is similar to that of models trained using the original table for unknown testing cases. We call this property model compatibility. We believe that anonymization/perturbation/synthesis methods without model compatibility are of little value. We used four real-world datasets from four different domains for our experiments and conducted in-depth comparisons with state-of-the-art anonymization\\perturbation\\generation techniques. Throughout our experiments, only our method consistently shows a balance between privacy level and model compatibility. ", "link": "http://arxiv.org/abs/1806.03384", "authors": [{"name": "Noseong Park", "link": "http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1"}, {"name": "Mahmoud Mohammadi", "link": "http://arxiv.org/find/cs/1/au:+Mohammadi_M/0/1/0/all/0/1"}, {"name": "Kshitij Gorde", "link": "http://arxiv.org/find/cs/1/au:+Gorde_K/0/1/0/all/0/1"}, {"name": "Sushil Jajodia", "link": "http://arxiv.org/find/cs/1/au:+Jajodia_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "IVUS-Net: An Intravascular Ultrasound Segmentation Network. (arXiv:1806.03583v2 [stat.ML] UPDATED)", "description": "IntraVascular UltraSound (IVUS) is one of the most effective imaging modalities that provides assistance to experts in order to diagnose and treat cardiovascular diseases. We address a central problem in IVUS image analysis with Fully Convolutional Network (FCN): automatically delineate the lumen and media-adventitia borders in IVUS images, which is crucial to shorten the diagnosis process or benefits a faster and more accurate 3D reconstruction of the artery. Particularly, we propose an FCN architecture, called IVUS-Net, followed by a post-processing contour extraction step, in order to automatically segments the interior (lumen) and exterior (media-adventitia) regions of the human arteries. We evaluated our IVUS-Net on the test set of a standard publicly available dataset containing 326 IVUS B-mode images with two measurements, namely Jaccard Measure (JM) and Hausdorff Distances (HD). The evaluation result shows that IVUS-Net outperforms the state-of-the-art lumen and media segmentation methods by 4% to 20% in terms of HD distance. IVUS-Net performs well on images in the test set that contain a significant amount of major artifacts such as bifurcations, shadows, and side branches that are not common in the training set. Furthermore, using a modern GPU, IVUS-Net segments each IVUS frame only in 0.15 seconds. The proposed work, to the best of our knowledge, is the first deep learning based method for segmentation of both the lumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the best results without any manual intervention. Code is available at https://github.com/Kulbear/ivus-segmentation-icsm2018 ", "link": "http://arxiv.org/abs/1806.03583", "authors": [{"name": "Ji Yang", "link": "http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1"}, {"name": "Lin Tong", "link": "http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1"}, {"name": "Mehdi Faraji", "link": "http://arxiv.org/find/stat/1/au:+Faraji_M/0/1/0/all/0/1"}, {"name": "Anup Basu", "link": "http://arxiv.org/find/stat/1/au:+Basu_A/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Context-Aware Policy Reuse. (arXiv:1806.03793v2 [cs.AI] UPDATED)", "description": "Transfer learning can greatly speed up reinforcement learning for a new task by leveraging policies of relevant tasks. ", "link": "http://arxiv.org/abs/1806.03793", "authors": [{"name": "Siyuan Li", "link": "http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"}, {"name": "Fangda Gu", "link": "http://arxiv.org/find/cs/1/au:+Gu_F/0/1/0/all/0/1"}, {"name": "Guangxiang Zhu", "link": "http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1"}, {"name": "Chongjie Zhang", "link": "http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Aggregating Predictions on Multiple Non-disclosed Datasets using Conformal Prediction. (arXiv:1806.04000v2 [stat.ML] UPDATED)", "description": "Conformal Prediction is a machine learning methodology that produces valid prediction regions under mild conditions. In this paper, we explore the application of making predictions over multiple data sources of different sizes without disclosing data between the sources. We propose that each data source applies a transductive conformal predictor independently using the local data, and that the individual predictions are then aggregated to form a combined prediction region. We demonstrate the method on several data sets, and show that the proposed method produces conservatively valid predictions and reduces the variance in the aggregated predictions. We also study the effect that the number of data sources and size of each source has on aggregated predictions, as compared with equally sized sources and pooled data. ", "link": "http://arxiv.org/abs/1806.04000", "authors": [{"name": "Ola Spjuth", "link": "http://arxiv.org/find/stat/1/au:+Spjuth_O/0/1/0/all/0/1"}, {"name": "Lars Carlsson", "link": "http://arxiv.org/find/stat/1/au:+Carlsson_L/0/1/0/all/0/1"}, {"name": "Niharika Gauraha", "link": "http://arxiv.org/find/stat/1/au:+Gauraha_N/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Projecting Embeddings for Domain Adaptation: Joint Modeling of Sentiment Analysis in Diverse Domains. (arXiv:1806.04381v2 [cs.CL] UPDATED)", "description": "Domain adaptation for sentiment analysis is challenging due to the fact that supervised classifiers are very sensitive to changes in domain. The two most prominent approaches to this problem are structural correspondence learning and autoencoders. However, they either require long training times or suffer greatly on highly divergent domains. Inspired by recent advances in cross-lingual sentiment analysis, we provide a novel perspective and cast the domain adaptation problem as an embedding projection task. Our model takes as input two mono-domain embedding spaces and learns to project them to a bi-domain space, which is jointly optimized to (1) project across domains and to (2) predict sentiment. We perform domain adaptation experiments on 20 source-target domain pairs for sentiment classification and report novel state-of-the-art results on 11 domain pairs, including the Amazon domain adaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows that our model performs comparably to state-of-the-art approaches on domains that are similar, while performing significantly better on highly divergent domains. Our code is available at https://github.com/jbarnesspain/domain_blse ", "link": "http://arxiv.org/abs/1806.04381", "authors": [{"name": "Jeremy Barnes", "link": "http://arxiv.org/find/cs/1/au:+Barnes_J/0/1/0/all/0/1"}, {"name": "Roman Klinger", "link": "http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1"}, {"name": "Sabine Schulte im Walde", "link": "http://arxiv.org/find/cs/1/au:+Walde_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Evaluation of Unsupervised Compositional Representations. (arXiv:1806.04713v2 [cs.CL] UPDATED)", "description": "We evaluated various compositional models, from bag-of-words representations to compositional RNN-based models, on several extrinsic supervised and unsupervised evaluation benchmarks. Our results confirm that weighted vector averaging can outperform context-sensitive models in most benchmarks, but structural features encoded in RNN models can also be useful in certain classification tasks. We analyzed some of the evaluation datasets to identify the aspects of meaning they measure and the characteristics of the various models that explain their performance variance. ", "link": "http://arxiv.org/abs/1806.04713", "authors": [{"name": "Hanan Aldarmaki", "link": "http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1"}, {"name": "Mona Diab", "link": "http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Talakat: Bullet Hell Generation through Constrained Map-Elites. (arXiv:1806.04718v2 [cs.AI] UPDATED)", "description": "We describe a search-based approach to generating new levels for bullet hell games, which are action games characterized by and requiring avoidance of a very large amount of projectiles. Levels are represented using a domain-specific description language, and search in the space defined by this language is performed by a novel variant of the Map-Elites algorithm which incorporates a feasible- infeasible approach to constraint satisfaction. Simulation-based evaluation is used to gauge the fitness of levels, using an agent based on best-first search. The performance of the agent can be tuned according to the two dimensions of strategy and dexterity, making it possible to search for level configurations that require a specific combination of both. As far as we know, this paper describes the first generator for this game genre, and includes several algorithmic innovations. ", "link": "http://arxiv.org/abs/1806.04718", "authors": [{"name": "Ahmed Khalifa", "link": "http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1"}, {"name": "Scott Lee", "link": "http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"}, {"name": "Andy Nealen", "link": "http://arxiv.org/find/cs/1/au:+Nealen_A/0/1/0/all/0/1"}, {"name": "Julian Togelius", "link": "http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Deep learning to represent sub-grid processes in climate models. (arXiv:1806.04731v2 [physics.ao-ph] UPDATED)", "description": "The representation of nonlinear sub-grid processes, especially clouds, has been a major source of uncertainty in climate models for decades. Cloud-resolving models better represent many of these processes and can now be run globally but only for short-term simulations of at most a few years because of computational limitations. Here we demonstrate that deep learning can be used to capture many advantages of cloud-resolving modeling at a fraction of the computational cost. We train a deep neural network to represent all atmospheric sub-grid processes in a climate model by learning from a multi-scale model in which convection is treated explicitly. The trained neural network then replaces the traditional sub-grid parameterizations in a global general circulation model in which it freely interacts with the resolved dynamics and the surface-flux scheme. The prognostic multi-year simulations are stable and closely reproduce not only the mean climate of the cloud-resolving simulation but also key aspects of variability, including precipitation extremes and the equatorial wave spectrum. Furthermore, the neural network approximately conserves energy despite not being explicitly instructed to. Finally, we show that the neural network parameterization generalizes to new surface forcing patterns but struggles to cope with temperatures far outside its training manifold. Our results show the feasibility of using deep learning for climate model parameterization. In a broader context, we anticipate that data-driven Earth System Model development could play a key role in reducing climate prediction uncertainty in the coming decade. ", "link": "http://arxiv.org/abs/1806.04731", "authors": [{"name": "Stephan Rasp", "link": "http://arxiv.org/find/physics/1/au:+Rasp_S/0/1/0/all/0/1"}, {"name": "Michael S. Pritchard", "link": "http://arxiv.org/find/physics/1/au:+Pritchard_M/0/1/0/all/0/1"}, {"name": "Pierre Gentine", "link": "http://arxiv.org/find/physics/1/au:+Gentine_P/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "SGM: Sequence Generation Model for Multi-label Classification. (arXiv:1806.04822v2 [cs.CL] UPDATED)", "description": "Multi-label classification is an important yet challenging task in natural language processing. It is more complex than single-label classification in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently for predicting different labels, which is not considered by existing models. In this paper, we propose to view the multi-label classification task as a sequence generation problem, and apply a sequence generation model with a novel decoder structure to solve it. Extensive experimental results show that our proposed methods outperform previous work by a substantial margin. Further analysis of experimental results demonstrates that the proposed methods not only capture the correlations between labels, but also select the most informative words automatically when predicting different labels. ", "link": "http://arxiv.org/abs/1806.04822", "authors": [{"name": "Pengcheng Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"}, {"name": "Xu Sun", "link": "http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"}, {"name": "Wei Li", "link": "http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"}, {"name": "Shuming Ma", "link": "http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"}, {"name": "Wei Wu", "link": "http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"}, {"name": "Houfeng Wang", "link": "http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Interpretable Partitioned Emebedding for Customized Fashion Outfit Composition. (arXiv:1806.04845v2 [cs.CV] UPDATED)", "description": "Intelligent fashion outfit composition becomes more and more popular in these years. Some deep learning based approaches reveal competitive composition recently. However, the unexplainable characteristic makes such deep learning based approach cannot meet the the designer, businesses and consumers' urge to comprehend the importance of different attributes in an outfit composition. To realize interpretable and customized fashion outfit compositions, we propose a partitioned embedding network to learn interpretable representations from clothing items. The overall network architecture consists of three components: an auto-encoder module, a supervised attributes module and a multi-independent module. The auto-encoder module serves to encode all useful information into the embedding. In the supervised attributes module, multiple attributes labels are adopted to ensure that different parts of the overall embedding correspond to different attributes. In the multi-independent module, adversarial operation are adopted to fulfill the mutually independent constraint. With the interpretable and partitioned embedding, we then construct an outfit composition graph and an attribute matching map. Given specified attributes description, our model can recommend a ranked list of outfit composition with interpretable matching scores. Extensive experiments demonstrate that 1) the partitioned embedding have unmingled parts which corresponding to different attributes and 2) outfits recommended by our model are more desirable in comparison with the existing methods. ", "link": "http://arxiv.org/abs/1806.04845", "authors": [{"name": "Zunlei Feng", "link": "http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"}, {"name": "Zhenyu Yu", "link": "http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"}, {"name": "Yezhou Yang", "link": "http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"}, {"name": "Yongcheng Jing", "link": "http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1"}, {"name": "Junxiao Jiang", "link": "http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"}, {"name": "Mingli Song", "link": "http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Adversarial Learning with Local Coordinate Coding. (arXiv:1806.04895v2 [cs.CV] UPDATED)", "description": "Generative adversarial networks (GANs) aim to generate realistic data from some prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data, which, however, is hard to be used for sampling in GANs. In this paper, rather than sampling from the pre-defined prior distribution, we propose a Local Coordinate Coding (LCC) based sampling method to improve GANs. We derive a generalization bound for LCC based GANs and prove that a small dimensional input is sufficient to achieve good generalization. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method. ", "link": "http://arxiv.org/abs/1806.04895", "authors": [{"name": "Jiezhang Cao", "link": "http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"}, {"name": "Yong Guo", "link": "http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"}, {"name": "Qingyao Wu", "link": "http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"}, {"name": "Chunhua Shen", "link": "http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"}, {"name": "Junzhou Huang", "link": "http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"}, {"name": "Mingkui Tan", "link": "http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "On Accurate Evaluation of GANs for Language Generation. (arXiv:1806.04936v2 [cs.CL] UPDATED)", "description": "Generative Adversarial Networks (GANs) are a promising approach to language generation. The latest works introducing novel GAN models for language generation use n-gram based metrics for evaluation and only report single scores of the best run. In this paper, we argue that this often misrepresents the true picture and does not tell the full story, as GAN models can be extremely sensitive to the random initialization and small deviations from the best hyperparameter choice. In particular, we demonstrate that the previously used BLEU score is not sensitive to semantic deterioration of generated texts and propose alternative metrics that better capture the quality and diversity of the generated samples. We also conduct a set of experiments comparing a number of GAN models for text with a conventional Language Model (LM) and find that neither of the considered models performs convincingly better than the LM. ", "link": "http://arxiv.org/abs/1806.04936", "authors": [{"name": "Stanislau Semeniuta", "link": "http://arxiv.org/find/cs/1/au:+Semeniuta_S/0/1/0/all/0/1"}, {"name": "Aliaksei Severyn", "link": "http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1"}, {"name": "Sylvain Gelly", "link": "http://arxiv.org/find/cs/1/au:+Gelly_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Only Bayes should learn a manifold (on the estimation of differential geometric structure from data). (arXiv:1806.04994v2 [stat.ML] UPDATED)", "description": "We investigate learning of the differential geometric structure of a data manifold embedded in a high-dimensional Euclidean space. We first analyze kernel-based algorithms and show that under the usual regularizations, non-probabilistic methods cannot recover the differential geometric structure, but instead find mostly linear manifolds or spaces equipped with teleports. To properly learn the differential geometric structure, non-probabilistic methods must apply regularizations that enforce large gradients, which go against common wisdom. We repeat the analysis for probabilistic methods and find that under reasonable priors, the geometric structure can be recovered. Fully exploiting the recovered structure, however, requires the development of stochastic extensions to classic Riemannian geometry. We take early steps in that regard. Finally, we partly extend the analysis to modern models based on neural networks, thereby highlighting geometric and probabilistic shortcomings of current deep generative models. ", "link": "http://arxiv.org/abs/1806.04994", "authors": [{"name": "S&#xf8;ren Hauberg", "link": "http://arxiv.org/find/stat/1/au:+Hauberg_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Multilingual End-to-End Speech Recognition with A Single Transformer on Low-Resource Languages. (arXiv:1806.05059v2 [eess.AS] UPDATED)", "description": "Sequence-to-sequence attention-based models integrate an acoustic, pronunciation and language model into a single neural network, which make them very suitable for multilingual automatic speech recognition (ASR). In this paper, we are concerned with multilingual speech recognition on low-resource languages by a single Transformer, one of sequence-to-sequence attention-based models. Sub-words are employed as the multilingual modeling unit without using any pronunciation lexicon. First, we show that a single multilingual ASR Transformer performs well on low-resource languages despite of some language confusion. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence under the condition of language information being known during training. Experiments on CALLHOME datasets demonstrate that the multilingual ASR Transformer with the language symbol at the end performs better and can obtain relatively 10.5\\% average word error rate (WER) reduction compared to SHL-MLSTM with residual learning. We go on to show that, assuming the language information being known during training and testing, about relatively 12.4\\% average WER reduction can be observed compared to SHL-MLSTM with residual learning through giving the language symbol as the sentence start token. ", "link": "http://arxiv.org/abs/1806.05059", "authors": [{"name": "Shiyu Zhou", "link": "http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1"}, {"name": "Shuang Xu", "link": "http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1"}, {"name": "Bo Xu", "link": "http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "Cross-modal Hallucination for Few-shot Fine-grained Recognition. (arXiv:1806.05147v2 [cs.CV] UPDATED)", "description": "State-of-the-art deep learning algorithms generally require large amounts of data for model training. Lack thereof can severely deteriorate the performance, particularly in scenarios with fine-grained boundaries between categories. To this end, we propose a multimodal approach that facilitates bridging the information gap by means of meaningful joint embeddings. Specifically, we present a benchmark that is multimodal during training (i.e. images and texts) and single-modal in testing time (i.e. images), with the associated task to utilize multimodal data in base classes (with many samples), to learn explicit visual classifiers for novel classes (with few samples). Next, we propose a framework built upon the idea of cross-modal data hallucination. In this regard, we introduce a discriminative text-conditional GAN for sample generation with a simple self-paced strategy for sample selection. We show the results of our proposed discriminative hallucinated method for 1-, 2-, and 5- shot learning on the CUB dataset, where the accuracy is improved by employing multimodal data. ", "link": "http://arxiv.org/abs/1806.05147", "authors": [{"name": "Frederik Pahde", "link": "http://arxiv.org/find/cs/1/au:+Pahde_F/0/1/0/all/0/1"}, {"name": "Patrick J&#xe4;hnichen", "link": "http://arxiv.org/find/cs/1/au:+Jahnichen_P/0/1/0/all/0/1"}, {"name": "Tassilo Klein", "link": "http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1"}, {"name": "Moin Nabi", "link": "http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
{"title": "How Usable are Rust Cryptography APIs?. (arXiv:1806.04929v1 [cs.CR] CROSS LISTED)", "description": "Context: Poor usability of cryptographic APIs is a severe source of vulnerabilities. Aim: We wanted to find out what kind of cryptographic libraries are present in Rust and how usable they are. Method: We explored Rust's cryptographic libraries through a systematic search, conducted an exploratory study on the major libraries and a controlled experiment on two of these libraries with 28 student participants. Results: Only half of the major libraries explicitly focus on usability and misuse resistance, which is reflected in their current APIs. We found that participants were more successful using rust-crypto which we considered less usable than ring before the experiment. Conclusion: We discuss API design insights and make recommendations for the design of crypto libraries in Rust regarding the detail and structure of the documentation, higher-level APIs as wrappers for the existing low-level libraries, and selected, good-quality example code to improve the emerging cryptographic libraries of Rust. ", "link": "http://arxiv.org/abs/1806.04929", "authors": [{"name": "Kai Mindermann", "link": "http://arxiv.org/find/cs/1/au:+Mindermann_K/0/1/0/all/0/1"}, {"name": "Philipp Keck", "link": "http://arxiv.org/find/cs/1/au:+Keck_P/0/1/0/all/0/1"}, {"name": "Stefan Wagner", "link": "http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1"}], "date": "2018-06-14T20:30:00-05:00", "lang": "en-us", "publisher": "www-admin@arxiv.org", "subject": "Computer Science"}
