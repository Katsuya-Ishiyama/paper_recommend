<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>stat updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Statistics (stat) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-10T20:30:00-05:00</dc:date>
<dc:publisher>www-admin@arxiv.org</dc:publisher>
<dc:subject>Statistics</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.03287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1401.5508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1506.05855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1506.06696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1606.00925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1608.05655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1612.03450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1702.04625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1703.00154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1703.04691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1704.05046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1705.01372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1705.01677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1705.08105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1709.02798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1709.05380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1711.01244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1711.03190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1711.04425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1711.07168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1711.11157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1712.03646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1712.03878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1712.10252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1801.04062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1801.06309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1802.00008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1802.03801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1802.05027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1802.06403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1802.06501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1802.08183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1802.09128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1802.09477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1803.00094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1803.03148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1803.06058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1803.08355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1803.09050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1803.09159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1803.09539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1804.02477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1804.03184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1804.07090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1804.07193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1804.08598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1804.08841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1804.09699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1805.01852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1805.04591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1805.07395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1805.08877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1805.11921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.02510" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/1806.02825">
<title>Estimating Train Delays in a Large Rail Network Using a Zero Shot Markov Model. (arXiv:1806.02825v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1806.02825</link>
<description rdf:parseType="Literal">&lt;p&gt;India runs the fourth largest railway transport network size carrying over 8
billion passengers per year. However, the travel experience of passengers is
frequently marked by delays, i.e., late arrival of trains at stations, causing
inconvenience. In a first, we study the systemic delays in train arrivals using
n-order Markov frameworks and experiment with two regression based models.
Using train running-status data collected for two years, we report on an
efficient algorithm for estimating delays at railway stations with near
accurate results. This work can help railways to manage their resources, while
also helping passengers and businesses served by them to efficiently plan their
activities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaurav_R/0/1/0/all/0/1&quot;&gt;Ramashish Gaurav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivastava_B/0/1/0/all/0/1&quot;&gt;Biplav Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02855">
<title>Scalable Natural Gradient Langevin Dynamics in Practice. (arXiv:1806.02855v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02855</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic Gradient Langevin Dynamics (SGLD) is a sampling scheme for
Bayesian modeling adapted to large datasets and models. SGLD relies on the
injection of Gaussian Noise at each step of a Stochastic Gradient Descent (SGD)
update. In this scheme, every component in the noise vector is independent and
has the same scale, whereas the parameters we seek to estimate exhibit strong
variations in scale and significant correlation structures, leading to poor
convergence and mixing times. We compare different preconditioning approaches
to the normalization of the noise vector and benchmark these approaches on the
following criteria: 1) mixing times of the multivariate parameter vector, 2)
regularizing effect on small dataset where it is easy to overfit, 3) covariate
shift detection and 4) resistance to adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacci_H/0/1/0/all/0/1&quot;&gt;Henri Palacci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hess_H/0/1/0/all/0/1&quot;&gt;Henry Hess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02863">
<title>Semi-supervised and Transfer learning approaches for low resource sentiment classification. (arXiv:1806.02863v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1806.02863</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentiment classification involves quantifying the affective reaction of a
human to a document, media item or an event. Although researchers have
investigated several methods to reliably infer sentiment from lexical, speech
and body language cues, training a model with a small set of labeled datasets
is still a challenge. For instance, in expanding sentiment analysis to new
languages and cultures, it may not always be possible to obtain comprehensive
labeled datasets. In this paper, we investigate the application of
semi-supervised and transfer learning methods to improve performances on low
resource sentiment classification tasks. We experiment with extracting dense
feature representations, pre-training and manifold regularization in enhancing
the performance of sentiment classification systems. Our goal is a coherent
implementation of these methods and we evaluate the gains achieved by these
methods in matched setting involving training and testing on a single corpus
setting as well as two cross corpora settings. In both the cases, our
experiments demonstrate that the proposed methods can significantly enhance the
model performance against a purely supervised approach, particularly in cases
involving a handful of training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1&quot;&gt;Rahul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1&quot;&gt;Saurabh Sahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espy_Wilson_C/0/1/0/all/0/1&quot;&gt;Carol Espy-Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1&quot;&gt;Shrikanth Narayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02865">
<title>Kernel Machines With Missing Responses. (arXiv:1806.02865v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02865</link>
<description rdf:parseType="Literal">&lt;p&gt;Missing responses is a missing data format in which outcomes are not always
observed. In this work we develop kernel machines that can handle missing
responses. First, we propose a kernel machine family that uses mainly the
complete cases. For the quadratic loss, we then propose a family of
doubly-robust kernel machines. The proposed kernel-machine estimators can be
applied to both regression and classification problems. We prove oracle
inequalities for the finite-sample differences between the kernel machine risk
and Bayes risk. We use these oracle inequalities to prove consistency and to
calculate convergence rates. We demonstrate the performance of the two proposed
kernel machine families using both a simulation study and a real-world data
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tiantian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goldberg_Y/0/1/0/all/0/1&quot;&gt;Yair Goldberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02867">
<title>Direct Optimization through $\arg \max$ for Discrete Variational Auto-Encoder. (arXiv:1806.02867v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02867</link>
<description rdf:parseType="Literal">&lt;p&gt;Reparameterization of variational auto-encoders with continuous latent spaces
is an effective method for reducing the variance of their gradient estimates.
However, using the same approach when latent variables are discrete is
problematic, due to the resulting non-differentiable objective. In this work,
we present a direct optimization method that propagates gradients through a
non-differentiable $\arg \max$ prediction operation. We apply this method to
discrete variational auto-encoders, by modeling a discrete random variable by
the $\arg \max$ function of the Gumbel-Max perturbation model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorberbom_G/0/1/0/all/0/1&quot;&gt;Guy Lorberbom&lt;/a&gt; (Technion), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gane_A/0/1/0/all/0/1&quot;&gt;Andreea Gane&lt;/a&gt; (MIT), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi Jaakkola&lt;/a&gt; (MIT), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazan_T/0/1/0/all/0/1&quot;&gt;Tamir Hazan&lt;/a&gt; (Technion)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02878">
<title>Learning Tasks for Multitask Learning: Heterogenous Patient Populations in the ICU. (arXiv:1806.02878v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02878</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning approaches have been effective in predicting adverse
outcomes in different clinical settings. These models are often developed and
evaluated on datasets with heterogeneous patient populations. However, good
predictive performance on the aggregate population does not imply good
performance for specific groups.
&lt;/p&gt;
&lt;p&gt;In this work, we present a two-step framework to 1) learn relevant patient
subgroups, and 2) predict an outcome for separate patient populations in a
multi-task framework, where each population is a separate task. We demonstrate
how to discover relevant groups in an unsupervised way with a
sequence-to-sequence autoencoder. We show that using these groups in a
multi-task framework leads to better predictive performance of in-hospital
mortality both across groups and overall. We also highlight the need for more
granular evaluation of performance when dealing with heterogeneous populations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1&quot;&gt;Harini Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1&quot;&gt;Jen J. Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1&quot;&gt;John Guttag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02887">
<title>Residual Unfairness in Fair Machine Learning from Prejudiced Data. (arXiv:1806.02887v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02887</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work in fairness in machine learning has proposed adjusting for
fairness by equalizing accuracy metrics across groups and has also studied how
datasets affected by historical prejudices may lead to unfair decision
policies. We connect these lines of work and study the residual unfairness that
arises when a fairness-adjusted predictor is not actually fair on the target
population due to systematic censoring of training data by existing biased
policies. This scenario is particularly common in the same applications where
fairness is a concern. We characterize theoretically the impact of such
censoring on standard fairness metrics for binary classifiers and provide
criteria for when residual unfairness may or may not appear. We prove that,
under certain conditions, fairness-adjusted classifiers will in fact induce
residual unfairness that perpetuates the same injustices, against the same
groups, that biased the data to begin with, thus showing that even
state-of-the-art fair machine learning can have a &quot;bias in, bias out&quot; property.
When certain benchmark data is available, we show how sample reweighting can
estimate and adjust fairness metrics while accounting for censoring. We use
this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that
attempting to adjust for fairness perpetuates the same injustices that the
policy is infamous for.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kallus_N/0/1/0/all/0/1&quot;&gt;Nathan Kallus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Angela Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02890">
<title>A Comprehensive Framework for Dynamic Bike Rebalancing in a Large Bike Sharing Network. (arXiv:1806.02890v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/1806.02890</link>
<description rdf:parseType="Literal">&lt;p&gt;Bike sharing is a vital component of a modern multi-modal transportation
system. However, its implementation can lead to bike supply-demand imbalance
due to fluctuating spatial and temporal demands. This study proposes a
comprehensive framework to develop optimal dynamic bike rebalancing strategies
in a large bike sharing network. It consists of three components, including a
station-level pick-up/drop-off prediction model, station clustering model, and
capacitated location-routing optimization model. For the first component, we
propose a powerful deep learning model called graph convolution neural network
model (GCNN) with data-driven graph filter (DDGF), which can automatically
learn the hidden spatial-temporal correlations among stations to provide more
accurate predictions; for the second component, we apply a graph clustering
algorithm labeled the Community Detection algorithm to cluster stations that
locate geographically close to each other and have a small net demand gap;
last, a capacitated location-routing problem (CLRP) is solved to deal with the
combination of two types of decision variables: the locations of bike
distribution centers and the design of distribution routes for each cluster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lei Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02892">
<title>Training Faster by Separating Modes of Variation in Batch-normalized Models. (arXiv:1806.02892v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02892</link>
<description rdf:parseType="Literal">&lt;p&gt;Batch Normalization (BN) is essential to effectively train state-of-the-art
deep Convolutional Neural Networks (CNN). It normalizes inputs to the layers
during training using the statistics of each mini-batch. In this work, we study
BN from the viewpoint of Fisher kernels. We show that assuming samples within a
mini-batch are from the same probability density function, then BN is identical
to the Fisher vector of a Gaussian distribution. That means BN can be explained
in terms of kernels that naturally emerge from the probability density function
of the underlying data distribution. However, given the rectifying
non-linearities employed in CNN architectures, distribution of inputs to the
layers show heavy tail and asymmetric characteristics. Therefore, we propose
approximating underlying data distribution not with one, but a mixture of
Gaussian densities. Deriving Fisher vector for a Gaussian Mixture Model (GMM),
reveals that BN can be improved by independently normalizing with respect to
the statistics of disentangled sub-populations. We refer to our proposed soft
piecewise version of BN as Mixture Normalization (MN). Through extensive set of
experiments on CIFAR-10 and CIFAR-100, we show that MN not only effectively
accelerates training image classification and Generative Adversarial networks,
but also reaches higher quality models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalayeh_M/0/1/0/all/0/1&quot;&gt;Mahdi M. Kalayeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02901">
<title>Probabilistic FastText for Multi-Sense Word Embeddings. (arXiv:1806.02901v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.02901</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Probabilistic FastText, a new model for word embeddings that can
capture multiple word senses, sub-word structure, and uncertainty information.
In particular, we represent each word with a Gaussian mixture density, where
the mean of a mixture component is given by the sum of n-grams. This
representation allows the model to share statistical strength across sub-word
structures (e.g. Latin roots), producing accurate representations of rare,
misspelt, or even unseen words. Moreover, each component of the mixture can
capture a different word sense. Probabilistic FastText outperforms both
FastText, which has no probabilistic model, and dictionary-level probabilistic
embeddings, which do not incorporate subword structures, on several
word-similarity benchmarks, including English RareWord and foreign language
datasets. We also achieve state-of-art performance on benchmarks that measure
ability to discern different meanings. Thus, the proposed model is the first to
achieve multi-sense representations while having enriched semantics on rare
words.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athiwaratkun_B/0/1/0/all/0/1&quot;&gt;Ben Athiwaratkun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02905">
<title>Tensor network factorizations: Relationships between brain structural connectomes and traits. (arXiv:1806.02905v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1806.02905</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced brain imaging techniques make it possible to measure individuals&apos;
structural connectomes in large cohort studies non-invasively. The structural
connectome is initially shaped by genetics and subsequently refined by the
environment. It is extremely interesting to study relationships between
structural connectomes and environment factors or human traits, such as
substance use and cognition. Due to limitations in structural connectome
recovery, previous studies largely focus on functional connectomes. Questions
remain about how well structural connectomes can explain variance in different
human traits. Using a state-of-the-art structural connectome processing
pipeline and a novel dimensionality reduction technique applied to data from
the Human Connectome Project (HCP), we show strong relationships between
structural connectomes and various human traits. Our dimensionality reduction
approach uses a tensor characterization of the connectome and relies on a
generalization of principal components analysis. We analyze over 1100 scans for
1076 subjects from the HCP and the Sherbrooke test-retest data set, as well as
$175$ human traits that measure domains including cognition, substance use,
motor, sensory and emotion. We find that structural connectomes are associated
with many traits. Specifically, fluid intelligence, language comprehension, and
motor skills are associated with increased cortical-cortical brain structural
connectivity, while the use of alcohol, tobacco, and marijuana are associated
with decreased cortical-cortical connectivity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengwu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Allen_G/0/1/0/all/0/1&quot;&gt;Genevera I. Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dunson_D/0/1/0/all/0/1&quot;&gt;David Dunson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02920">
<title>GAIN: Missing Data Imputation using Generative Adversarial Nets. (arXiv:1806.02920v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02920</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel method for imputing missing data by adapting the
well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call
our method Generative Adversarial Imputation Nets (GAIN). The generator (G)
observes some components of a real data vector, imputes the missing components
conditioned on what is actually observed, and outputs a completed vector. The
discriminator (D) then takes a completed vector and attempts to determine which
components were actually observed and which were imputed. To ensure that D
forces G to learn the desired distribution, we provide D with some additional
information in the form of a hint vector. The hint reveals to D partial
information about the missingness of the original sample, which is used by D to
focus its attention on the imputation quality of particular components. This
hint ensures that G does in fact learn to generate according to the true data
distribution. We tested our method on various datasets and found that GAIN
significantly outperforms state-of-the-art imputation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jinsung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordon_J/0/1/0/all/0/1&quot;&gt;James Jordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02922">
<title>Feature selection in functional data classification with recursive maxima hunting. (arXiv:1806.02922v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02922</link>
<description rdf:parseType="Literal">&lt;p&gt;Dimensionality reduction is one of the key issues in the design of effective
machine learning methods for automatic induction. In this work, we introduce
recursive maxima hunting (RMH) for variable selection in classification
problems with functional data. In this context, variable selection techniques
are especially attractive because they reduce the dimensionality, facilitate
the interpretation and can improve the accuracy of the predictive models. The
method, which is a recursive extension of maxima hunting (MH), performs
variable selection by identifying the maxima of a relevance function, which
measures the strength of the correlation of the predictor functional variable
with the class label. At each stage, the information associated with the
selected variable is removed by subtracting the conditional expectation of the
process. The results of an extensive empirical evaluation are used to
illustrate that, in the problems investigated, RMH has comparable or higher
predictive accuracy than the standard dimensionality reduction techniques, such
as PCA and PLS, and state-of-the-art feature selection methods for functional
data, such as maxima hunting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Torrecilla_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; L. Torrecilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suarez_A/0/1/0/all/0/1&quot;&gt;Alberto Su&amp;#xe1;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02924">
<title>On Adversarial Risk and Training. (arXiv:1806.02924v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02924</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we formally define the notions of adversarial perturbations,
adversarial risk and adversarial training and analyze their properties. Our
analysis provides several interesting insights into adversarial risk,
adversarial training, and their relation to the classification risk,
&quot;traditional&quot; training. We also show that adversarial training can result in
models with better classification accuracy and can result in better explainable
models than traditional training. Although adversarial training is
computationally expensive, our results and insights suggest that one should
prefer adversarial training over traditional risk minimization for learning
complex models from data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suggala_A/0/1/0/all/0/1&quot;&gt;Arun Sai Suggala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Prasad_A/0/1/0/all/0/1&quot;&gt;Adarsh Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nagarajan_V/0/1/0/all/0/1&quot;&gt;Vaishnavh Nagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02925">
<title>A Spectral Approach to Gradient Estimation for Implicit Distributions. (arXiv:1806.02925v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02925</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently there have been increasing interests in learning and inference with
implicit distributions (i.e., distributions without tractable densities). To
this end, we develop a gradient estimator for implicit distributions based on
Stein&apos;s identity and a spectral decomposition of kernel operators, where the
eigenfunctions are approximated by the Nystr\&quot;om method. Unlike the previous
works that only provide estimates at the sample points, our approach directly
estimates the gradient function, thus allows for a simple and principled
out-of-sample extension. We provide theoretical results on the error bound of
the estimator and discuss the bias-variance tradeoff in practice. The
effectiveness of our method is demonstrated by applications to gradient-free
Hamiltonian Monte Carlo and variational inference with implicit distributions.
Finally, we discuss the intuition behind the estimator by drawing connections
between the Nystr\&quot;om method and kernel PCA, which indicates that the estimator
can automatically adapt to the geometry of the underlying distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shengyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02927">
<title>Lightweight Stochastic Optimization for Minimizing Finite Sums with Infinite Data. (arXiv:1806.02927v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02927</link>
<description rdf:parseType="Literal">&lt;p&gt;Variance reduction has been commonly used in stochastic optimization. It
relies crucially on the assumption that the data set is finite. However, when
the data are imputed with random noise as in data augmentation, the perturbed
data set be- comes essentially infinite. Recently, the stochastic MISO (S-MISO)
algorithm is introduced to address this expected risk minimization problem.
Though it converges faster than SGD, a significant amount of memory is
required. In this pa- per, we propose two SGD-like algorithms for expected risk
minimization with random perturbation, namely, stochastic sample average
gradient (SSAG) and stochastic SAGA (S-SAGA). The memory cost of SSAG does not
depend on the sample size, while that of S-SAGA is the same as those of
variance reduction methods on un- perturbed data. Theoretical analysis and
experimental results on logistic regression and AUC maximization show that SSAG
has faster convergence rate than SGD with comparable space requirement, while
S-SAGA outperforms S-MISO in terms of both iteration complexity and storage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1&quot;&gt;James T. Kwok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02934">
<title>Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse Annotations. (arXiv:1806.02934v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02934</link>
<description rdf:parseType="Literal">&lt;p&gt;Many structured prediction problems (particularly in vision and language
domains) are ambiguous, with multiple outputs being correct for an input - e.g.
there are many ways of describing an image, multiple ways of translating a
sentence; however, exhaustively annotating the applicability of all possible
outputs is intractable due to exponentially large output spaces (e.g. all
English sentences). In practice, these problems are cast as multi-class
prediction, with the likelihood of only a sparse set of annotations being
maximized - unfortunately penalizing for placing beliefs on plausible but
unannotated outputs. We make and test the following hypothesis - for a given
input, the annotations of its neighbors may serve as an additional supervisory
signal. Specifically, we propose an objective that transfers supervision from
neighboring examples. We first study the properties of our developed method in
a controlled toy setup before reporting results on multi-label classification
and two image-grounded sequence modeling tasks - captioning and question
generation. We evaluate using standard task-specific metrics and measures of
output diversity, finding consistent improvements over standard maximum
likelihood training and other baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kalyan_A/0/1/0/all/0/1&quot;&gt;Ashwin Kalyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Stefan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Anitha Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02935">
<title>Causal effects based on distributional distances. (arXiv:1806.02935v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02935</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a novel framework for estimating causal effects based on the
discrepancy between unobserved counterfactual distributions. In our setting a
causal effect is defined in terms of the $L_1$ distance between different
counterfactual outcome distributions, rather than a mean difference in outcome
values. Directly comparing counterfactual outcome distributions can provide
more nuanced and valuable information about causality than a simple comparison
of means. We consider single- and multi-source randomized studies, as well as
observational studies, and analyze error bounds and asymptotic properties of
the proposed estimators. We further propose methods to construct confidence
intervals for the unknown mean distribution distance. Finally, we illustrate
the new methods and verify their effectiveness in empirical studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwangho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kennedy_E/0/1/0/all/0/1&quot;&gt;Edward H. Kennedy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02942">
<title>SupportNet: solving catastrophic forgetting in class incremental learning with support data. (arXiv:1806.02942v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.02942</link>
<description rdf:parseType="Literal">&lt;p&gt;A plain well-trained deep learning model often does not have the ability to
learn new knowledge without forgetting the previously learned knowledge, which
is known as the catastrophic forgetting. Here we propose a novel method,
SupportNet, to solve the catastrophic forgetting problem in class incremental
learning scenario efficiently and effectively. SupportNet combines the strength
of deep learning and support vector machine (SVM), where SVM is used to
identify the support data from the old data, which are fed to the deep learning
model together with the new data for further training so that the model can
review the essential information of the old data when learning the new
information. Two powerful consolidation regularizers are applied to ensure the
robustness of the learned model. Comprehensive experiments on various tasks,
including enzyme function prediction, subcellular structure classification and
breast tumor classification, show that SupportNet drastically outperforms the
state-of-the-art incremental learning methods and even reaches similar
performance as the deep learning model trained from scratch on both old and new
data. Our program is accessible at: https://github.com/lykaust15/SupportNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhongxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Lizhong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Peng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuhui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02954">
<title>Using Social Network Information in Bayesian Truth Discovery. (arXiv:1806.02954v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1806.02954</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of truth discovery based on opinions from multiple
agents who may be unreliable or biased. We consider the case where agents&apos;
reliabilities or biases are correlated if they belong to the same community,
which defines a group of agents with similar opinions regarding a particular
event. An agent can belong to different communities for different events, and
these communities are unknown \emph{a priori}. We incorporate knowledge of the
agents&apos; social network in our truth discovery framework and develop Laplace
variational inference methods to estimate agents&apos; reliabilities, communities,
and the event states. We also develop a stochastic variational inference method
to scale our model to large social networks. Simulations and experiments on
real data suggest that when observations are sparse, our proposed methods
perform better than several other inference methods, including majority voting,
the popular Bayesian Classifier Combination (BCC) method, and the Community BCC
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jielong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junshan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_W/0/1/0/all/0/1&quot;&gt;Wee Peng Tay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02957">
<title>A Deep Neural Network Surrogate for High-Dimensional Random Partial Differential Equations. (arXiv:1806.02957v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02957</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing efficient numerical algorithms for high dimensional random Partial
Differential Equations (PDEs) has been a challenging task due to the well-known
curse of dimensionality problem. We present a new framework for solving
high-dimensional PDEs characterized by random parameters based on a deep
learning approach. The random PDE is approximated by a feed-forward
fully-connected deep neural network, with either strong or weak enforcement of
initial and boundary constraints. The framework is mesh-free, and can handle
irregular computational domains. Parameters of the approximating deep neural
network are determined iteratively using variants of the Stochastic Gradient
Descent (SGD) algorithm, which removes the memory issues that some existing
algorithms for random PDEs are currently experiencing. The performance of the
proposed framework in accurate estimation of solution to random PDEs is
examined by implementing it for diffusion and heat conduction problems. Results
are compared with the sampling-based finite element results, and suggest that
the proposed framework achieves satisfactory accuracy and can handle
high-dimensional random PDEs. A discussion on the advantages of the proposed
method is also provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabian_M/0/1/0/all/0/1&quot;&gt;Mohammad Amin Nabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meidani_H/0/1/0/all/0/1&quot;&gt;Hadi Meidani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02958">
<title>The Case for Full-Matrix Adaptive Regularization. (arXiv:1806.02958v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02958</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive regularization methods come in diagonal and full-matrix variants.
However, only the former have enjoyed widespread adoption in training
large-scale deep models. This is due to the computational overhead of
manipulating a full matrix in high dimension. In this paper, we show how to
make full-matrix adaptive regularization practical and useful. We present GGT,
a truly scalable full-matrix adaptive optimizer. At the heart of our algorithm
is an efficient method for computing the inverse square root of a low-rank
matrix. We show that GGT converges to first-order local minima, providing the
first rigorous theoretical analysis of adaptive regularization in non-convex
optimization. In preliminary experiments, GGT trains faster across a variety of
synthetic tasks and standard deep learning benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1&quot;&gt;Naman Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bullins_B/0/1/0/all/0/1&quot;&gt;Brian Bullins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1&quot;&gt;Elad Hazan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Karan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cyril Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02970">
<title>PAC Ranking from Pairwise and Listwise Queries: Lower Bounds and Upper Bounds. (arXiv:1806.02970v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02970</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the adaptively (active) PAC (probably approximately
correct) top-$k$ ranking and total ranking from $l$-wise ($l\geq 2$)
comparisons under the popular multinomial logit (MNL) model. By adaptively
choosing sets to query and observing the noisy output about the most favored
item of each query, we want to design ranking algorithms that recover the
top-$k$ or total ranking using as few queries as possible. For the PAC top-$k$
ranking problem, we prove a lower bound on the sample complexity (aka number of
queries), and propose an algorithm that is sample complexity optimal up to a
$O(\log(k+l)/\log{k})$ factor. When $l=2$ (i.e., pairwise) or $l=O(poly(k))$,
the algorithm matches the lower bound. For the PAC total ranking problem, we
prove a lower bound, and propose an algorithm that matches the lower bound.
When $l=2$, this model reduces to the popular Plackett-Luce (PL) model, and our
results still outperform the state-of-the-art theoretically and numerically. We
also run comparisons of our algorithms with the state-of-the-art on synthesized
data as well as real-world data, and demonstrate the improvement on sample
complexity numerically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1&quot;&gt;Wenbo Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1&quot;&gt;Ness B. Shroff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02977">
<title>Monge beats Bayes: Hardness Results for Adversarial Training. (arXiv:1806.02977v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02977</link>
<description rdf:parseType="Literal">&lt;p&gt;The last few years have seen extensive empirical study of the robustness of
neural networks, with a concerning conclusion: several state-of-the-art
approaches are highly sensitive to adversarial perturbations of their inputs.
There has been an accompanying surge of interest in learning including defense
mechanisms against specific adversaries, known as adversarial training. Despite
some impressive advances, little remains known on how to best frame a
resource-bounded adversary so that it can be severely detrimental to learning,
a non-trivial problem which entails at a minimum the choice of loss and
classifiers.
&lt;/p&gt;
&lt;p&gt;We suggest here a formal answer to this question, and pin down a simple
sufficient property for any given class of adversaries to be detrimental to
learning. This property involves a central measure of &quot;harmfulness&quot; which
generalizes the well-known class of integral probability metrics, and thus the
maximum mean discrepancy. A key feature of our result is that it holds for all
proper losses, and for a popular subset of these, the optimisation of this
central measure appears to be \textit{independent of the loss}.
&lt;/p&gt;
&lt;p&gt;We then deliver a sufficient condition for this sufficient property to hold
for Lipschitz classifiers, which relies on framing it into optimal transport
theory. We finally deliver a negative boosting result which shows how weakly
contractive adversaries for a RKHS can be combined to build a maximally
detrimental adversary, show that some implemented existing adversaries involve
proxies of our optimal transport adversaries and finally provide a toy
experiment assessing such adversaries in a simple context, displaying that
additional robustness on testing can be granted through adversarial training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cranko_Z/0/1/0/all/0/1&quot;&gt;Zac Cranko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1&quot;&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1&quot;&gt;Richard Nock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1&quot;&gt;Cheng-Soon Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1&quot;&gt;Christian Walder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02978">
<title>JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets. (arXiv:1806.02978v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02978</link>
<description rdf:parseType="Literal">&lt;p&gt;A new generative adversarial network is developed for joint distribution
matching. Distinct from most existing approaches, that only learn conditional
distributions, the proposed model aims to learn a joint distribution of
multiple random variables (domains). This is achieved by learning to sample
from conditional distributions between the domains, while simultaneously
learning to sample from the marginals of each individual domain. The proposed
framework consists of multiple generators and a single softmax-based critic,
all jointly trained via adversarial learning. From a simple noise source, the
proposed framework allows synthesis of draws from the marginals, conditional
draws given observations from a subset of random variables, or complete draws
from the full joint distribution. Most examples considered are for joint
analysis of two domains, with examples for three domains also presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yunchen Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1&quot;&gt;Shuyang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1&quot;&gt;Zhe Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yizhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1&quot;&gt;Ricardo Henao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02988">
<title>Towards Binary-Valued Gates for Robust LSTM Training. (arXiv:1806.02988v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02988</link>
<description rdf:parseType="Literal">&lt;p&gt;Long Short-Term Memory (LSTM) is one of the most widely used recurrent
structures in sequence modeling. It aims to use gates to control information
flow (e.g., whether to skip some information or not) in the recurrent
computations, although its practical implementation based on soft gates only
partially achieves this goal. In this paper, we propose a new way for LSTM
training, which pushes the output values of the gates towards 0 or 1. By doing
so, we can better control the information flow: the gates are mostly open or
closed, instead of in a middle state, which makes the results more
interpretable. Empirical studies show that (1) Although it seems that we
restrict the model capacity, there is no performance drop: we achieve better or
comparable performances due to its better generalization ability; (2) The
outputs of gates are not sensitive to their inputs: we can easily compress the
LSTM unit in multiple ways, e.g., low-rank approximation and low-precision
approximation. The compressed models are even better than the baseline models
without compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1&quot;&gt;Di He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1&quot;&gt;Fei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02997">
<title>q-Space Novelty Detection with Variational Autoencoders. (arXiv:1806.02997v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02997</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning, novelty detection is the task of identifying novel
unseen data. During training, only samples from the normal class are available.
Test samples are classified as normal or abnormal by assignment of a novelty
score. Here we propose novelty detection methods based on training variational
autoencoders (VAEs) on normal data. Since abnormal samples are not used during
training, we define novelty metrics based on the (partially complementary)
assumptions that the VAE is less capable of reconstructing abnormal samples
well; that abnormal samples more strongly violate the VAE regularizer; and that
abnormal samples differ from normal samples not only in input-feature space,
but also in the VAE latent space and VAE output. These approaches, combined
with various possibilities of using (e.g.~sampling) the probabilistic VAE to
obtain scalar novelty scores, yield a large family of methods. We apply these
methods to magnetic resonance imaging, namely to the detection of
diffusion-space (\mbox{q-space}) abnormalities in diffusion MRI scans of
multiple sclerosis patients, i.e.~to detect multiple sclerosis lesions without
using any lesion labels for training. Many of our methods outperform previously
proposed q-space novelty detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vasilev_A/0/1/0/all/0/1&quot;&gt;Aleksei Vasilev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Golkov_V/0/1/0/all/0/1&quot;&gt;Vladimir Golkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lipp_I/0/1/0/all/0/1&quot;&gt;Ilona Lipp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sgarlata_E/0/1/0/all/0/1&quot;&gt;Eleonora Sgarlata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tomassini_V/0/1/0/all/0/1&quot;&gt;Valentina Tomassini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jones_D/0/1/0/all/0/1&quot;&gt;Derek K. Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03000">
<title>Noise-adding Methods of Saliency Map as Series of Higher Order Partial Derivative. (arXiv:1806.03000v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03000</link>
<description rdf:parseType="Literal">&lt;p&gt;SmoothGrad and VarGrad are techniques that enhance the empirical quality of
standard saliency maps by adding noise to input. However, there were few works
that provide a rigorous theoretical interpretation of those methods. We
analytically formalize the result of these noise-adding methods. As a result,
we observe two interesting results from the existing noise-adding methods.
First, SmoothGrad does not make the gradient of the score function smooth.
Second, VarGrad is independent of the gradient of the score function. We
believe that our findings provide a clue to reveal the relationship between
local explanation methods of deep neural networks and higher-order partial
derivatives of the score function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Junghoon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1&quot;&gt;Jeongyeol Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1&quot;&gt;Jamyoung Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1&quot;&gt;Seunghyeon Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Beomsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_T/0/1/0/all/0/1&quot;&gt;Taegyun Jeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03044">
<title>Investigating the Impact of CNN Depth on Neonatal Seizure Detection Performance. (arXiv:1806.03044v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03044</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents a novel, deep, fully convolutional architecture which is
optimized for the task of EEG-based neonatal seizure detection. Architectures
of different depths were designed and tested; varying network depth impacts
convolutional receptive fields and the corresponding learned feature
complexity. Two deep convolutional networks are compared with a shallow
SVM-based neonatal seizure detector, which relies on the extraction of
hand-crafted features. On a large clinical dataset, of over 800 hours of
multichannel unedited EEG, containing 1389 seizure events, the deep 11-layer
architecture significantly outperforms the shallower architectures, improving
the AUC90 from 82.6% to 86.8%. Combining the end-to-end deep architecture with
the feature-based shallow SVM further improves the AUC90 to 87.6%. The fusion
of classifiers of different depths gives greatly improved performance and
reduced variability, making the combined classifier more clinically reliable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+OShea_A/0/1/0/all/0/1&quot;&gt;Alison O&amp;#x27;Shea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lightbody_G/0/1/0/all/0/1&quot;&gt;Gordon Lightbody&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boylan_G/0/1/0/all/0/1&quot;&gt;Geraldine Boylan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Temko_A/0/1/0/all/0/1&quot;&gt;Andriy Temko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03045">
<title>System Level Framework for Assessing the Accuracy of Neonatal EEG Acquisition. (arXiv:1806.03045v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/1806.03045</link>
<description rdf:parseType="Literal">&lt;p&gt;Significant research has been conducted in recent years to design low-cost
alternatives to the current EEG monitoring systems used in healthcare
facilities. Testing such systems on a vulnerable population such as newborns is
complicated due to ethical and regulatory considerations that slow down the
technical development. This paper presents and validates a method for
quantifying the accuracy of neonatal EEG acquisition systems and electrode
technologies via clinical data simulations that do not require neonatal
participants. The proposed method uses an extensive neonatal EEG database to
simulate analogue signals, which are subsequently passed through electrical
models of the skin-electrode interface, which are developed using wet and dry
EEG electrode designs. The signal losses in the system are quantified at each
stage of the acquisition process for electrode and acquisition board losses.
SNR, correlation and noise values were calculated. The results verify that
low-cost EEG acquisition systems are capable of obtaining clinical grade EEG.
Although dry electrodes result in a significant increase in the skin-electrode
impedance, accurate EEG recordings are still achievable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+OSullivan_M/0/1/0/all/0/1&quot;&gt;Mark O&amp;#x27;Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Popovici_E/0/1/0/all/0/1&quot;&gt;Emanuel Popovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bocchino_A/0/1/0/all/0/1&quot;&gt;Andrea Bocchino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+OMahony_C/0/1/0/all/0/1&quot;&gt;Conor O&amp;#x27;Mahony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Boylan_G/0/1/0/all/0/1&quot;&gt;Geraldine Boylan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Temko_A/0/1/0/all/0/1&quot;&gt;Andriy Temko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03046">
<title>Heart Rate Variability during Periods of Low Blood Pressure as a Predictor of Short-Term Outcome in Preterms. (arXiv:1806.03046v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1806.03046</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient management of low blood pressure (BP) in preterm neonates remains
challenging with a considerable variability in clinical practice. The ability
to assess preterm wellbeing during episodes of low BP will help to decide when
and whether hypotension treatment should be initiated. This work aims to
investigate the relationship between heart rate variability (HRV), BP and the
short-term neurological outcome in preterm infants less than 32 weeks
gestational age (GA). The predictive power of common HRV features with respect
to the outcome is assessed and shown to improve when HRV is observed during
episodes of low mean arterial pressure (MAP) - with a single best feature
leading to an AUC of 0.87. Combining multiple features with a boosted decision
tree classifier achieves an AUC of 0.97. The work presents a promising step
towards the use of multimodal data in building an objective decision support
tool for clinical prediction of short-term outcome in preterms who suffer
episodes of low BP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Semenova_O/0/1/0/all/0/1&quot;&gt;Oksana Semenova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carra_G/0/1/0/all/0/1&quot;&gt;Giorgia Carra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lightbody_G/0/1/0/all/0/1&quot;&gt;Gordon Lightbody&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boylan_G/0/1/0/all/0/1&quot;&gt;Geraldine Boylan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dempsey_E/0/1/0/all/0/1&quot;&gt;Eugene Dempsey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Temko_A/0/1/0/all/0/1&quot;&gt;Andriy Temko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03047">
<title>On sound-based interpretation of neonatal EEG. (arXiv:1806.03047v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1806.03047</link>
<description rdf:parseType="Literal">&lt;p&gt;Significant training is required to visually interpret neonatal EEG signals.
This study explores alternative sound-based methods for EEG interpretation
which are designed to allow for intuitive and quick differentiation between
healthy background activity and abnormal activity such as seizures. A novel
method based on frequency and amplitude modulation (FM/AM) is presented. The
algorithm is tuned to facilitate the audio domain perception of rhythmic
activity which is specific to neonatal seizures. The method is compared with
the previously developed phase vocoder algorithm for different time compressing
factors. A survey is conducted amongst a cohort of non-EEG experts to
quantitatively and qualitatively examine the performance of sound-based methods
in comparison with the visual interpretation. It is shown that both
sonification methods perform similarly well, with a smaller inter-observer
variability in comparison with visual. A post-survey analysis of results is
performed by examining the sensitivity of the ear to frequency evolution in
audio.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gomez_S/0/1/0/all/0/1&quot;&gt;Sergi Gomez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+OSullivan_M/0/1/0/all/0/1&quot;&gt;Mark O&amp;#x27;Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Popovici_E/0/1/0/all/0/1&quot;&gt;Emanuel Popovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mathieson_S/0/1/0/all/0/1&quot;&gt;Sean Mathieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Boylan_G/0/1/0/all/0/1&quot;&gt;Geraldine Boylan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Temko_A/0/1/0/all/0/1&quot;&gt;Andriy Temko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03085">
<title>A Stein variational Newton method. (arXiv:1806.03085v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03085</link>
<description rdf:parseType="Literal">&lt;p&gt;Stein variational gradient descent (SVGD) was recently proposed as a general
purpose nonparametric variational inference algorithm [Liu &amp;amp; Wang, NIPS 2016]:
it minimizes the Kullback-Leibler divergence between the target distribution
and its approximation by implementing a form of functional gradient descent on
a reproducing kernel Hilbert space. In this paper, we accelerate and generalize
the SVGD algorithm by including second-order information, thereby approximating
a Newton-like iteration in function space. We also show how second-order
information can lead to more effective choices of kernel. We observe
significant computational gains over the original SVGD algorithm in multiple
test cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Detommaso_G/0/1/0/all/0/1&quot;&gt;Gianluca Detommaso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cui_T/0/1/0/all/0/1&quot;&gt;Tiangang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marzouk_Y/0/1/0/all/0/1&quot;&gt;Youssef Marzouk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scheichl_R/0/1/0/all/0/1&quot;&gt;Robert Scheichl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spantini_A/0/1/0/all/0/1&quot;&gt;Alessio Spantini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03087">
<title>Estimation of marginal model with subgroup auxiliary information. (arXiv:1806.03087v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1806.03087</link>
<description rdf:parseType="Literal">&lt;p&gt;Marginal model is a popular instrument for studying longitudinal data and
cluster data. This paper investigates the estimator of marginal model with
subgroup auxiliary information. To marginal model, we propose a new type of
auxiliary information, and combine them with the traditional estimating
equations of the quadratic inference function (QIF) method based on the
generalized method of moments (GMM). Thus obtaining a more efficient estimator.
The asymptotic normality and the test statistics of the proposed estimator are
established. The theoretical result shows that the estimator with subgroup
information is more efficient than the conventional QIF one. Simulation studies
are carried out to examine the performance of the proposed method under finite
sample. We apply the proposed method to a real data for illustration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jie He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duan_X/0/1/0/all/0/1&quot;&gt;Xiaogang Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shumei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03107">
<title>Temporal Difference Variational Auto-Encoder. (arXiv:1806.03107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03107</link>
<description rdf:parseType="Literal">&lt;p&gt;One motivation for learning generative models of environments is to use them
as simulators for model-based reinforcement learning. Yet, it is intuitively
clear that when time horizons are long, rolling out single step transitions is
inefficient and often prohibitive. In this paper, we propose a generative model
that learns state representations containing explicit beliefs about states
several time steps in the future and that can be rolled out directly in these
states without executing single step transitions. The model is trained on pairs
of temporally separated time points, using an analogue of temporal difference
learning used in reinforcement learning, taking the belief about possible
futures at one time point as a bootstrap for training the belief at an earlier
time. While we focus purely on the study of the model rather than its use in
reinforcement learning, the model architecture we design respects agents&apos;
constraints as it builds the representation online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregor_K/0/1/0/all/0/1&quot;&gt;Karol Gregor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Besse_F/0/1/0/all/0/1&quot;&gt;Frederic Besse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03120">
<title>Variational inference for sparse network reconstruction from count data. (arXiv:1806.03120v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1806.03120</link>
<description rdf:parseType="Literal">&lt;p&gt;In multivariate statistics, the question of finding direct interactions can
be formulated as a problem of network inference - or network reconstruction -
for which the Gaussian graphical model (GGM) provides a canonical framework.
Unfortunately, the Gaussian assumption does not apply to count data which are
encountered in domains such as genomics, social sciences or ecology.
&lt;/p&gt;
&lt;p&gt;To circumvent this limitation, state-of-the-art approaches use two-step
strategies that first transform counts to pseudo Gaussian observations and then
apply a (partial) correlation-based approach from the abundant literature of
GGM inference. We adopt a different stance by relying on a latent model where
we directly model counts by means of Poisson distributions that are conditional
to latent (hidden) Gaussian correlated variables. In this multivariate Poisson
lognormal-model, the dependency structure is completely captured by the latent
layer. This parametric model enables to account for the effects of covariates
on the counts.
&lt;/p&gt;
&lt;p&gt;To perform network inference, we add a sparsity inducing constraint on the
inverse covariance matrix of the latent Gaussian vector. Unlike the usual
Gaussian setting, the penalized likelihood is generally not tractable, and we
resort instead to a variational approach for approximate likelihood
maximization. The corresponding optimization problem is solved by alternating a
gradient ascent on the variational parameters and a graphical-Lasso step on the
covariance matrix.
&lt;/p&gt;
&lt;p&gt;We show that our approach is highly competitive with the existing methods on
simulation inspired from microbiological data. We then illustrate on three
various data sets how accounting for sampling efforts via offsets and
integrating external covariates (which is mostly never done in the existing
literature) drastically changes the topology of the inferred network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chiquet_J/0/1/0/all/0/1&quot;&gt;Julien Chiquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mariadassou_M/0/1/0/all/0/1&quot;&gt;Mahendra Mariadassou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robin_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Robin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03121">
<title>Machine Learning CICY Threefolds. (arXiv:1806.03121v1 [hep-th])</title>
<link>http://arxiv.org/abs/1806.03121</link>
<description rdf:parseType="Literal">&lt;p&gt;The latest techniques from Neural Networks and Support Vector Machines (SVM)
are used to investigate geometric properties of Complete Intersection
Calabi-Yau (CICY) threefolds, a class of manifolds that facilitate string model
building. An advanced neural network classifier and SVM are employed to (1)
learn Hodge numbers and report a remarkable improvement over previous efforts,
(2) query for favourability, and (3) predict discrete symmetries, a highly
imbalanced problem to which the Synthetic Minority Oversampling Technique
(SMOTE) is applied to boost performance. In each case study, we employ a
genetic algorithm to optimise the hyperparameters of the neural network. We
demonstrate that our approach provides quick diagnostic tools capable of
shortlisting quasi-realistic string models based on compactification over
smooth CICYs and further supports the paradigm that classes of problems in
algebraic geometry can be machine learned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Bull_K/0/1/0/all/0/1&quot;&gt;Kieran Bull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yang-Hui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Jejjala_V/0/1/0/all/0/1&quot;&gt;Vishnu Jejjala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Mishra_C/0/1/0/all/0/1&quot;&gt;Challenger Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03125">
<title>Text Classification based on Word Subspace with Term-Frequency. (arXiv:1806.03125v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03125</link>
<description rdf:parseType="Literal">&lt;p&gt;Text classification has become indispensable due to the rapid increase of
text in digital form. Over the past three decades, efforts have been made to
approach this task using various learning algorithms and statistical models
based on bag-of-words (BOW) features. Despite its simple implementation, BOW
features lack semantic meaning representation. To solve this problem, neural
networks started to be employed to learn word vectors, such as the word2vec.
Word2vec embeds word semantic structure into vectors, where the angle between
vectors indicates the meaningful similarity between words. To measure the
similarity between texts, we propose the novel concept of word subspace, which
can represent the intrinsic variability of features in a set of word vectors.
Through this concept, it is possible to model text from word vectors while
holding semantic information. To incorporate the word frequency directly in the
subspace model, we further extend the word subspace to the term-frequency (TF)
weighted word subspace. Based on these new concepts, text classification can be
performed under the mutual subspace method (MSM) framework. The validity of our
modeling is shown through experiments on the Reuters text database, comparing
the results to various state-of-art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shimomoto_E/0/1/0/all/0/1&quot;&gt;Erica K. Shimomoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Souza_L/0/1/0/all/0/1&quot;&gt;Lincon S. Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gatto_B/0/1/0/all/0/1&quot;&gt;Bernardo B. Gatto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fukui_K/0/1/0/all/0/1&quot;&gt;Kazuhiro Fukui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03135">
<title>Semi-parametric estimation of the variogram of a Gaussian process with stationary increments. (arXiv:1806.03135v1 [math.ST])</title>
<link>http://arxiv.org/abs/1806.03135</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the semi-parametric estimation of a scale parameter of a
one-dimensional Gaussian process with known smoothness. We suggest an estimator
based on quadratic variations and on the moment method. We provide asymptotic
approximations of the mean and variance of this estimator, together with
asymptotic normality results, for a large class of Gaussian processes. We allow
for general mean functions and study the aggregation of several estimators
based on various variation sequences. In extensive simulation studies, we show
that the asymptotic results accurately depict thefinite-sample situations
already for small to moderate sample sizes. We also compare various variation
sequences and highlight the efficiency of the aggregation procedure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Azais_J/0/1/0/all/0/1&quot;&gt;Jean-Marc Aza&amp;#xef;s&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bachoc_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Bachoc&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Klein_T/0/1/0/all/0/1&quot;&gt;Thierry Klein&lt;/a&gt; (IMT, ENAC), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lagnoux_A/0/1/0/all/0/1&quot;&gt;Agn&amp;#xe8;s Lagnoux&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thi Mong Ngoc Nguyen&lt;/a&gt; (VNU-HCM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03143">
<title>Black Box FDR. (arXiv:1806.03143v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03143</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing large-scale, multi-experiment studies requires scientists to test
each experimental outcome for statistical significance and then assess the
results as a whole. We present Black Box FDR (BB-FDR), an empirical-Bayes
method for analyzing multi-experiment studies when many covariates are gathered
per experiment. BB-FDR learns a series of black box predictive models to boost
power and control the false discovery rate (FDR) at two stages of study
analysis. In Stage 1, it uses a deep neural network prior to report which
experiments yielded significant outcomes. In Stage 2, a separate black box
model of each covariate is used to select features that have significant
predictive power across all experiments. In benchmarks, BB-FDR outperforms
competing state-of-the-art methods in both stages of analysis. We apply BB-FDR
to two real studies on cancer drug efficacy. For both studies, BB-FDR increases
the proportion of significant outcomes discovered and selects variables that
reveal key genomic drivers of drug sensitivity and resistance in cancer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tansey_W/0/1/0/all/0/1&quot;&gt;Wesley Tansey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rabadan_R/0/1/0/all/0/1&quot;&gt;Raul Rabadan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03145">
<title>Fidelity-based Probabilistic Q-learning for Control of Quantum Systems. (arXiv:1806.03145v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03145</link>
<description rdf:parseType="Literal">&lt;p&gt;The balance between exploration and exploitation is a key problem for
reinforcement learning methods, especially for Q-learning. In this paper, a
fidelity-based probabilistic Q-learning (FPQL) approach is presented to
naturally solve this problem and applied for learning control of quantum
systems. In this approach, fidelity is adopted to help direct the learning
process and the probability of each action to be selected at a certain state is
updated iteratively along with the learning process, which leads to a natural
exploration strategy instead of a pointed one with configured parameters. A
probabilistic Q-learning (PQL) algorithm is first presented to demonstrate the
basic idea of probabilistic action selection. Then the FPQL algorithm is
presented for learning control of quantum systems. Two examples (a spin- 1/2
system and a lamda-type atomic system) are demonstrated to test the performance
of the FPQL algorithm. The results show that FPQL algorithms attain a better
balance between exploration and exploitation, and can also avoid local optimal
policies and accelerate the learning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chunlin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1&quot;&gt;Daoyi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han-Xiong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1&quot;&gt;Jian Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarn_T/0/1/0/all/0/1&quot;&gt;Tzyh-Jong Tarn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03146">
<title>Neural Message Passing with Edge Updates for Predicting Properties of Molecules and Materials. (arXiv:1806.03146v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03146</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural message passing on molecular graphs is one of the most promising
methods for predicting formation energy and other properties of molecules and
materials. In this work we extend the neural message passing model with an edge
update network which allows the information exchanged between atoms to depend
on the hidden state of the receiving atom. We benchmark the proposed model on
three publicly available datasets (QM9, The Materials Project and OQMD) and
show that the proposed model yields superior prediction of formation energies
and other properties on all three datasets in comparison with the best
published results. Furthermore we investigate different methods for
constructing the graph used to represent crystalline structures and we find
that using a graph based on K-nearest neighbors achieves better prediction
accuracy than using maximum distance cutoff or the Voronoi tessellation graph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jorgensen_P/0/1/0/all/0/1&quot;&gt;Peter Bj&amp;#xf8;rn J&amp;#xf8;rgensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jacobsen_K/0/1/0/all/0/1&quot;&gt;Karsten Wedel Jacobsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1&quot;&gt;Mikkel N. Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03168">
<title>Data-driven Analytics for Business Architectures: Proposed Use of Graph Theory. (arXiv:1806.03168v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1806.03168</link>
<description rdf:parseType="Literal">&lt;p&gt;Business Architecture (BA) plays a significant role in helping organizations
understand enterprise structures and processes, and align them with strategic
objectives. However, traditional BAs are represented in fixed structure with
static model elements and fail to dynamically capture business insights based
on internal and external data. To solve this problem, this paper introduces the
graph theory into BAs with aim of building extensible data-driven analytics and
automatically generating business insights. We use IBM&apos;s Component Business
Model (CBM) as an example to illustrate various ways in which graph theory can
be leveraged for data-driven analytics, including what and how business
insights can be obtained. Future directions for applying graph theory to
business architecture analytics are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1&quot;&gt;Guangjie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arar_R/0/1/0/all/0/1&quot;&gt;Raphael Arar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1&quot;&gt;Eric Young Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03182">
<title>Deep learning based inverse method for layout design. (arXiv:1806.03182v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1806.03182</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout design with complex constraints is a challenging problem to solve due
to the non-uniqueness of the solution and the difficulties in incorporating the
constraints into the conventional optimization-based methods. In this paper, we
propose a design method based on the recently developed machine learning
technique, Variational Autoencoder (VAE). We utilize the learning capability of
the VAE to learn the constraints and the generative capability of the VAE to
generate design candidates that automatically satisfy all the constraints. As
such, no constraints need to be imposed during the design stage. In addition,
we show that the VAE network is also capable of learning the underlying physics
of the design problem, leading to an efficient design tool that does not need
any physical simulation once the network is constructed. We demonstrated the
performance of the method on two cases: inverse design of surface diffusion
induced morphology change and mask design for optical microlithography.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yujie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wenjing Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03185">
<title>Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation. (arXiv:1806.03185v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1806.03185</link>
<description rdf:parseType="Literal">&lt;p&gt;Models for audio source separation usually operate on the magnitude spectrum,
which ignores phase information and makes separation performance dependant on
hyper-parameters for the spectral front-end. Therefore, we investigate
end-to-end source separation in the time-domain, which allows modelling phase
information and avoids fixed spectral transformations. Due to high sampling
rates for audio, employing a long temporal input context on the sample level is
difficult, but required for high quality separation results because of
long-range temporal correlations. In this context, we propose the Wave-U-Net,
an adaptation of the U-Net to the one-dimensional time domain, which repeatedly
resamples feature maps to compute and combine features at different time
scales. We introduce further architectural improvements, including an output
layer that enforces source additivity, an upsampling technique and a
context-aware prediction framework to reduce output artifacts. Experiments for
singing voice separation indicate that our architecture yields a performance
comparable to a state-of-the-art spectrogram-based U-Net architecture, given
the same data. Finally, we reveal a problem with outliers in the currently used
SDR evaluation metrics and suggest reporting rank-based statistics to alleviate
this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoller_D/0/1/0/all/0/1&quot;&gt;Daniel Stoller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ewert_S/0/1/0/all/0/1&quot;&gt;Sebastian Ewert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1&quot;&gt;Simon Dixon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03190">
<title>The Well Tempered Lasso. (arXiv:1806.03190v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1806.03190</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the complexity of the entire regularization path for least squares
regression with 1-norm penalty, known as the Lasso. Every regression parameter
in the Lasso changes linearly as a function of the regularization value. The
number of changes is regarded as the Lasso&apos;s complexity. Experimental results
using exact path following exhibit polynomial complexity of the Lasso in the
problem size. Alas, the path complexity of the Lasso on artificially designed
regression problems is exponential.
&lt;/p&gt;
&lt;p&gt;We use smoothed analysis as a mechanism for bridging the gap between worst
case settings and the de facto low complexity. Our analysis assumes that the
observed data has a tiny amount of intrinsic noise. We then prove that the
Lasso&apos;s complexity is polynomial in the problem size. While building upon the
seminal work of Spielman and Teng on smoothed complexity, our analysis is
morally different as it is divorced from specific path following algorithms. We
verify the validity of our analysis in experiments with both worst case
settings and real datasets. The empirical results we obtain closely match our
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singer_Y/0/1/0/all/0/1&quot;&gt;Yoram Singer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03195">
<title>Obtaining fairness using optimal transport theory. (arXiv:1806.03195v1 [math.ST])</title>
<link>http://arxiv.org/abs/1806.03195</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical algorithms are usually helping in making decisions in many
aspects of our lives. But, how do we know if these algorithms are biased and
commit unfair discrimination of a particular group of people, typically a
minority? \textit{Fairness} is generally studied in a probabilistic framework
where it is assumed that there exists a protected variable, whose use as an
input of the algorithm may imply discrimination. There are different
definitions of Fairness in the literature. In this paper we focus on two of
them which are called Disparate Impact (DI) and Balanced Error Rate (BER). Both
are based on the outcome of the algorithm across the different groups
determined by the protected variable. The relationship between these two
notions is also studied. The goals of this paper are to detect when a binary
classification rule lacks fairness and to try to fight against the potential
discrimination attributable to it. This can be done by modifying either the
classifiers or the data itself. Our work falls into the second category and
modifies the input data using optimal transport theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Barrio_E/0/1/0/all/0/1&quot;&gt;Eustasio del Barrio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gamboa_F/0/1/0/all/0/1&quot;&gt;Fabrice Gamboa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gordaliza_P/0/1/0/all/0/1&quot;&gt;Paula Gordaliza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Loubes_J/0/1/0/all/0/1&quot;&gt;Jean-Michel Loubes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03198">
<title>A neural network catalyzer for multi-dimensional similarity search. (arXiv:1806.03198v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03198</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims at learning a function mapping input vectors to an output
space in a way that improves high-dimensional similarity search. As a proxy
objective, we design and train a neural network that favors uniformity in the
spherical output space, while preserving the neighborhood structure after the
mapping. For this purpose, we propose a new regularizer derived from the
Kozachenko-Leonenko differential entropy estimator and combine it with a
locality-aware triplet loss. Our method operates as a catalyzer for traditional
indexing methods such as locality sensitive hashing or iterative quantization,
boosting the overall recall. Additionally, the network output distribution
makes it possible to leverage structured quantizers with efficient algebraic
encoding, in particular spherical lattice quantizers such as the Gosset lattice
E8. Our experiments show that this approach is competitive with
state-of-the-art methods such as optimized product quantization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sablayrolles_A/0/1/0/all/0/1&quot;&gt;Alexandre Sablayrolles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Douze_M/0/1/0/all/0/1&quot;&gt;Matthijs Douze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmid_C/0/1/0/all/0/1&quot;&gt;Cordelia Schmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jegou_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; J&amp;#xe9;gou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03207">
<title>Learning in Integer Latent Variable Models with Nested Automatic Differentiation. (arXiv:1806.03207v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03207</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop nested automatic differentiation (AD) algorithms for exact
inference and learning in integer latent variable models. Recently, Winner,
Sujono, and Sheldon showed how to reduce marginalization in a class of integer
latent variable models to evaluating a probability generating function which
contains many levels of nested high-order derivatives. We contribute faster and
more stable AD algorithms for this challenging problem and a novel algorithm to
compute exact gradients for learning. These contributions lead to significantly
faster and more accurate learning algorithms, and are the first AD algorithms
whose running time is polynomial in the number of levels of nesting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sheldon_D/0/1/0/all/0/1&quot;&gt;Daniel Sheldon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Winner_K/0/1/0/all/0/1&quot;&gt;Kevin Winner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sujono_D/0/1/0/all/0/1&quot;&gt;Debora Sujono&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03208">
<title>Prediction of the FIFA World Cup 2018 - A random forest approach with an emphasis on estimated team ability parameters. (arXiv:1806.03208v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1806.03208</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we compare three different modeling approaches for the scores
of soccer matches with regard to their predictive performances based on all
matches from the four previous FIFA World Cups 2002 - 2014: Poisson regression
models, random forests and ranking methods. While the former two are based on
the teams&apos; covariate information, the latter method estimates adequate ability
parameters that reflect the current strength of the teams best. Within this
comparison the best-performing prediction methods on the training data turn out
to be the ranking methods and the random forests. However, we show that by
combining the random forest with the team ability parameters from the ranking
methods as an additional covariate we can improve the predictive power
substantially. Finally, this combination of methods is chosen as the final
model and based on its estimates, the FIFA World Cup 2018 is simulated
repeatedly and winning probabilities are obtained for all teams. The model
slightly favors Spain before the defending champion Germany. Additionally, we
provide survival probabilities for all teams and at all tournament stages as
well as the most probable tournament outcome.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Groll_A/0/1/0/all/0/1&quot;&gt;Andreas Groll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ley_C/0/1/0/all/0/1&quot;&gt;Christophe Ley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schauberger_G/0/1/0/all/0/1&quot;&gt;Gunther Schauberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eetvelde_H/0/1/0/all/0/1&quot;&gt;Hans Van Eetvelde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03211">
<title>The landscape of NeuroImage-ing research. (arXiv:1806.03211v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1806.03211</link>
<description rdf:parseType="Literal">&lt;p&gt;As the field of neuroimaging grows, it can be difficult for scientists within
the field to gain and maintain a detailed understanding of its ever-changing
landscape. While collaboration and citation networks highlight important
contributions within the field, the roles of and relations among specific areas
of study can remain quite opaque. Here, we apply techniques from network
science to map the landscape of neuroimaging research documented in the journal
NeuroImage over the past decade. We create a network in which nodes represent
research topics, and edges give the degree to which these topics tend to be
covered in tandem. The network displays small-world architecture, with
communities characterized by common imaging modalities and medical
applications, and with bridges that integrate these distinct subfields. Using
node-level analysis, we quantify the structural roles of individual topics
within the neuroimaging landscape, and find high levels of clustering within
the structural MRI subfield as well as increasing participation among topics
related to psychiatry. The overall prevalence of a topic is unrelated to the
prevalence of its neighbors, but the degree to which a topic becomes more or
less popular over time is strongly related to changes in the prevalence of its
neighbors. Broadly, this work presents a cohesive model for understanding the
landscape of neuroimaging research across the field, in broad subfields, and
within specific topic areas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dworkin_J/0/1/0/all/0/1&quot;&gt;Jordan D. Dworkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinohara_R/0/1/0/all/0/1&quot;&gt;Russell T. Shinohara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassett_D/0/1/0/all/0/1&quot;&gt;Danielle S. Bassett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03218">
<title>Data-driven model for the identification of the rock type at a drilling bit. (arXiv:1806.03218v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03218</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to bridge the gap of more than 15m between the drilling bit and
high-fidelity rock type sensors during the directional drilling, we present a
novel approach for identifying rock type at the drilling bit. The approach is
based on application of machine learning techniques for Measurements While
Drilling (MWD) data. We demonstrate capabilities of the developed approach for
distinguishing between the rock types corresponding to (1) a target oil bearing
interval of a reservoir and (2) a non-productive shale layer and compare it to
more traditional physics-driven approaches. The dataset includes MWD data and
lithology mapping along multiple wellbores obtained by processing of Logging
While Drilling (LWD) measurements from a massive drilling effort on one of the
major newly developed oilfield in the North of Western Siberia. We compare
various machine-learning algorithms, examine extra features coming from
physical modeling of drilling mechanics, and show that the classification error
can be reduced from 13.5% to 9%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klyuchnikov_N/0/1/0/all/0/1&quot;&gt;Nikita Klyuchnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1&quot;&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruzdev_A/0/1/0/all/0/1&quot;&gt;Arseniy Gruzdev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovchinnikov_G/0/1/0/all/0/1&quot;&gt;Georgiy Ovchinnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antipova_K/0/1/0/all/0/1&quot;&gt;Ksenia Antipova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ismailova_L/0/1/0/all/0/1&quot;&gt;Leyla Ismailova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muravleva_E/0/1/0/all/0/1&quot;&gt;Ekaterina Muravleva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semenikhin_A/0/1/0/all/0/1&quot;&gt;Artyom Semenikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherepanov_A/0/1/0/all/0/1&quot;&gt;Alexey Cherepanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koryabkin_V/0/1/0/all/0/1&quot;&gt;Vitaliy Koryabkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_I/0/1/0/all/0/1&quot;&gt;Igor Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsurgan_A/0/1/0/all/0/1&quot;&gt;Alexey Tsurgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krasnov_F/0/1/0/all/0/1&quot;&gt;Fedor Krasnov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1&quot;&gt;Dmitry Koroteev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03227">
<title>An Information-Percolation Bound for Spin Synchronization on General Graphs. (arXiv:1806.03227v1 [math.PR])</title>
<link>http://arxiv.org/abs/1806.03227</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the problem of reconstructing $n$ independent uniform
spins $X_1,\dots,X_n$ living on the vertices of an $n$-vertex graph $G$, by
observing their interactions on the edges of the graph. This captures instances
of models such as (i) broadcasting on trees, (ii) block models, (iii)
synchronization on grids, (iv) spiked Wigner models. The paper gives an
upper-bound on the mutual information between two vertices in terms of a bond
percolation estimate. Namely, the information between two vertices&apos; spins is
bounded by the probability that these vertices are connected in a bond
percolation model, where edges are opened with a probability that &quot;emulates&quot;
the edge-information. Both the information and the open-probability are based
on the Chi-squared mutual information. The main results allow us to re-derive
known results for information-theoretic non-reconstruction in models (i)-(iv),
with more direct or improved bounds in some cases, and to obtain new results,
such as for a spiked Wigner model on grids. The main result also implies a new
subadditivity property for the Chi-squared mutual information for symmetric
channels and general graphs, extending the subadditivity property obtained by
Evans-Kenyon-Peres-Schulman [EKPS00] for trees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Abbe_E/0/1/0/all/0/1&quot;&gt;Emmanuel Abbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Boix_E/0/1/0/all/0/1&quot;&gt;Enric Boix&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03232">
<title>Randomized Optimal Transport on a Graph: Framework and New Distance Measures. (arXiv:1806.03232v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1806.03232</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently developed bag-of-paths framework consists in setting a
Gibbs-Boltzmann distribution on all feasible paths of a graph. This probability
distribution favors short paths over long ones, with a free parameter (the
temperature $T &amp;gt; 0$) controlling the entropic level of the distribution. This
formalism enables the computation of new distances or dissimilarities,
interpolating between the shortest-path and the resistance distance, which have
been shown to perform well in clustering and classification tasks. In this
work, the bag-of-paths formalism is extended by adding two independent equality
constraints fixing starting and ending nodes distributions of paths. When the
temperature is low, this formalism is shown to be equivalent to a relaxation of
the optimal transport problem on a network where paths carry a flow between two
discrete distributions on nodes. The randomization is achieved by considering
free energy minimization instead of traditional cost minimization. Algorithms
computing the optimal free energy solution are developed for two types of
paths: hitting (or absorbing) paths and non-hitting, regular paths, and require
the inversion of an $n \times n$ matrix with $n$ being the number of nodes.
Interestingly, for regular paths, the resulting optimal policy interpolates
between the deterministic optimal transport policy ($T \rightarrow 0^{+}$) and
the solution to the corresponding electrical circuit ($T \rightarrow \infty$).
Two distance measures between nodes and a dissimilarity between groups of
nodes, both integrating weights on nodes, are derived from this framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guex_G/0/1/0/all/0/1&quot;&gt;Guillaume Guex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kivimaki_I/0/1/0/all/0/1&quot;&gt;Ilkka Kivim&amp;#xe4;ki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saerens_M/0/1/0/all/0/1&quot;&gt;Marco Saerens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03240">
<title>Measuring Item Similarity in Introductory Programming: Python and Robot Programming Case Studies. (arXiv:1806.03240v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1806.03240</link>
<description rdf:parseType="Literal">&lt;p&gt;A personalized learning system needs a large pool of items for learners to
solve. When working with a large pool of items, it is useful to measure the
similarity of items. We outline a general approach to measuring the similarity
of items and discuss specific measures for items used in introductory
programming. Evaluation of quality of similarity measures is difficult. To this
end, we propose an evaluation approach utilizing three levels of abstraction.
We illustrate our approach to measuring similarity and provide evaluation using
items from three diverse programming environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelanek_R/0/1/0/all/0/1&quot;&gt;Radek Pel&amp;#xe1;nek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Effenberger_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Effenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanek_M/0/1/0/all/0/1&quot;&gt;Mat&amp;#x11b;j Van&amp;#x11b;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sassmann_V/0/1/0/all/0/1&quot;&gt;Vojt&amp;#x11b;ch Sassmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gmiterko_D/0/1/0/all/0/1&quot;&gt;Dominik Gmiterko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03281">
<title>Blind Justice: Fairness with Encrypted Sensitive Attributes. (arXiv:1806.03281v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03281</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has explored how to train machine learning models which do not
discriminate against any subgroup of the population as determined by sensitive
attributes such as gender or race. To avoid disparate treatment, sensitive
attributes should not be considered. On the other hand, in order to avoid
disparate impact, sensitive attributes must be examined, e.g., in order to
learn a fair model, or to check if a given model is fair. We introduce methods
from secure multi-party computation which allow us to avoid both. By encrypting
sensitive attributes, we show how an outcome-based fair model may be learned,
checked, or have its outputs verified and held to account, without users
revealing their sensitive attributes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kilbertus_N/0/1/0/all/0/1&quot;&gt;Niki Kilbertus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gascon_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe0; Gasc&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kusner_M/0/1/0/all/0/1&quot;&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Veale_M/0/1/0/all/0/1&quot;&gt;Michael Veale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gummadi_K/0/1/0/all/0/1&quot;&gt;Krishna P. Gummadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03285">
<title>Pricing Engine: Estimating Causal Impacts in Real World Business Settings. (arXiv:1806.03285v1 [econ.EM])</title>
<link>http://arxiv.org/abs/1806.03285</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Pricing Engine package to enable the use of Double ML
estimation techniques in general panel data settings. Customization allows the
user to specify first-stage models, first-stage featurization, second stage
treatment selection and second stage causal-modeling. We also introduce a
DynamicDML class that allows the user to generate dynamic treatment-aware
forecasts at a range of leads and to understand how the forecasts will vary as
a function of causally estimated treatment parameters. The Pricing Engine is
built on Python 3.5 and can be run on an Azure ML Workbench environment with
the addition of only a few Python packages. This note provides high-level
discussion of the Double ML method, describes the packages intended use and
includes an example Jupyter notebook demonstrating application to some publicly
available data. Installation of the package and additional technical
documentation is available at https://github.com/bquistorff/pricingengine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Goldman_M/0/1/0/all/0/1&quot;&gt;Matt Goldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Quistorff_B/0/1/0/all/0/1&quot;&gt;Brian Quistorff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03286">
<title>Nonparametric Regression with Comparisons: Escaping the Curse of Dimensionality with Ordinal Information. (arXiv:1806.03286v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03286</link>
<description rdf:parseType="Literal">&lt;p&gt;In supervised learning, we leverage a labeled dataset to design methods for
function estimation. In many practical situations, we are able to obtain
alternative feedback, possibly at a low cost. A broad goal is to understand the
usefulness of, and to design algorithms to exploit, this alternative feedback.
We focus on a semi-supervised setting where we obtain additional ordinal (or
comparison) information for potentially unlabeled samples. We consider ordinal
feedback of varying qualities where we have either a perfect ordering of the
samples, a noisy ordering of the samples or noisy pairwise comparisons between
the samples. We provide a precise quantification of the usefulness of these
types of ordinal feedback in non-parametric regression, showing that in many
cases it is possible to accurately estimate an underlying function with a very
small labeled set, effectively escaping the curse of dimensionality. We develop
an algorithm called Ranking-Regression (RR) and analyze its accuracy as a
function of size of the labeled and unlabeled datasets and various noise
parameters. We also present lower bounds, that establish fundamental limits for
the task and show that RR is optimal in a variety of settings. Finally, we
present experiments that show the efficacy of RR and investigate its robustness
to various sources of noise and model-misspecification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yichong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Muthakana_H/0/1/0/all/0/1&quot;&gt;Hariank Muthakana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balakrishnan_S/0/1/0/all/0/1&quot;&gt;Sivaraman Balakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aarti Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubrawski_A/0/1/0/all/0/1&quot;&gt;Artur Dubrawski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03287">
<title>Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware. (arXiv:1806.03287v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03287</link>
<description rdf:parseType="Literal">&lt;p&gt;As Machine Learning (ML) gets applied to security-critical or sensitive
domains, there is a growing need for integrity and privacy guarantees for ML
computations running in untrusted environments. A pragmatic solution comes from
Trusted Execution Environments, which use hardware and software protections to
isolate sensitive computations from the untrusted software stack. However,
these isolation guarantees come at a price in performance, compared to
untrusted alternatives. This paper initiates the study of high performance
execution of Deep Neural Networks (DNNs) in trusted environments by efficiently
partitioning computations between trusted and untrusted devices. Building upon
a simple secure outsourcing scheme for matrix multiplication, we propose
Slalom, a framework that outsources execution of all linear layers in a DNN
from any trusted environment (e.g., SGX, TrustZone or Sanctum) to a faster
co-located device. We evaluate Slalom by executing DNNs in an Intel SGX
enclave, which selectively outsources work to an untrusted GPU. For two
canonical DNNs, VGG16 and MobileNet, we obtain 20x and 6x increases in
throughput for verifiable inference, and 10x and 3.5x for verifiable and
private inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tramer_F/0/1/0/all/0/1&quot;&gt;Florian Tramer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boneh_D/0/1/0/all/0/1&quot;&gt;Dan Boneh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1401.5508">
<title>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression. (arXiv:1401.5508v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1401.5508</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel scheme for reduced-rank Gaussian process
regression. The method is based on an approximate series expansion of the
covariance function in terms of an eigenfunction expansion of the Laplace
operator in a compact subset of $\mathbb{R}^d$. On this approximate eigenbasis
the eigenvalues of the covariance function can be expressed as simple functions
of the spectral density of the Gaussian process, which allows the GP inference
to be solved under a computational cost scaling as $\mathcal{O}(nm^2)$
(initial) and $\mathcal{O}(m^3)$ (hyperparameter learning) with $m$ basis
functions and $n$ data points. The approach also allows for rigorous error
analysis with Hilbert space theory, and we show that the approximation becomes
exact when the size of the compact subset and the number of eigenfunctions go
to infinity. The expansion generalizes to Hilbert spaces with an inner product
which is defined as an integral over a specified input density. The method is
compared to previously proposed methods theoretically and through empirical
tests with simulated and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarkka_S/0/1/0/all/0/1&quot;&gt;Simo S&amp;#xe4;rkk&amp;#xe4;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1506.05855">
<title>Information-based inference for singular models and finite sample sizes: A frequentist information criterion. (arXiv:1506.05855v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1506.05855</link>
<description rdf:parseType="Literal">&lt;p&gt;In the information-based paradigm of inference, model selection is performed
by selecting the candidate model with the best estimated predictive
performance. The success of this approach depends on the accuracy of the
estimate of the predictive complexity. In the large-sample-size limit of a
regular model, the predictive performance is well estimated by the Akaike
Information Criterion (AIC). However, this approximation can either
significantly under or over-estimating the complexity in a wide range of
important applications where models are either non-regular or
finite-sample-size corrections are significant. We introduce an improved
approximation for the complexity that is used to define a new information
criterion: the Frequentist Information Criterion (QIC). QIC extends the
applicability of information-based inference to the finite-sample-size regime
of regular models and to singular models. We demonstrate the power and the
comparative advantage of QIC in a number of example analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+LaMont_C/0/1/0/all/0/1&quot;&gt;Colin H. LaMont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiggins_P/0/1/0/all/0/1&quot;&gt;Paul A. Wiggins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1506.06696">
<title>A Theoretical and Empirical Comparison of the Temporal Exponential Random Graph Model and the Stochastic Actor-Oriented Model. (arXiv:1506.06696v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1506.06696</link>
<description rdf:parseType="Literal">&lt;p&gt;The temporal exponential random graph model (TERGM) and the stochastic
actor-oriented model (SAOM, e.g., SIENA) are popular models for longitudinal
network analysis. We compare these models theoretically, via simulation, and
through a real-data example in order to assess their relative strengths and
weaknesses. Though we do not aim to make a general claim about either being
superior to the other across all specifications, we highlight several
theoretical differences the analyst might consider and find that with some
specifications, the two models behave very similarly, while each model
out-predicts the other one the more the specific assumptions of the respective
model are met.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leifeld_P/0/1/0/all/0/1&quot;&gt;Philip Leifeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cranmer_S/0/1/0/all/0/1&quot;&gt;Skyler J. Cranmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.00925">
<title>Convolutional Imputation of Matrix Networks. (arXiv:1606.00925v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1606.00925</link>
<description rdf:parseType="Literal">&lt;p&gt;A matrix network is a family of matrices, with relatedness modeled by a
weighted graph. We consider the task of completing a partially observed matrix
network. We assume a novel sampling scheme where a fraction of matrices might
be completely unobserved. How can we recover the entire matrix network from
incomplete observations? This mathematical problem arises in many applications
including medical imaging and social networks.
&lt;/p&gt;
&lt;p&gt;To recover the matrix network, we propose a structural assumption that the
matrices have a graph Fourier transform which is low-rank. We formulate a
convex optimization problem and prove an exact recovery guarantee for the
optimization problem. Furthermore, we numerically characterize the exact
recovery regime for varying rank and sampling rate and discover a new phase
transition phenomenon. Then we give an iterative imputation algorithm to
efficiently solve the optimization problem and complete large scale matrix
networks. We demonstrate the algorithm with a variety of applications such as
MRI and Facebook user network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qingyun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donoho_M/0/1/0/all/0/1&quot;&gt;Mengyuan Yan David Donoho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyd_S/0/1/0/all/0/1&quot;&gt;Stephen Boyd&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.05655">
<title>Nonstationary Spatial Prediction of Soil Organic Carbon: Implications for Stock Assessment Decision Making. (arXiv:1608.05655v5 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1608.05655</link>
<description rdf:parseType="Literal">&lt;p&gt;The Rapid Carbon Assessment (RaCA) project was conducted by the US Department
of Agriculture&apos;s National Resources Conservation Service between 2010-2012 in
order to provide contemporaneous measurements of soil organic carbon (SOC)
across the US. Despite the broad extent of the RaCA data collection effort,
direct observations of SOC are not available at the high spatial resolution
needed for studying carbon storage in soil and its implications for important
problems in climate science and agriculture. As a result, there is a need for
predicting SOC at spatial locations not included as part of the RaCA project.
In this paper, we compare spatial prediction of SOC using a subset of the RaCA
data for a variety of statistical methods. We investigate the performance of
methods with off-the-shelf software available (both stationary and
nonstationary) as well as a novel nonstationary approach based on partitioning
relevant spatially-varying covariate processes. Our new method addresses open
questions regarding (1) how to partition the spatial domain for
segmentation-based nonstationary methods, (2) incorporating partially observed
covariates into a spatial model, and (3) accounting for uncertainty in the
partitioning. In applying the various statistical methods we find that there
are minimal differences in out-of-sample criteria for this particular data set,
however, there are major differences in maps of uncertainty in SOC predictions.
We argue that the spatially-varying measures of prediction uncertainty produced
by our new approach are valuable to decision makers, as they can be used to
better benchmark mechanistic models, identify target areas for soil restoration
projects, and inform carbon sequestration projects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Risser_M/0/1/0/all/0/1&quot;&gt;Mark D. Risser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Calder_C/0/1/0/all/0/1&quot;&gt;Catherine A. Calder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berrocal_V/0/1/0/all/0/1&quot;&gt;Veronica J. Berrocal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berrett_C/0/1/0/all/0/1&quot;&gt;Candace Berrett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.03450">
<title>Noisy subspace clustering via matching pursuits. (arXiv:1612.03450v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1612.03450</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparsity-based subspace clustering algorithms have attracted significant
attention thanks to their excellent performance in practical applications. A
prominent example is the sparse subspace clustering (SSC) algorithm by
Elhamifar and Vidal, which performs spectral clustering based on an adjacency
matrix obtained by sparsely representing each data point in terms of all the
other data points via the Lasso. When the number of data points is large or the
dimension of the ambient space is high, the computational complexity of SSC
quickly becomes prohibitive. Dyer et al. observed that SSC-OMP obtained by
replacing the Lasso by the greedy orthogonal matching pursuit (OMP) algorithm
results in significantly lower computational complexity, while often yielding
comparable performance. The central goal of this paper is an analytical
performance characterization of SSC-OMP for noisy data. Moreover, we introduce
and analyze the SSC-MP algorithm, which employs matching pursuit (MP) in lieu
of OMP. Both SSC-OMP and SSC-MP are proven to succeed even when the subspaces
intersect and when the data points are contaminated by severe noise. The
clustering conditions we obtain for SSC-OMP and SSC-MP are similar to those for
SSC and for the thresholding-based subspace clustering (TSC) algorithm due to
Heckel and B\&quot;olcskei. Analytical results in combination with numerical results
indicate that both SSC-OMP and SSC-MP with a data-dependent stopping criterion
automatically detect the dimensions of the subspaces underlying the data.
Moreover, experiments on synthetic and on real data show that SSC-MP compares
very favorably to SSC, SSC-OMP, TSC, and the nearest subspace neighbor
algorithm, both in terms of clustering performance and running time. In
addition, we find that, in contrast to SSC-OMP, the performance of SSC-MP is
very robust with respect to the choice of parameters in the stopping criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1&quot;&gt;Michael Tschannen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1&quot;&gt;Helmut B&amp;#xf6;lcskei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.04625">
<title>Non-separable Models with High-dimensional Data. (arXiv:1702.04625v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1702.04625</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies non-separable models with a continuous treatment when the
dimension of the control variables is high and potentially larger than the
effective sample size. We propose a three-step estimation procedure to estimate
the average, quantile, and marginal treatment effects. In the first stage we
estimate the conditional mean, distribution, and density objects by penalized
local least squares, penalized local maximum likelihood estimation, and
penalized conditional density estimation, respectively, where control variables
are selected via a localized method of $L_{1}$ -penalization at each value of
the continuous treatment. In the second stage we estimate the average and the
marginal distribution of the potential outcome via the plug-in principle. In
the third stage, we estimate the quantile and marginal treatment effects by
inverting the estimated distribution function and using the local linear
regression, respectively. We study the asymptotic properties of these
estimators and propose a weighted-bootstrap method for inference. Using
simulated and real datasets, we demonstrate the proposed estimators perform
well in finite samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Liangjun Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ura_T/0/1/0/all/0/1&quot;&gt;Takuya Ura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00154">
<title>Inertial Odometry on Handheld Smartphones. (arXiv:1703.00154v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00154</link>
<description rdf:parseType="Literal">&lt;p&gt;Building a complete inertial navigation system using the limited quality data
provided by current smartphones has been regarded challenging, if not
impossible. This paper shows that by careful crafting and accounting for the
weak information in the sensor samples, smartphones are capable of pure
inertial navigation. We present a probabilistic approach for orientation and
use-case free inertial odometry, which is based on double-integrating rotated
accelerations. The strength of the model is in learning additive and
multiplicative IMU biases online. We are able to track the phone position,
velocity, and pose in real-time and in a computationally lightweight fashion by
solving the inference with an extended Kalman filter. The information fusion is
completed with zero-velocity updates (if the phone remains stationary),
altitude correction from barometric pressure readings (if available), and
pseudo-updates constraining the momentary speed. We demonstrate our approach
using an iPad and iPhone in several indoor dead-reckoning applications and in a
measurement tool setup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cortes_S/0/1/0/all/0/1&quot;&gt;Santiago Cortes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1&quot;&gt;Esa Rahtu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1&quot;&gt;Juho Kannala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.04691">
<title>Conditional Time Series Forecasting with Convolutional Neural Networks. (arXiv:1703.04691v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.04691</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for conditional time series forecasting based on an
adaptation of the recent deep convolutional WaveNet architecture. The proposed
network contains stacks of dilated convolutions that allow it to access a broad
range of history when forecasting, a ReLU activation function and conditioning
is performed by applying multiple convolutional filters in parallel to separate
time series which allows for the fast processing of data and the exploitation
of the correlation structure between the multivariate time series. We test and
analyze the performance of the convolutional network both unconditionally as
well as conditionally for financial time series forecasting using the S&amp;amp;P500,
the volatility index, the CBOE interest rate and several exchange rates and
extensively compare it to the performance of the well-known autoregressive
model and a long-short term memory network. We show that a convolutional
network is well-suited for regression-type problems and is able to effectively
learn dependencies in and between the series without the need for long
historical time series, is a time-efficient and easy to implement alternative
to recurrent-type networks and tends to outperform linear and recurrent models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Borovykh_A/0/1/0/all/0/1&quot;&gt;Anastasia Borovykh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bohte_S/0/1/0/all/0/1&quot;&gt;Sander Bohte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oosterlee_C/0/1/0/all/0/1&quot;&gt;Cornelis W. Oosterlee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.05046">
<title>Counting Process Based Dimension Reduction Methods for Censored Outcomes. (arXiv:1704.05046v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1704.05046</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a class of dimension reduction methods for right censored survival
data using a counting process representation of the failure process.
Semiparametric estimating equations are constructed to estimate the dimension
reduction subspace for the failure time model. The proposed method addresses
two fundamental limitations of existing approaches. First, using the counting
process formulation, it does not require any estimation of the censoring
distribution to compensate the bias in estimating the dimension reduction
subspace. Second, the nonparametric part in the estimating equations is
adaptive to the structural dimension, hence the approach circumvents the curse
of dimensionality. Asymptotic normality is established for the obtained
estimators. We further propose a computationally efficient approach that
simplifies the estimation equation formulations and requires only a singular
value decomposition to estimate the dimension reduction subspace. Numerical
studies suggest that our new approaches exhibit significantly improved
performance for estimating the true dimension reduction subspace. We further
conduct a real data analysis on a skin cutaneous melanoma dataset from The
Cancer Genome Atlas. The proposed method is implemented in the R package
&quot;orthoDr&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Ruoqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Donglin Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.01372">
<title>Brownian forgery of statistical dependences. (arXiv:1705.01372v3 [cond-mat.stat-mech] UPDATED)</title>
<link>http://arxiv.org/abs/1705.01372</link>
<description rdf:parseType="Literal">&lt;p&gt;The balance held by Brownian motion between temporal regularity and
randomness is embodied in a remarkable way by Levy&apos;s forgery of continuous
functions. Here we describe how this property can be extended to forge
arbitrary dependences between two statistical systems, and then establish a new
Brownian independence test based on fluctuating random paths. We also argue
that this result allows revisiting the theory of Brownian covariance from a
physical perspective and opens the possibility of engineering nonlinear
correlation measures from more general functional integrals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wens_V/0/1/0/all/0/1&quot;&gt;Vincent Wens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.01677">
<title>Optimized Regression Discontinuity Designs. (arXiv:1705.01677v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1705.01677</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing popularity of regression discontinuity methods for causal
inference in observational studies has led to a proliferation of different
estimating strategies, most of which involve first fitting non-parametric
regression models on both sides of a treatment assignment boundary and then
reporting plug-in estimates for the effect of interest. In applications,
however, it is often difficult to tune the non-parametric regressions in a way
that is well calibrated for the specific target of inference; for example, the
model with the best global in-sample fit may provide poor estimates of the
discontinuity parameter. In this paper, we propose an alternative method for
estimation and statistical inference in regression discontinuity designs that
uses numerical convex optimization to directly obtain the finite-sample-minimax
linear estimator for the regression discontinuity parameter, subject to bounds
on the second derivative of the conditional response function. Given a bound on
the second derivative, our proposed method is fully data-driven, and provides
uniform confidence intervals for the regression discontinuity parameter with
both discrete and continuous running variables. The method also naturally
extends to the case of multiple running variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Imbens_G/0/1/0/all/0/1&quot;&gt;Guido Imbens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wager_S/0/1/0/all/0/1&quot;&gt;Stefan Wager&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.08105">
<title>FRK: An R Package for Spatial and Spatio-Temporal Prediction with Large Datasets. (arXiv:1705.08105v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08105</link>
<description rdf:parseType="Literal">&lt;p&gt;FRK is an R software package for spatial/spatio-temporal modelling and
prediction with large datasets. It facilitates optimal spatial prediction
(kriging) on the most commonly used manifolds (in Euclidean space and on the
surface of the sphere), for both spatial and spatio-temporal fields. It differs
from many of the packages for spatial modelling and prediction by avoiding
stationary and isotropic covariance and variogram models, instead constructing
a spatial random effects (SRE) model on a fine-resolution discretised spatial
domain. The discrete element is known as a basic areal unit (BAU), whose
introduction in the software leads to several practical advantages. The
software can be used to (i) integrate multiple observations with different
supports with relative ease; (ii) obtain exact predictions at millions of
prediction locations (without conditional simulation); and (iii) distinguish
between measurement error and fine-scale variation at the resolution of the
BAU, thereby allowing for reliable uncertainty quantification. The temporal
component is included by adding another dimension. A key component of the SRE
model is the specification of spatial or spatio-temporal basis functions; in
the package, they can be generated automatically or by the user. The package
also offers automatic BAU construction, an expectation-maximisation (EM)
algorithm for parameter estimation, and functionality for prediction over any
user-specified polygons or BAUs. Use of the package is illustrated on several
spatial and spatio-temporal datasets, and its predictions and the model it
implements are extensively compared to others commonly used for spatial
prediction and modelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zammit_Mangion_A/0/1/0/all/0/1&quot;&gt;Andrew Zammit-Mangion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cressie_N/0/1/0/all/0/1&quot;&gt;Noel Cressie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02798">
<title>Data Assimilation in the Geosciences - An overview on methods, issues and perspectives. (arXiv:1709.02798v3 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02798</link>
<description rdf:parseType="Literal">&lt;p&gt;We commonly refer to state-estimation theory in geosciences as data
assimilation. This term encompasses the entire sequence of operations that,
starting from the observations of a system, and from additional statistical and
dynamical information (such as a dynamical evolution model), provides an
estimate of its state. Data assimilation is standard practice in numerical
weather prediction, but its application is becoming widespread in many other
areas of climate, atmosphere, ocean and environment modeling; in all
circumstances where one intends to estimate the state of a large dynamical
system based on limited information. While the complexity of data assimilation,
and of the methods thereof, stands on its interdisciplinary nature across
statistics, dynamical systems and numerical optimization, when applied to
geosciences an additional difficulty arises by the continually increasing
sophistication of the environmental models. Thus, in spite of data assimilation
being nowadays ubiquitous in geosciences, it has so far remained a topic mostly
reserved to experts. We aim this overview article at geoscientists with a
background in mathematical and physical modeling, who are interested in the
rapid development of data assimilation and its growing domains of application
in environmental science, but so far have not delved into its conceptual and
methodological complexities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Carrassi_A/0/1/0/all/0/1&quot;&gt;Alberto Carrassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bocquet_M/0/1/0/all/0/1&quot;&gt;Marc Bocquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bertino_L/0/1/0/all/0/1&quot;&gt;Laurent Bertino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Evensen_G/0/1/0/all/0/1&quot;&gt;Geir Evensen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05380">
<title>The Uncertainty Bellman Equation and Exploration. (arXiv:1709.05380v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05380</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the exploration/exploitation problem in reinforcement learning.
For exploitation, it is well known that the Bellman equation connects the value
at any time-step to the expected value at subsequent time-steps. In this paper
we consider a similar \textit{uncertainty} Bellman equation (UBE), which
connects the uncertainty at any time-step to the expected uncertainties at
subsequent time-steps, thereby extending the potential exploratory benefit of a
policy beyond individual time-steps. We prove that the unique fixed point of
the UBE yields an upper bound on the variance of the posterior distribution of
the Q-values induced by any policy. This bound can be much tighter than
traditional count-based bonuses that compound standard deviation rather than
variance. Importantly, and unlike several existing approaches to optimism, this
method scales naturally to large systems with complex generalization.
Substituting our UBE-exploration strategy for $\epsilon$-greedy improves DQN
performance on 51 out of 57 games in the Atari suite.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ODonoghue_B/0/1/0/all/0/1&quot;&gt;Brendan O&amp;#x27;Donoghue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1&quot;&gt;Ian Osband&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1&quot;&gt;Remi Munos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1&quot;&gt;Volodymyr Mnih&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01244">
<title>Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory. (arXiv:1711.01244v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01244</link>
<description rdf:parseType="Literal">&lt;p&gt;In meta-learning an agent extracts knowledge from observed tasks, aiming to
facilitate learning of novel future tasks. Under the assumption that future
tasks are &apos;related&apos; to previous tasks, representations should be learned in a
way which captures the common structure across learned tasks, while allowing
the learner sufficient flexibility to adapt to novel aspects of new tasks. We
present a framework for meta-learning that is based on generalization error
bounds, allowing us to extend various PAC-Bayes bounds to meta-learning.
Learning takes place through the construction of a distribution over hypotheses
based on the observed tasks, and its utilization for learning a new task. Thus,
prior knowledge is incorporated through setting an experience-dependent prior
for novel tasks. We develop a gradient-based algorithm which minimizes an
objective function derived from the bounds and demonstrate its effectiveness
numerically with deep neural networks. In addition to establishing the improved
performance available through meta-learning, we demonstrate the intuitive way
by which prior information is manifested at different levels of the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amit_R/0/1/0/all/0/1&quot;&gt;Ron Amit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meir_R/0/1/0/all/0/1&quot;&gt;Ron Meir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03190">
<title>Learning Credible Models. (arXiv:1711.03190v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03190</link>
<description rdf:parseType="Literal">&lt;p&gt;In many settings, it is important that a model be capable of providing
reasons for its predictions (i.e., the model must be interpretable). However,
the model&apos;s reasoning may not conform with well-established knowledge. In such
cases, while interpretable, the model lacks \textit{credibility}. In this work,
we formally define credibility in the linear setting and focus on techniques
for learning models that are both accurate and credible. In particular, we
propose a regularization penalty, expert yielded estimates (EYE), that
incorporates expert knowledge about well-known relationships among covariates
and the outcome of interest. We give both theoretical and empirical results
comparing our proposed method to several other regularization techniques.
Across a range of settings, experiments on both synthetic and real data show
that models learned using the EYE penalty are significantly more credible than
those learned using other penalties. Applied to a large-scale patient risk
stratification task, our proposed technique results in a model whose top
features overlap significantly with known clinical risk factors, while still
achieving good predictive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jeeheh Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1&quot;&gt;Jenna Wiens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04425">
<title>Message Passing Stein Variational Gradient Descent. (arXiv:1711.04425v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04425</link>
<description rdf:parseType="Literal">&lt;p&gt;Stein variational gradient descent (SVGD) is a recently proposed
particle-based Bayesian inference method, which has attracted a lot of interest
due to its remarkable approximation ability and particle efficiency compared to
traditional variational inference and Markov Chain Monte Carlo methods.
However, we observed that particles of SVGD tend to collapse to modes of the
target distribution, and this particle degeneracy phenomenon becomes more
severe with higher dimensions. Our theoretical analysis finds out that there
exists a negative correlation between the dimensionality and the repulsive
force of SVGD which should be blamed for this phenomenon. We propose Message
Passing SVGD (MP-SVGD) to solve this problem. By leveraging the conditional
independence structure of probabilistic graphical models (PGMs), MP-SVGD
converts the original high-dimensional global inference problem into a set of
local ones over the Markov blanket with lower dimensions. Experimental results
show its advantages of preventing vanishing repulsive force in high-dimensional
space over SVGD, and its particle efficiency and approximation flexibility over
other inference methods on graphical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhuo_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Ning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07168">
<title>Stein Variational Message Passing for Continuous Graphical Models. (arXiv:1711.07168v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07168</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel distributed inference algorithm for continuous graphical
models, by extending Stein variational gradient descent (SVGD) to leverage the
Markov dependency structure of the distribution of interest. Our approach
combines SVGD with a set of structured local kernel functions defined on the
Markov blanket of each node, which alleviates the curse of high dimensionality
and simultaneously yields a distributed algorithm for decentralized inference
tasks. We justify our method with theoretical analysis and show that the use of
local kernels can be viewed as a new type of localized approximation that
matches the target distribution on the conditional distributions of each node
over its Markov blanket. Our empirical results show that our method outperforms
a variety of baselines including standard MCMC and particle message passing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dilin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhe Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11157">
<title>A Semantic Loss Function for Deep Learning with Symbolic Knowledge. (arXiv:1711.11157v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11157</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper develops a novel methodology for using symbolic knowledge in deep
learning. From first principles, we derive a semantic loss function that
bridges between neural output vectors and logical constraints. This loss
function captures how close the neural network is to satisfying the constraints
on its output. An experimental evaluation shows that it effectively guides the
learner to achieve (near-)state-of-the-art results on semi-supervised
multi-class classification. Moreover, it significantly increases the ability of
the neural network to predict structured objects, such as rankings and paths.
These discrete concepts are tremendously difficult to learn, and benefit from a
tight integration of deep learning and symbolic reasoning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jingyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zilu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_T/0/1/0/all/0/1&quot;&gt;Tal Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yitao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1&quot;&gt;Guy Van den Broeck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03646">
<title>Dynamic Mixed Frequency Synthesis for Economic Nowcasting. (arXiv:1712.03646v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03646</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a novel Bayesian framework for dynamic modeling of mixed frequency
data to nowcast quarterly U.S. GDP growth. The introduced framework utilizes
foundational Bayesian theory and treats data sampled at different frequencies
as latent factors that are later synthesized, allowing flexible methodological
specifications based on interests and utility. Time-varying inter-dependencies
between the mixed frequency data are learnt and effectively mapped onto easily
interpretable parameters. A macroeconomic study of nowcasting quarterly U.S.
GDP growth using a number of monthly economic variables demonstrates
improvements in terms of nowcast performance and interpretability compared to
the standard in the literature. The study further shows that incorporating
information during a quarter markedly improves the performance in terms of both
point and density nowcasts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McAlinn_K/0/1/0/all/0/1&quot;&gt;Kenichiro McAlinn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03878">
<title>Generalized Zero-Shot Learning via Synthesized Examples. (arXiv:1712.03878v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03878</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a generative framework for generalized zero-shot learning where
the training and test classes are not necessarily disjoint. Built upon a
variational autoencoder based architecture, consisting of a probabilistic
encoder and a probabilistic conditional decoder, our model can generate novel
exemplars from seen/unseen classes, given their respective class attributes.
These exemplars can subsequently be used to train any off-the-shelf
classification model. One of the key aspects of our encoder-decoder
architecture is a feedback-driven mechanism in which a discriminator (a
multivariate regressor) learns to map the generated exemplars to the
corresponding class attribute vectors, leading to an improved generator. Our
model&apos;s ability to generate and leverage examples from unseen classes to train
the classification model naturally helps to mitigate the bias towards
predicting seen classes in generalized zero-shot learning settings. Through a
comprehensive set of experiments, we show that our model outperforms several
state-of-the-art methods, on several benchmark datasets, for both standard as
well as generalized zero-shot learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1&quot;&gt;Gundeep Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1&quot;&gt;Vinay Kumar Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Ashish Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1&quot;&gt;Piyush Rai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10252">
<title>Spectral analysis for nonstationary audio. (arXiv:1712.10252v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1712.10252</link>
<description rdf:parseType="Literal">&lt;p&gt;A new approach for the analysis of nonstationary signals is proposed, with a
focus on audio applications. Following earlier contributions, nonstationarity
is modeled via stationarity-breaking operators acting on Gaussian stationary
random signals. The focus is here on time warping and amplitude modulation, and
an approximate maximum-likelihood approach based on suitable approximations in
the wavelet transform domain is developed. This paper provides theoretical
analysis of the approximations, and describes and analyzes a corresponding
estimation algorithm. The latter is tested and validated on synthetic as well
as real audio signal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meynard_A/0/1/0/all/0/1&quot;&gt;Adrien Meynard&lt;/a&gt; (I2M), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Torresani_B/0/1/0/all/0/1&quot;&gt;Bruno Torresani&lt;/a&gt; (I2M)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04062">
<title>MINE: Mutual Information Neural Estimation. (arXiv:1801.04062v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04062</link>
<description rdf:parseType="Literal">&lt;p&gt;We argue that the estimation of mutual information between high dimensional
continuous random variables can be achieved by gradient descent over neural
networks. We present a Mutual Information Neural Estimator (MINE) that is
linearly scalable in dimensionality as well as in sample size, trainable
through back-prop, and strongly consistent. We present a handful of
applications on which MINE can be used to minimize or maximize mutual
information. We apply MINE to improve adversarially trained generative models.
We also use MINE to implement Information Bottleneck, applying it to supervised
classification; our results demonstrate substantial improvement in flexibility
and performance in these settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belghazi_M/0/1/0/all/0/1&quot;&gt;Mohamed Ishmael Belghazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1&quot;&gt;Aristide Baratin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1&quot;&gt;Sai Rajeswar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozair_S/0/1/0/all/0/1&quot;&gt;Sherjil Ozair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06309">
<title>Composite Functional Gradient Learning of Generative Adversarial Models. (arXiv:1801.06309v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06309</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper first presents a theory for generative adversarial methods that
does not rely on the traditional minimax formulation. It shows that with a
strong discriminator, a good generator can be learned so that the KL divergence
between the distributions of real data and generated data improves after each
functional gradient step until it converges to zero. Based on the theory, we
propose a new stable generative adversarial method. A theoretical insight into
the original GAN from this new viewpoint is also provided. The experiments on
image generation show the effectiveness of our new method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Johnson_R/0/1/0/all/0/1&quot;&gt;Rie Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00008">
<title>On the Topic of Jets: Disentangling Quarks and Gluons at Colliders. (arXiv:1802.00008v2 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00008</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce jet topics: a framework to identify underlying classes of jets
from collider data. Because of a close mathematical relationship between
distributions of observables in jets and emergent themes in sets of documents,
we can apply recent techniques in &quot;topic modeling&quot; to extract jet topics from
data with minimal or no input from simulation or theory. As a proof of concept
with parton shower samples, we apply jet topics to determine separate quark and
gluon jet distributions for constituent multiplicity. We also determine
separate quark and gluon rapidity spectra from a mixed Z-plus-jet sample. While
jet topics are defined directly from hadron-level multi-differential cross
sections, one can also predict jet topics from first-principles theoretical
calculations, with potential implications for how to define quark and gluon
jets beyond leading-logarithmic accuracy. These investigations suggest that jet
topics will be useful for extracting underlying jet distributions and fractions
in a wide range of contexts at the Large Hadron Collider.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Metodiev_E/0/1/0/all/0/1&quot;&gt;Eric M. Metodiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Thaler_J/0/1/0/all/0/1&quot;&gt;Jesse Thaler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03801">
<title>SGD and Hogwild! Convergence Without the Bounded Gradients Assumption. (arXiv:1802.03801v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03801</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic gradient descent (SGD) is the optimization algorithm of choice in
many machine learning applications such as regularized empirical risk
minimization and training deep neural networks. The classical convergence
analysis of SGD is carried out under the assumption that the norm of the
stochastic gradient is uniformly bounded. While this might hold for some loss
functions, it is always violated for cases where the objective function is
strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD
is performed under the assumption that stochastic gradients are bounded with
respect to the true gradient norm. Here we show that for stochastic problems
arising in machine learning such bound always holds; and we also propose an
alternative convergence analysis of SGD with diminishing learning rate regime,
which results in more relaxed conditions than those in (Bottou et al.,2016). We
then move on the asynchronous parallel setting, and prove convergence of
Hogwild! algorithm in the same regime, obtaining the first convergence results
for this method in the case of diminished learning rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Lam M. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuong Ha Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dijk_M/0/1/0/all/0/1&quot;&gt;Marten van Dijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1&quot;&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scheinberg_K/0/1/0/all/0/1&quot;&gt;Katya Scheinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Takac_M/0/1/0/all/0/1&quot;&gt;Martin Tak&amp;#xe1;&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05027">
<title>Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care. (arXiv:1802.05027v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05027</link>
<description rdf:parseType="Literal">&lt;p&gt;Patients in the intensive care unit (ICU) require constant and close
supervision. To assist clinical staff in this task, hospitals use monitoring
systems that trigger audiovisual alarms if their algorithms indicate that a
patient&apos;s condition may be worsening. However, current monitoring systems are
extremely sensitive to movement artefacts and technical errors. As a result,
they typically trigger hundreds to thousands of false alarms per patient per
day - drowning the important alarms in noise and adding to the exhaustion of
clinical staff. In this setting, data is abundantly available, but obtaining
trustworthy annotations by experts is laborious and expensive. We frame the
problem of false alarm reduction from multivariate time series as a
machine-learning task and address it with a novel multitask network
architecture that utilises distant supervision through multiple related
auxiliary tasks in order to reduce the number of expensive labels required for
training. We show that our approach leads to significant improvements over
several state-of-the-art baselines on real-world ICU data and provide new
insights on the importance of task selection and architectural choices in
distantly supervised multitask learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwab_P/0/1/0/all/0/1&quot;&gt;Patrick Schwab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_E/0/1/0/all/0/1&quot;&gt;Emanuela Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muroi_C/0/1/0/all/0/1&quot;&gt;Carl Muroi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mack_D/0/1/0/all/0/1&quot;&gt;David J. Mack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strassle_C/0/1/0/all/0/1&quot;&gt;Christian Str&amp;#xe4;ssle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlen_W/0/1/0/all/0/1&quot;&gt;Walter Karlen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06403">
<title>RadialGAN: Leveraging multiple datasets to improve target-specific predictive models using Generative Adversarial Networks. (arXiv:1802.06403v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06403</link>
<description rdf:parseType="Literal">&lt;p&gt;Training complex machine learning models for prediction often requires a
large amount of data that is not always readily available. Leveraging these
external datasets from related but different sources is therefore an important
task if good predictive models are to be built for deployment in settings where
data can be rare. In this paper we propose a novel approach to the problem in
which we use multiple GAN architectures to learn to translate from one dataset
to another, thereby allowing us to effectively enlarge the target dataset, and
therefore learn better predictive models than if we simply used the target
dataset. We show the utility of such an approach, demonstrating that our method
improves the prediction performance on the target domain over using just the
target dataset and also show that our framework outperforms several other
benchmarks on a collection of real-world medical datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jinsung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordon_J/0/1/0/all/0/1&quot;&gt;James Jordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06501">
<title>Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning. (arXiv:1802.06501v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06501</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems play a crucial role in mitigating the problem of
information overload by suggesting users&apos; personalized items or services. The
vast majority of traditional recommender systems consider the recommendation
procedure as a static process and make recommendations following a fixed
strategy. In this paper, we propose a novel recommender system with the
capability of continuously improving its strategies during the interactions
with users. We model the sequential interactions between users and a
recommender system as a Markov Decision Process (MDP) and leverage
Reinforcement Learning (RL) to automatically learn the optimal strategies via
recommending trial-and-error items and receiving reinforcements of these items
from users&apos; feedback. Users&apos; feedback can be positive and negative and both
types of feedback have great potentials to boost recommendations. However, the
number of negative feedback is much larger than that of positive one; thus
incorporating them simultaneously is challenging since positive feedback could
be buried by negative one. In this paper, we develop a novel approach to
incorporate them into the proposed deep recommender system (DEERS) framework.
The experimental results based on real-world e-commerce data demonstrate the
effectiveness of the proposed framework. Further experiments have been
conducted to understand the importance of both positive and negative feedback
in recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhuoye Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Long Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08183">
<title>Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity. (arXiv:1802.08183v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08183</link>
<description rdf:parseType="Literal">&lt;p&gt;Online optimization has been a successful framework for solving large-scale
problems under computational constraints and partial information. Current
methods for online convex optimization require either a projection or exact
gradient computation at each step, both of which can be prohibitively expensive
for large-scale applications. At the same time, there is a growing trend of
non-convex optimization in machine learning community and a need for online
methods. Continuous DR-submodular functions, which exhibit a natural
diminishing returns condition, have recently been proposed as a broad class of
non-convex functions which may be efficiently optimized. Although online
methods have been introduced, they suffer from similar problems. In this work,
we propose Meta-Frank-Wolfe, the first online projection-free algorithm that
uses stochastic gradient estimates. The algorithm relies on a careful sampling
of gradients in each round and achieves the optimal $O( \sqrt{T})$ adversarial
regret bounds for convex and continuous submodular optimization. We also
propose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single
stochastic gradient estimate in each round and achieves an $O(T^{2/3})$
stochastic regret bound for convex and continuous submodular optimization. We
apply our methods to develop a novel &quot;lifting&quot; framework for the online
discrete submodular maximization and also see that they outperform current
state-of-the-art techniques on various experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harshaw_C/0/1/0/all/0/1&quot;&gt;Christopher Harshaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karbasi_A/0/1/0/all/0/1&quot;&gt;Amin Karbasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09128">
<title>Averaging Stochastic Gradient Descent on Riemannian Manifolds. (arXiv:1802.09128v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09128</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the minimization of a function defined on a Riemannian manifold
$\mathcal{M}$ accessible only through unbiased estimates of its gradients. We
develop a geometric framework to transform a sequence of slowly converging
iterates generated from stochastic gradient descent (SGD) on $\mathcal{M}$ to
an averaged iterate sequence with a robust and fast $O(1/n)$ convergence rate.
We then present an application of our framework to geodesically-strongly-convex
(and possibly Euclidean non-convex) problems. Finally, we demonstrate how these
ideas apply to the case of streaming $k$-PCA, where we show how to accelerate
the slow rate of the randomized power method (without requiring knowledge of
the eigengap) into a robust algorithm achieving the optimal rate of
convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripuraneni_N/0/1/0/all/0/1&quot;&gt;Nilesh Tripuraneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1&quot;&gt;Nicolas Flammarion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09477">
<title>Addressing Function Approximation Error in Actor-Critic Methods. (arXiv:1802.09477v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09477</link>
<description rdf:parseType="Literal">&lt;p&gt;In value-based reinforcement learning methods such as deep Q-learning,
function approximation errors are known to lead to overestimated value
estimates and suboptimal policies. We show that this problem persists in an
actor-critic setting and propose novel mechanisms to minimize its effects on
both the actor and the critic. Our algorithm builds on Double Q-learning, by
taking the minimum value between a pair of critics to limit overestimation. We
draw the connection between target networks and overestimation bias, and
suggest delaying policy updates to reduce per-update error and further improve
performance. We evaluate our method on the suite of OpenAI gym tasks,
outperforming the state of the art in every environment tested.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujimoto_S/0/1/0/all/0/1&quot;&gt;Scott Fujimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoof_H/0/1/0/all/0/1&quot;&gt;Herke van Hoof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1&quot;&gt;David Meger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00094">
<title>Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions. (arXiv:1803.00094v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00094</link>
<description rdf:parseType="Literal">&lt;p&gt;In the recent literature the important role of depth in deep learning has
been emphasized. In this paper we argue that sufficient width of a feedforward
network is equally important by answering the simple question under which
conditions the decision regions of a neural network are connected. It turns out
that for a class of activation functions including leaky ReLU, neural networks
having a pyramidal structure, that is no layer has more hidden units than the
input dimension, produce necessarily connected decision regions. This implies
that a sufficiently wide hidden layer is necessary to guarantee that the
network can produce disconnected decision regions. We discuss the implications
of this result for the construction of neural networks, in particular the
relation to the problem of adversarial manipulation of classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quynh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukkamala_M/0/1/0/all/0/1&quot;&gt;Mahesh Chandra Mukkamala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03148">
<title>Generating Artificial Data for Private Deep Learning. (arXiv:1803.03148v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03148</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose generating artificial data that retain statistical
properties of real data as the means of providing privacy with respect to the
original dataset. We use generative adversarial network to draw
privacy-preserving artificial data samples and derive an empirical method to
assess the risk of information disclosure in a differential-privacy-like way.
Our experiments show that we are able to generate artificial data of high
quality and successfully train and validate machine learning models on this
data while limiting potential privacy loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triastcyn_A/0/1/0/all/0/1&quot;&gt;Aleksei Triastcyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1&quot;&gt;Boi Faltings&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06058">
<title>Constant-Time Predictive Distributions for Gaussian Processes. (arXiv:1803.06058v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06058</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most compelling features of Gaussian process (GP) regression is
its ability to provide well-calibrated posterior distributions. Recent advances
in inducing point methods have sped up GP marginal likelihood and posterior
mean computations, leaving posterior covariance estimation and sampling as the
remaining computational bottlenecks. In this paper we address these
shortcomings by using the Lanczos algorithm to rapidly approximate the
predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs
Variance Estimates), substantially improves time and space complexity. In our
experiments, LOVE computes covariances up to 2,000 times faster and draws
samples 18,000 times faster than existing methods, all without sacrificing
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1&quot;&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Jacob R. Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1&quot;&gt;Kilian Q. Weinberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08355">
<title>Structured Output Learning with Abstention: Application to Accurate Opinion Prediction. (arXiv:1803.08355v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08355</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by Supervised Opinion Analysis, we propose a novel framework
devoted to Structured Output Learning with Abstention (SOLA). The structure
prediction model is able to abstain from predicting some labels in the
structured output at a cost chosen by the user in a flexible way. For that
purpose, we decompose the problem into the learning of a pair of predictors,
one devoted to structured abstention and the other, to structured output
prediction. To compare fully labeled training data with predictions potentially
containing abstentions, we define a wide class of asymmetric abstention-aware
losses. Learning is achieved by surrogate regression in an appropriate feature
space while prediction with abstention is performed by solving a new pre-image
problem. Thus, SOLA extends recent ideas about Structured Output Prediction via
surrogate problems and calibration theory and enjoys statistical guarantees on
the resulting excess risk. Instantiated on a hierarchical abstention-aware
loss, SOLA is shown to be relevant for fine-grained opinion mining and gives
state-of-the-art results on this task. Moreover, the abstention-aware
representations can be used to competitively predict user-review ratings based
on a sentence-level opinion predictor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Alexandre Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Essid_S/0/1/0/all/0/1&quot;&gt;Slim Essid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1&quot;&gt;Chlo&amp;#xe9; Clavel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+dAlche_Buc_F/0/1/0/all/0/1&quot;&gt;Florence d&amp;#x27;Alch&amp;#xe9;-Buc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09050">
<title>Learning to Reweight Examples for Robust Deep Learning. (arXiv:1803.09050v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09050</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have been shown to be very powerful modeling tools for
many supervised learning tasks involving complex input patterns. However, they
can also easily overfit to training set biases and label noises. In addition to
various regularizers, example reweighting algorithms are popular solutions to
these problems, but they require careful tuning of additional hyperparameters,
such as example mining schedules and regularization hyperparameters. In
contrast to past reweighting methods, which typically consist of functions of
the cost value of each example, in this work we propose a novel meta-learning
algorithm that learns to assign weights to training examples based on their
gradient directions. To determine the example weights, our method performs a
meta gradient descent step on the current mini-batch example weights (which are
initialized from zero) to minimize the loss on a clean unbiased validation set.
Our proposed method can be easily implemented on any type of deep network, does
not require any additional hyperparameter tuning, and achieves impressive
performance on class imbalance and corrupted label problems where only a small
amount of clean validation data is available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengye Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenyuan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09159">
<title>Efficient Discovery of Heterogeneous Treatment Effects in Randomized Experiments via Anomalous Pattern Detection. (arXiv:1803.09159v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09159</link>
<description rdf:parseType="Literal">&lt;p&gt;In the recent literature on estimating heterogeneous treatment effects, each
proposed method makes its own set of restrictive assumptions about the
intervention&apos;s effects and which subpopulations to explicitly estimate.
Moreover, the majority of the literature provides no mechanism to identify
which subpopulations are the most affected--beyond manual inspection--and
provides little guarantee on the correctness of the identified subpopulations.
Therefore, we propose Treatment Effect Subset Scan (TESS), a new method for
discovering which subpopulation in a randomized experiment is most
significantly affected by a treatment. We frame this challenge as a pattern
detection problem where we efficiently maximize a nonparametric scan statistic
over subpopulations. Furthermore, we identify the subpopulation which
experiences the largest distributional change as a result of the intervention,
while making minimal assumptions about the intervention&apos;s effects or the
underlying data generating process. In addition to the algorithm, we
demonstrate that the asymptotic Type I and II error can be controlled, and
provide sufficient conditions for detection consistency--i.e., exact
identification of the affected subpopulation. Finally, we validate the efficacy
of the method by discovering heterogeneous treatment effects in simulations and
in real-world data from a well-known program evaluation study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McFowland_E/0/1/0/all/0/1&quot;&gt;Edward McFowland III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Somanchi_S/0/1/0/all/0/1&quot;&gt;Sriram Somanchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neill_D/0/1/0/all/0/1&quot;&gt;Daniel B. Neill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09539">
<title>On Matching Pursuit and Coordinate Descent. (arXiv:1803.09539v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09539</link>
<description rdf:parseType="Literal">&lt;p&gt;Two popular examples of first-order optimization methods over linear spaces
are coordinate descent and matching pursuit algorithms, with their randomized
variants. While the former targets the optimization by moving along
coordinates, the latter considers a generalized notion of directions.
Exploiting the connection between the two algorithms, we present a unified
analysis of both, providing affine invariant sublinear $O(1/t)$ rates on smooth
objectives and linear convergence on strongly convex objectives. As a byproduct
of our affine invariant analysis of matching pursuit, our rates for steepest
coordinate descent are the tightest known. Furthermore, we show the first
accelerated convergence rate $O(1/t^2)$ for matching pursuit and steepest
coordinate descent on convex objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Anant Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karimireddy_S/0/1/0/all/0/1&quot;&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1&quot;&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02477">
<title>Programmatically Interpretable Reinforcement Learning. (arXiv:1804.02477v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02477</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a reinforcement learning framework, called Programmatically
Interpretable Reinforcement Learning (PIRL), that is designed to generate
interpretable and verifiable agent policies. Unlike the popular Deep
Reinforcement Learning (DRL) paradigm, which represents policies by neural
networks, PIRL represents policies using a high-level, domain-specific
programming language. Such programmatic policies have the benefits of being
more easily interpreted than neural networks, and being amenable to
verification by symbolic methods. We propose a new method, called Neurally
Directed Program Search (NDPS), for solving the challenging nonsmooth
optimization problem of finding a programmatic policy with maximal reward. NDPS
works by first learning a neural policy network using DRL, and then performing
a local search over programmatic policies that seeks to minimize a distance
from this neural &quot;oracle&quot;. We evaluate NDPS on the task of learning to drive a
simulated car in the TORCS car-racing environment. We demonstrate that NDPS is
able to discover human-readable policies that pass some significant performance
bars. We also show that PIRL policies can have smoother trajectories, and can
be more easily transferred to environments not encountered during training,
than corresponding policies discovered by DRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1&quot;&gt;Abhinav Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murali_V/0/1/0/all/0/1&quot;&gt;Vijayaraghavan Murali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Swarat Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03184">
<title>Adversarial Time-to-Event Modeling. (arXiv:1804.03184v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03184</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern health data science applications leverage abundant molecular and
electronic health data, providing opportunities for machine learning to build
statistical models to support clinical practice. Time-to-event analysis, also
called survival analysis, stands as one of the most representative examples of
such statistical models. We present a deep-network-based approach that
leverages adversarial learning to address a key challenge in modern
time-to-event modeling: nonparametric estimation of event-time distributions.
We also introduce a principled cost function to exploit information from
censored events (events that occur subsequent to the observation window).
Unlike most time-to-event models, we focus on the estimation of time-to-event
distributions, rather than time ordering. We validate our model on both
benchmark and real datasets, demonstrating that the proposed formulation yields
significant performance gains relative to a parametric alternative, which we
also propose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chapfuwa_P/0/1/0/all/0/1&quot;&gt;Paidamoyo Chapfuwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1&quot;&gt;Chenyang Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Page_C/0/1/0/all/0/1&quot;&gt;Courtney Page&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goldstein_B/0/1/0/all/0/1&quot;&gt;Benjamin Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Henao_R/0/1/0/all/0/1&quot;&gt;Ricardo Henao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07090">
<title>Intriguing Properties of Learned Representations. (arXiv:1804.07090v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07090</link>
<description rdf:parseType="Literal">&lt;p&gt;A key feature of neural networks, particularly deep convolutional neural
networks, is their ability to &quot;learn&quot; useful representations from data. The
very last layer of a neural network is then simply a linear model trained on
these &quot;learned&quot; representations. Despite their numerous applications in other
tasks such as classification, retrieval, clustering etc., a.k.a. transfer
learning, not much work has been published that investigates the structure of
these representations or indeed whether structure can be imposed on them during
the training process.
&lt;/p&gt;
&lt;p&gt;In this paper, we study the effective dimensionality of the learned
representations by models that have proved highly successful for image
classification. We focus on ResNet-18, ResNet-50 and VGG-19 and observe that
when trained on CIFAR10 or CIFAR100, the learned representations exhibit a
fairly low rank structure. We propose a modification to the training procedure,
which further encourages low rank structure on learned activations.
Empirically, we show that this has implications for robustness to adversarial
examples and compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1&quot;&gt;Amartya Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1&quot;&gt;Varun Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H. S. Torr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07193">
<title>Lipschitz Continuity in Model-based Reinforcement Learning. (arXiv:1804.07193v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07193</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine the impact of learning Lipschitz continuous models in the context
of model-based reinforcement learning. We provide a novel bound on multi-step
prediction error of Lipschitz models where we quantify the error using the
Wasserstein metric. We go on to prove an error bound for the value-function
estimate arising from Lipschitz models and show that the estimated value
function is itself Lipschitz. We conclude with empirical results that show the
benefits of controlling the Lipschitz constant of neural-network models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asadi_K/0/1/0/all/0/1&quot;&gt;Kavosh Asadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1&quot;&gt;Dipendra Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Littman_M/0/1/0/all/0/1&quot;&gt;Michael L. Littman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08598">
<title>Black-box Adversarial Attacks with Limited Queries and Information. (arXiv:1804.08598v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08598</link>
<description rdf:parseType="Literal">&lt;p&gt;Current neural network-based classifiers are susceptible to adversarial
examples even in the black-box setting, where the attacker only has query
access to the model. In practice, the threat model for real-world systems is
often more restrictive than the typical black-box model where the adversary can
observe the full output of the network on arbitrarily many chosen inputs. We
define three realistic threat models that more accurately characterize many
real-world classifiers: the query-limited setting, the partial-information
setting, and the label-only setting. We develop new attacks that fool
classifiers under these more restrictive threat models, where previous methods
would be impractical or ineffective. We demonstrate that our methods are
effective against an ImageNet classifier under our proposed threat models. We
also demonstrate a targeted black-box attack against a commercial classifier,
overcoming the challenges of limited query access, partial information, and
other practical issues to break the Google Cloud Vision API.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilyas_A/0/1/0/all/0/1&quot;&gt;Andrew Ilyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engstrom_L/0/1/0/all/0/1&quot;&gt;Logan Engstrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athalye_A/0/1/0/all/0/1&quot;&gt;Anish Athalye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jessy Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08841">
<title>Between hard and soft thresholding: optimal iterative thresholding algorithms. (arXiv:1804.08841v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08841</link>
<description rdf:parseType="Literal">&lt;p&gt;Iterative thresholding algorithms seek to optimize a differentiable objective
function over a sparsity or rank constraint by alternating between gradient
steps that reduce the objective, and thresholding steps that enforce the
constraint. This work examines the choice of the thresholding operator, and
asks whether it is possible to achieve stronger guarantees than what is
possible with hard thresholding. We develop the notion of relative concavity of
a thresholding operator, a quantity that characterizes the convergence
performance of any thresholding operator on the target optimization problem.
Surprisingly, we find that commonly used thresholding operators, such as hard
thresholding and soft thresholding, are suboptimal in terms of convergence
guarantees. Instead, a general class of thresholding operators, lying between
hard thresholding and soft thresholding, is shown to be optimal with the
strongest possible convergence guarantee among all thresholding operators.
Examples of this general class includes $\ell_q$ thresholding with appropriate
choices of $q$, and a newly defined {\em reciprocal thresholding} operator. We
also investigate the implications of the improved optimization guarantee in the
statistical setting of sparse linear regression, and show that this new class
of thresholding operators attain the optimal rate for computationally efficient
estimators, matching the Lasso.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barber_R/0/1/0/all/0/1&quot;&gt;Rina Foygel Barber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09699">
<title>Towards Fast Computation of Certified Robustness for ReLU Networks. (arXiv:1804.09699v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09699</link>
<description rdf:parseType="Literal">&lt;p&gt;Verifying the robustness property of a general Rectified Linear Unit (ReLU)
network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer
CAV17]. Although finding the exact minimum adversarial distortion is hard,
giving a certified lower bound of the minimum distortion is possible. Current
available methods of computing such a bound are either time-consuming or
delivering low quality bounds that are too loose to be useful. In this paper,
we exploit the special structure of ReLU networks and provide two
computationally efficient algorithms Fast-Lin and Fast-Lip that are able to
certify non-trivial lower bounds of minimum distortions, by bounding the ReLU
units with appropriate linear functions Fast-Lin, or by bounding the local
Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods
deliver bounds close to (the gap is 2-3X) exact minimum distortion found by
Reluplex in small MNIST networks while our algorithms are more than 10,000
times faster; (2) our methods deliver similar quality of bounds (the gap is
within 35% and usually around 10%; sometimes our bounds are even better) for
larger networks compared to the methods based on solving linear programming
problems but our algorithms are 33-14,000 times faster; (3) our method is
capable of solving large MNIST and CIFAR networks up to 7 layers with more than
10,000 neurons within tens of seconds on a single CPU core.
&lt;/p&gt;
&lt;p&gt;In addition, we show that, in fact, there is no polynomial time algorithm
that can approximately find the minimum $\ell_1$ adversarial distortion of a
ReLU network with a $0.99\ln n$ approximation ratio unless
$\mathsf{NP}$=$\mathsf{P}$, where $n$ is the number of neurons in the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weng_T/0/1/0/all/0/1&quot;&gt;Tsui-Wei Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongge Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boning_D/0/1/0/all/0/1&quot;&gt;Duane Boning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1&quot;&gt;Inderjit S. Dhillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daniel_L/0/1/0/all/0/1&quot;&gt;Luca Daniel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01852">
<title>Selective Inference for $L_2$-Boosting. (arXiv:1805.01852v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01852</link>
<description rdf:parseType="Literal">&lt;p&gt;We review several recently proposed post-selection inference frameworks and
assess their transferability to the component-wise functional gradient descent
algorithm (CFGD) under normality assumption for model errors, also known as
$L_2$-Boosting. The CFGD is one of the most versatile toolboxes to analyze data
as it scales well to high-dimensional data sets, allows for a very flexible
definition of additive regression models and incorporates inbuilt variable
selection. Due to the iterative nature, which can repeatedly select the same
component to update, a statistical inference framework for component-wise
boosting algorithms requires adaptations of existing approaches; we propose
tests and confidence intervals for linear, grouped and penalized additive model
components selected by $L_2$-Boosting. We apply our framework to the prostate
cancer data set and investigate the properties of our concepts in simulation
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rugamer_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Greven_S/0/1/0/all/0/1&quot;&gt;Sonja Greven&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04591">
<title>Robust and Scalable Models of Microbiome Dynamics. (arXiv:1805.04591v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04591</link>
<description rdf:parseType="Literal">&lt;p&gt;Microbes are everywhere, including in and on our bodies, and have been shown
to play key roles in a variety of prevalent human diseases. Consequently, there
has been intense interest in the design of bacteriotherapies or &quot;bugs as
drugs,&quot; which are communities of bacteria administered to patients for specific
therapeutic applications. Central to the design of such therapeutics is an
understanding of the causal microbial interaction network and the population
dynamics of the organisms. In this work we present a Bayesian nonparametric
model and associated efficient inference algorithm that addresses the key
conceptual and practical challenges of learning microbial dynamics from time
series microbe abundance data. These challenges include high-dimensional (300+
strains of bacteria in the gut) but temporally sparse and non-uniformly sampled
data; high measurement noise; and, nonlinear and physically non-negative
dynamics. Our contributions include a new type of dynamical systems model for
microbial dynamics based on what we term interaction modules, or learned
clusters of latent variables with redundant interaction structure (reducing the
expected number of interaction coefficients from $O(n^2)$ to $O((\log n)^2)$);
a fully Bayesian formulation of the stochastic dynamical systems model that
propagates measurement and latent state uncertainty throughout the model; and
introduction of a temporally varying auxiliary variable technique to enable
efficient inference by relaxing the hard non-negativity constraint on states.
We apply our method to simulated and real data, and demonstrate the utility of
our technique for system identification from limited data and gaining new
biological insights into bacteriotherapy design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gibson_T/0/1/0/all/0/1&quot;&gt;Travis E. Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gerber_G/0/1/0/all/0/1&quot;&gt;Georg K. Gerber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07395">
<title>More green space is related to less antidepressant prescription rates in the Netherlands: A Bayesian geoadditive quantile regression approach. (arXiv:1805.07395v3 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07395</link>
<description rdf:parseType="Literal">&lt;p&gt;Exposure to green space seems to be beneficial for self-reported mental
health. In this study we used an objective health indicator, namely
antidepressant prescription rates. Current studies rely exclusively upon mean
regression models assuming linear associations. It is, however, plausible that
the presence of green space is non-linearly related with different quantiles of
the outcome antidepressant prescription rates. These restrictions may
contribute to inconsistent findings. Our aim was to assess antidepressant
prescription rates in relation to green space, and to analyze how the
relationship varies non-linearly across different quantiles of antidepressant
prescription rates. We used cross-sectional data for the year 2014 at a
municipality level in the Netherlands. Ecological Bayesian geoadditive quantile
regressions were fitted for the 15, 50, and 85 percent quantiles to estimate
green space-prescription rate correlations, controlling for confounders. The
results suggested that green space was overall inversely and non-linearly
associated with antidepressant prescription rates. More important, the
associations differed across the quantiles, although the variation was modest.
Significant non-linearities were apparent: The associations were slightly
positive in the lower quantile and strongly negative in the upper one. Our
findings imply that an increased availability of green space within a
municipality may contribute to a reduction in the number of antidepressant
prescriptions dispensed. Green space is thus a central health and community
asset, whilst a minimum level of 28 percent needs to be established for health
gains. The highest effectiveness occurred at a municipality surface percentage
higher than 79 percent. This inverse dose-dependent relation has important
implications for setting future community-level health and planning policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Helbich_M/0/1/0/all/0/1&quot;&gt;Marco Helbich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klein_N/0/1/0/all/0/1&quot;&gt;Nadja Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_H/0/1/0/all/0/1&quot;&gt;Hannah Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hagedoorn_P/0/1/0/all/0/1&quot;&gt;Paulien Hagedoorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Groenewegen_P/0/1/0/all/0/1&quot;&gt;Peter Groenewegen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08877">
<title>Adversarial Labeling for Learning without Labels. (arXiv:1805.08877v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08877</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the task of training classifiers without labels. We propose a
weakly supervised method---adversarial label learning---that trains classifiers
to perform well against an adversary that chooses labels for training data. The
weak supervision constrains what labels the adversary can choose. The method
therefore minimizes an upper bound of the classifier&apos;s error rate using
projected primal-dual subgradient descent. Minimizing this bound protects
against bias and dependencies in the weak supervision. Experiments on three
real datasets show that our method can train without labels and outperforms
other approaches for weakly supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arachie_C/0/1/0/all/0/1&quot;&gt;Chidubem Arachie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Bert Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11921">
<title>Anonymous Walk Embeddings. (arXiv:1805.11921v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11921</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of representing entire graphs has seen a surge of prominent results,
mainly due to learning convolutional neural networks (CNNs) on graph-structured
data. While CNNs demonstrate state-of-the-art performance in graph
classification task, such methods are supervised and therefore steer away from
the original problem of network representation in task-agnostic manner. Here,
we coherently propose an approach for embedding entire graphs and show that our
feature representations with SVM classifier increase classification accuracy of
CNN algorithms and traditional graph kernels. For this we describe a recently
discovered graph object, anonymous walk, on which we design task-independent
algorithms for learning graph representations in explicit and distributed way.
Overall, our work represents a new scalable unsupervised learning of
state-of-the-art representations of entire graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanov_S/0/1/0/all/0/1&quot;&gt;Sergey Ivanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02338">
<title>Towards Dependability Metrics for Neural Networks. (arXiv:1806.02338v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02338</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial neural networks (NN) are instrumental in realizing
highly-automated driving functionality. An overarching challenge is to identify
best safety engineering practices for NN and other learning-enabled components.
In particular, there is an urgent need for an adequate set of metrics for
measuring all-important NN dependability attributes. We address this challenge
by proposing a number of NN-specific and efficiently computable metrics for
measuring NN dependability attributes including robustness, interpretability,
completeness, and correctness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chih-Hong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuhrenberg_G/0/1/0/all/0/1&quot;&gt;Georg N&amp;#xfc;hrenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chung-Hao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruess_H/0/1/0/all/0/1&quot;&gt;Harald Ruess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasuoka_H/0/1/0/all/0/1&quot;&gt;Hirotoshi Yasuoka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02402">
<title>Localized Structured Prediction. (arXiv:1806.02402v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02402</link>
<description rdf:parseType="Literal">&lt;p&gt;Key to structured prediction is exploiting the problem structure to simplify
the learning process. A major challenge arises when data exhibit a local
structure (e.g., are made by &quot;parts&quot;) that can be leveraged to better
approximate the relation between (parts of) the input and (parts of) the
output. Recent literature on signal processing, and in particular computer
vision, has shown that capturing these aspects is indeed essential to achieve
state-of-the-art performance. While such algorithms are typically derived on a
case-by-case basis, in this work we propose the first theoretical framework to
deal with part-based data from a general perspective. We derive a novel
approach to deal with these problems and study its generalization properties
within the setting of statistical learning theory. Our analysis is novel in
that it explicitly quantifies the benefits of leveraging the part-based
structure of the problem with respect to the learning rates of the proposed
estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ciliberto_C/0/1/0/all/0/1&quot;&gt;Carlo Ciliberto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02455">
<title>MEBN-RM: A Mapping between Multi-Entity Bayesian Network and Relational Model. (arXiv:1806.02455v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02455</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Entity Bayesian Network (MEBN) is a knowledge representation formalism
combining Bayesian Networks (BN) with First-Order Logic (FOL). MEBN has
sufficient expressive power for general-purpose knowledge representation and
reasoning. Developing a MEBN model to support a given application is a
challenge, requiring definition of entities, relationships, random variables,
conditional dependence relationships, and probability distributions. When
available, data can be invaluable both to improve performance and to streamline
development. By far the most common format for available data is the relational
database (RDB). Relational databases describe and organize data according to
the Relational Model (RM). Developing a MEBN model from data stored in an RDB
therefore requires mapping between the two formalisms. This paper presents
MEBN-RM, a set of mapping rules between key elements of MEBN and RM. We
identify links between the two languages (RM and MEBN) and define four levels
of mapping from elements of RM to elements of MEBN. These definitions are
implemented in the MEBN-RM algorithm, which converts a relational schema in RM
to a partial MEBN model. Through this research, the software has been released
as a MEBN-RM open-source software tool. The method is illustrated through two
example use cases using MEBN-RM to develop MEBN models: a Critical
Infrastructure Defense System and a Smart Manufacturing System.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Cheol Young Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laskey_K/0/1/0/all/0/1&quot;&gt;Kathryn Blackmond Laskey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02510">
<title>Removing Algorithmic Discrimination (With Minimal Individual Error). (arXiv:1806.02510v1 [cs.AI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.02510</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of correcting group discriminations within a score
function, while minimizing the individual error. Each group is described by a
probability density function on the set of profiles. We first solve the problem
analytically in the case of two populations, with a uniform bonus-malus on the
zones where each population is a majority. We then address the general case of
n populations, where the entanglement of populations does not allow a similar
analytical solution. We show that an approximate solution with an arbitrarily
high level of precision can be computed with linear programming. Finally, we
address the inverse problem where the error should not go beyond a certain
value and we seek to minimize the discrimination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mhamdi_E/0/1/0/all/0/1&quot;&gt;El Mahdi El Mhamdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1&quot;&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1&quot;&gt;L&amp;#xea; Nguy&amp;#xea;n Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1&quot;&gt;Alexandre Maurer&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>