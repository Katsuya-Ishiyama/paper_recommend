{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we introduce the attentive unsupervised text (w)riter (autr), which is a word level generative model for natural language. it uses a recurrent neural network with a dynamic attention and canvas memory mechanism to iteratively construct sentences. by viewing the state of the memory at intermediate stages and where the model is placing its attention, we gain insight into how it constructs sentences. we demonstrate that autr learns a meaningful latent representation for each sentence, and achieves competitive log-likelihood lower bounds whilst being computationally efficient. it is effective at generating and reconstructing sentences, as well as imputing missing words.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\\\test\\\\test.json', 'r') as f:\n",
    "    data = json.loads(f.readline())\n",
    "\n",
    "description = data['description']\n",
    "normalized = description.lower().strip()\n",
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generating Sentences Using a Dynamic Canvas. (arXiv:1806.05178v1 [cs.CL])'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we introduce the attentive unsupervised text (w)riter (autr), which is a word level generative model for natural language.',\n",
       " 'it uses a recurrent neural network with a dynamic attention and canvas memory mechanism to iteratively construct sentences.',\n",
       " 'by viewing the state of the memory at intermediate stages and where the model is placing its attention, we gain insight into how it constructs sentences.',\n",
       " 'we demonstrate that autr learns a meaningful latent representation for each sentence, and achieves competitive log-likelihood lower bounds whilst being computationally efficient.',\n",
       " 'it is effective at generating and reconstructing sentences, as well as imputing missing words.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.sent_tokenize(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['achieves',\n",
       " 'achieves competitive',\n",
       " 'achieves competitive log',\n",
       " 'attention',\n",
       " 'attention canvas',\n",
       " 'attention canvas memory',\n",
       " 'attention gain',\n",
       " 'attention gain insight',\n",
       " 'attentive',\n",
       " 'attentive unsupervised',\n",
       " 'attentive unsupervised text',\n",
       " 'autr',\n",
       " 'autr learns',\n",
       " 'autr learns meaningful',\n",
       " 'autr word',\n",
       " 'autr word level',\n",
       " 'bounds',\n",
       " 'bounds whilst',\n",
       " 'bounds whilst computationally',\n",
       " 'canvas',\n",
       " 'canvas memory',\n",
       " 'canvas memory mechanism',\n",
       " 'competitive',\n",
       " 'competitive log',\n",
       " 'competitive log likelihood',\n",
       " 'computationally',\n",
       " 'computationally efficient',\n",
       " 'computationally efficient effective',\n",
       " 'construct',\n",
       " 'construct sentences',\n",
       " 'construct sentences viewing',\n",
       " 'constructs',\n",
       " 'constructs sentences',\n",
       " 'constructs sentences demonstrate',\n",
       " 'demonstrate',\n",
       " 'demonstrate autr',\n",
       " 'demonstrate autr learns',\n",
       " 'dynamic',\n",
       " 'dynamic attention',\n",
       " 'dynamic attention canvas',\n",
       " 'effective',\n",
       " 'effective generating',\n",
       " 'effective generating reconstructing',\n",
       " 'efficient',\n",
       " 'efficient effective',\n",
       " 'efficient effective generating',\n",
       " 'gain',\n",
       " 'gain insight',\n",
       " 'gain insight constructs',\n",
       " 'generating',\n",
       " 'generating reconstructing',\n",
       " 'generating reconstructing sentences',\n",
       " 'generative',\n",
       " 'generative model',\n",
       " 'generative model natural',\n",
       " 'imputing',\n",
       " 'imputing missing',\n",
       " 'imputing missing words',\n",
       " 'insight',\n",
       " 'insight constructs',\n",
       " 'insight constructs sentences',\n",
       " 'intermediate',\n",
       " 'intermediate stages',\n",
       " 'intermediate stages model',\n",
       " 'introduce',\n",
       " 'introduce attentive',\n",
       " 'introduce attentive unsupervised',\n",
       " 'iteratively',\n",
       " 'iteratively construct',\n",
       " 'iteratively construct sentences',\n",
       " 'language',\n",
       " 'language uses',\n",
       " 'language uses recurrent',\n",
       " 'latent',\n",
       " 'latent representation',\n",
       " 'latent representation sentence',\n",
       " 'learns',\n",
       " 'learns meaningful',\n",
       " 'learns meaningful latent',\n",
       " 'level',\n",
       " 'level generative',\n",
       " 'level generative model',\n",
       " 'likelihood',\n",
       " 'likelihood lower',\n",
       " 'likelihood lower bounds',\n",
       " 'log',\n",
       " 'log likelihood',\n",
       " 'log likelihood lower',\n",
       " 'lower',\n",
       " 'lower bounds',\n",
       " 'lower bounds whilst',\n",
       " 'meaningful',\n",
       " 'meaningful latent',\n",
       " 'meaningful latent representation',\n",
       " 'mechanism',\n",
       " 'mechanism iteratively',\n",
       " 'mechanism iteratively construct',\n",
       " 'memory',\n",
       " 'memory intermediate',\n",
       " 'memory intermediate stages',\n",
       " 'memory mechanism',\n",
       " 'memory mechanism iteratively',\n",
       " 'missing',\n",
       " 'missing words',\n",
       " 'model',\n",
       " 'model natural',\n",
       " 'model natural language',\n",
       " 'model placing',\n",
       " 'model placing attention',\n",
       " 'natural',\n",
       " 'natural language',\n",
       " 'natural language uses',\n",
       " 'network',\n",
       " 'network dynamic',\n",
       " 'network dynamic attention',\n",
       " 'neural',\n",
       " 'neural network',\n",
       " 'neural network dynamic',\n",
       " 'placing',\n",
       " 'placing attention',\n",
       " 'placing attention gain',\n",
       " 'reconstructing',\n",
       " 'reconstructing sentences',\n",
       " 'reconstructing sentences imputing',\n",
       " 'recurrent',\n",
       " 'recurrent neural',\n",
       " 'recurrent neural network',\n",
       " 'representation',\n",
       " 'representation sentence',\n",
       " 'representation sentence achieves',\n",
       " 'riter',\n",
       " 'riter autr',\n",
       " 'riter autr word',\n",
       " 'sentence',\n",
       " 'sentence achieves',\n",
       " 'sentence achieves competitive',\n",
       " 'sentences',\n",
       " 'sentences demonstrate',\n",
       " 'sentences demonstrate autr',\n",
       " 'sentences imputing',\n",
       " 'sentences imputing missing',\n",
       " 'sentences viewing',\n",
       " 'sentences viewing state',\n",
       " 'stages',\n",
       " 'stages model',\n",
       " 'stages model placing',\n",
       " 'state',\n",
       " 'state memory',\n",
       " 'state memory intermediate',\n",
       " 'text',\n",
       " 'text riter',\n",
       " 'text riter autr',\n",
       " 'unsupervised',\n",
       " 'unsupervised text',\n",
       " 'unsupervised text riter',\n",
       " 'uses',\n",
       " 'uses recurrent',\n",
       " 'uses recurrent neural',\n",
       " 'viewing',\n",
       " 'viewing state',\n",
       " 'viewing state memory',\n",
       " 'whilst',\n",
       " 'whilst computationally',\n",
       " 'whilst computationally efficient',\n",
       " 'word',\n",
       " 'word level',\n",
       " 'word level generative',\n",
       " 'words']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 3), stop_words=\"english\")\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform([normalized])\n",
    "# sentence = nltk.tokenize.sent_tokenize(normalized)\n",
    "# tfidf_vector = tfidf_vectorizer.fit_transform(sentence)\n",
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "interests = 'I want to predict mascots\\' popularity from its photo using machine learning methodologies.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
